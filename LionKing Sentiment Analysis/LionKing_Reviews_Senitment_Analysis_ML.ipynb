{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" align=center><b>\n",
    "    LION KING MOVIE : Reviews Sentiment Analysis</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual packages \n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "from pandas.io.json import json_normalize \n",
    "from itertools import chain\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from itertools import compress\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "# text related packages\n",
    "import re\n",
    "import unicodedata\n",
    "import emoji\n",
    "import spacy\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import gensim\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "## modelling related packages\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Data from Web site: www.RottenTomatoes.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrapping(collect=False,no_pages=None,collection_dir=None):\n",
    "    \n",
    "    if(collect):\n",
    "        \n",
    "        # Settings required for scrapping\n",
    "        headers = {\n",
    "         'Referer': 'https://www.rottentomatoes.com/m/the_lion_king_2019/reviews?type=user',\\\n",
    "         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, likeGecko) Chrome/74.0.3729.108 Safari/537.36',\\\n",
    "         'X-Requested-With': 'XMLHttpRequest',\\\n",
    "        }\n",
    "\n",
    "\n",
    "        url = 'https://www.rottentomatoes.com/napi/movie/9057c2cf-7cab-317f-876f-e50b245ca76e/reviews/user'\n",
    "\n",
    "\n",
    "        payload = {\n",
    "            'direction': 'next',\n",
    "            'endCursor': '',\n",
    "            'startCursor': ''\n",
    "        }\n",
    "\n",
    "\n",
    "        ### Scrap page by page as allowed\n",
    "\n",
    "        parent_path =os.getcwd()\n",
    "        ## Storing all the json data in movie_rdata dir\n",
    "        reviews_path = parent_path +\"\\\\\"+collection_dir\n",
    "        os.chdir(reviews_path)\n",
    "\n",
    "\n",
    "        s = requests.Session()\n",
    "\n",
    "\n",
    "        i=0\n",
    "        while (i < no_pages):\n",
    "            time.sleep(5)\n",
    "            data=''\n",
    "            r=''\n",
    "\n",
    "            #print(payload,\"i=\",i)\n",
    "\n",
    "            r = s.get(url, headers=headers, params=payload) # GET Call\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "\n",
    "            if(data['pageInfo']['hasNextPage']):\n",
    "                next_endCursor=data['pageInfo']['endCursor']\n",
    "\n",
    "            payload = {\n",
    "                'direction': 'next',\n",
    "                'endCursor': next_endCursor,\n",
    "                'startCursor': ''\n",
    "            }\n",
    "\n",
    "            filename=\"page\"+str(i)+\".json\"\n",
    "            with open(filename, 'w') as json_file:\n",
    "                json.dump(data, json_file)\n",
    "            i=i+1\n",
    "\n",
    "            json_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to continue with fresh web-scrapping [yes:no] no\n",
      "Please enter the repository[Dir] from which to process the collected webscrapped pages movie_rdata\n"
     ]
    }
   ],
   "source": [
    "scrap_dir = \"movie_rdata\"\n",
    "total_pages = 300 #default\n",
    "\n",
    "scrap_choice=input(\"Would you like to continue with fresh web-scrapping [yes:no] \")\n",
    "if(scrap_choice=='yes'):\n",
    "    web_scrapping(collect=True,no_pages=total_pages,collection_dir=scrap_dir)\n",
    "else:\n",
    "    scrap_dir=input(\"Please enter the repository[Dir] from which to process the collected webscrapped pages \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the collected data and populate the input DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trex\\Documents\\GitHub\\NLP\\LionKing Sentiment Analysis\\movie_rdata\n",
      "Total files processed= 300\n"
     ]
    }
   ],
   "source": [
    "# Ask User for Movie Collection and the Directory to store\n",
    "\n",
    "parent_path =os.getcwd()\n",
    "reviews_path = parent_path +\"\\\\\"+scrap_dir\n",
    "os.chdir(reviews_path)\n",
    "\n",
    "# Process data from the collected Dir and make a df\n",
    "print(reviews_path)\n",
    "\n",
    "total_files_processed=0\n",
    "\n",
    "for review_file in os.listdir(reviews_path):\n",
    "    if review_file.endswith(\".json\"):\n",
    "        with open(review_file) as infile:\n",
    "            #print(review_file)\n",
    "            jdata=json.load(infile)\n",
    "            if(total_files_processed>0):\n",
    "                movie_df=movie_df.append(json_normalize(jdata['reviews']))\n",
    "                #print(\"coming here\",total_files_processed)\n",
    "            else:\n",
    "                movie_df=json_normalize(jdata['reviews'])\n",
    "                #print(\"first time\",total_files_processed)\n",
    "          \n",
    "            total_files_processed=total_files_processed+1\n",
    "\n",
    "            \n",
    "print(\"Total files processed=\",total_files_processed)\n",
    "os.chdir(parent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting index as each page indexes page as 0-9 from json collection method\n",
    "movie_df=movie_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createDate</th>\n",
       "      <th>displayImageUrl</th>\n",
       "      <th>displayName</th>\n",
       "      <th>hasProfanity</th>\n",
       "      <th>hasSpoilers</th>\n",
       "      <th>isSuperReviewer</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>timeFromCreation</th>\n",
       "      <th>updateDate</th>\n",
       "      <th>user.accountLink</th>\n",
       "      <th>user.displayName</th>\n",
       "      <th>user.realm</th>\n",
       "      <th>user.userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-18T08:54:30.664Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Joanne H</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_5</td>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1h ago</td>\n",
       "      <td>2019-08-18T08:54:30.890Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Joanne H</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>2c73ed20-5b9f-41b3-a4fd-8dd3ff8bb20a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-18T08:03:49.380Z</td>\n",
       "      <td>https://graph.facebook.com/v3.3/594379764/picture</td>\n",
       "      <td>Frankie C</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>STAR_5</td>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2h ago</td>\n",
       "      <td>2019-08-18T08:03:49.380Z</td>\n",
       "      <td>/user/id/871398953</td>\n",
       "      <td>Frankie C</td>\n",
       "      <td>RT</td>\n",
       "      <td>871398953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-18T07:13:32.422Z</td>\n",
       "      <td>None</td>\n",
       "      <td>jaycee</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>STAR_5</td>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3h ago</td>\n",
       "      <td>2019-08-18T07:13:32.422Z</td>\n",
       "      <td>None</td>\n",
       "      <td>jaycee</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>DD2453B0-37CE-4B47-A099-D15378FC310E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-18T07:06:08.698Z</td>\n",
       "      <td>https://graph.facebook.com/v3.3/10000047937306...</td>\n",
       "      <td>Peter A</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>STAR_2</td>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3h ago</td>\n",
       "      <td>2019-08-18T07:06:46.610Z</td>\n",
       "      <td>/user/id/906750266</td>\n",
       "      <td>Peter A</td>\n",
       "      <td>RT</td>\n",
       "      <td>906750266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-18T06:38:46.892Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Kevin M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_4</td>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3h ago</td>\n",
       "      <td>2019-08-18T06:38:46.892Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Kevin M</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>21FE7D93-351C-43B5-833D-B1DB11F3FD6A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-08-18T05:34:19.360Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Angelwolf</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_4</td>\n",
       "      <td>The animation was really good</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4h ago</td>\n",
       "      <td>2019-08-18T05:34:19.360Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Angelwolf</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>E62CDAD2-329A-4924-8455-35249C3BCC4D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-08-18T05:22:03.783Z</td>\n",
       "      <td>None</td>\n",
       "      <td>sheril</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_5</td>\n",
       "      <td>i thought it was awesone. i really like the or...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5h ago</td>\n",
       "      <td>2019-08-18T05:22:03.783Z</td>\n",
       "      <td>None</td>\n",
       "      <td>sheril</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>8229f609-93a7-4c8e-8ec7-37b18fa65679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-08-18T05:14:42.958Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Maggie L</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_3</td>\n",
       "      <td>I think that they should’ve brought back more ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5h ago</td>\n",
       "      <td>2019-08-18T05:14:42.958Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Maggie L</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>8B9ED608-C9C6-4EAF-82A3-1DF9FFFD0EA4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-08-18T04:58:59.214Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Bryan A</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_5</td>\n",
       "      <td>Great, absolutely great!  The animation was gr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5h ago</td>\n",
       "      <td>2019-08-18T04:58:59.214Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Bryan A</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>C792B3A4-EC33-48C2-A911-A2AAB946B94F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-08-18T04:51:44.361Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Bethany</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_5</td>\n",
       "      <td>It was just like the first one. Very good anim...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5h ago</td>\n",
       "      <td>2019-08-18T04:51:44.361Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Bethany</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>45c8dfaa-e20f-44df-8f01-445c0f055e42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-08-18T04:35:16.243Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Alexander G</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>STAR_3</td>\n",
       "      <td>Didn't make me feel anything unlike the original.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5h ago</td>\n",
       "      <td>2019-08-18T04:35:16.243Z</td>\n",
       "      <td>/user/id/978203293</td>\n",
       "      <td>Alexander G</td>\n",
       "      <td>RT</td>\n",
       "      <td>978203293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-08-18T04:10:11.153Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Lori</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_5</td>\n",
       "      <td>Well done! A great way to redo a classic. It f...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6h ago</td>\n",
       "      <td>2019-08-18T04:10:11.153Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Lori</td>\n",
       "      <td>Fandango</td>\n",
       "      <td>8ce21b61-d2b1-453e-ab78-fe0ba06088d5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  createDate  \\\n",
       "0   2019-08-18T08:54:30.664Z   \n",
       "1   2019-08-18T08:03:49.380Z   \n",
       "2   2019-08-18T07:13:32.422Z   \n",
       "3   2019-08-18T07:06:08.698Z   \n",
       "4   2019-08-18T06:38:46.892Z   \n",
       "5   2019-08-18T05:34:19.360Z   \n",
       "6   2019-08-18T05:22:03.783Z   \n",
       "7   2019-08-18T05:14:42.958Z   \n",
       "8   2019-08-18T04:58:59.214Z   \n",
       "9   2019-08-18T04:51:44.361Z   \n",
       "10  2019-08-18T04:35:16.243Z   \n",
       "11  2019-08-18T04:10:11.153Z   \n",
       "\n",
       "                                      displayImageUrl  displayName  \\\n",
       "0                                                None     Joanne H   \n",
       "1   https://graph.facebook.com/v3.3/594379764/picture    Frankie C   \n",
       "2                                                None       jaycee   \n",
       "3   https://graph.facebook.com/v3.3/10000047937306...      Peter A   \n",
       "4                                                None      Kevin M   \n",
       "5                                                None    Angelwolf   \n",
       "6                                                None       sheril   \n",
       "7                                                None     Maggie L   \n",
       "8                                                None      Bryan A   \n",
       "9                                                None      Bethany   \n",
       "10                                               None  Alexander G   \n",
       "11                                               None         Lori   \n",
       "\n",
       "    hasProfanity  hasSpoilers  isSuperReviewer  isVerified  rating  \\\n",
       "0          False        False            False        True  STAR_5   \n",
       "1          False        False            False       False  STAR_5   \n",
       "2          False        False            False       False  STAR_5   \n",
       "3          False        False            False       False  STAR_2   \n",
       "4          False        False            False        True  STAR_4   \n",
       "5          False        False            False        True  STAR_4   \n",
       "6          False        False            False        True  STAR_5   \n",
       "7          False        False            False        True  STAR_3   \n",
       "8          False        False            False        True  STAR_5   \n",
       "9          False        False            False        True  STAR_5   \n",
       "10         False        False            False       False  STAR_3   \n",
       "11         False        False            False        True  STAR_5   \n",
       "\n",
       "                                               review  score timeFromCreation  \\\n",
       "0   I liked most that the animation made the anima...    5.0           1h ago   \n",
       "1          Amazing! So realistic and incredible music    5.0           2h ago   \n",
       "2   Classic. Good remake. Loved it. Glover was out...    5.0           3h ago   \n",
       "3   Nice animation/CGI but completely lacking the ...    2.0           3h ago   \n",
       "4        Good but go mainly for the sentimental value    4.0           3h ago   \n",
       "5                       The animation was really good    4.0           4h ago   \n",
       "6   i thought it was awesone. i really like the or...    5.0           5h ago   \n",
       "7   I think that they should’ve brought back more ...    3.0           5h ago   \n",
       "8   Great, absolutely great!  The animation was gr...    5.0           5h ago   \n",
       "9   It was just like the first one. Very good anim...    5.0           5h ago   \n",
       "10  Didn't make me feel anything unlike the original.    3.0           5h ago   \n",
       "11  Well done! A great way to redo a classic. It f...    5.0           6h ago   \n",
       "\n",
       "                  updateDate    user.accountLink user.displayName user.realm  \\\n",
       "0   2019-08-18T08:54:30.890Z                None         Joanne H   Fandango   \n",
       "1   2019-08-18T08:03:49.380Z  /user/id/871398953        Frankie C         RT   \n",
       "2   2019-08-18T07:13:32.422Z                None           jaycee   Fandango   \n",
       "3   2019-08-18T07:06:46.610Z  /user/id/906750266          Peter A         RT   \n",
       "4   2019-08-18T06:38:46.892Z                None          Kevin M   Fandango   \n",
       "5   2019-08-18T05:34:19.360Z                None        Angelwolf   Fandango   \n",
       "6   2019-08-18T05:22:03.783Z                None           sheril   Fandango   \n",
       "7   2019-08-18T05:14:42.958Z                None         Maggie L   Fandango   \n",
       "8   2019-08-18T04:58:59.214Z                None          Bryan A   Fandango   \n",
       "9   2019-08-18T04:51:44.361Z                None          Bethany   Fandango   \n",
       "10  2019-08-18T04:35:16.243Z  /user/id/978203293      Alexander G         RT   \n",
       "11  2019-08-18T04:10:11.153Z                None             Lori   Fandango   \n",
       "\n",
       "                             user.userId  \n",
       "0   2c73ed20-5b9f-41b3-a4fd-8dd3ff8bb20a  \n",
       "1                              871398953  \n",
       "2   DD2453B0-37CE-4B47-A099-D15378FC310E  \n",
       "3                              906750266  \n",
       "4   21FE7D93-351C-43B5-833D-B1DB11F3FD6A  \n",
       "5   E62CDAD2-329A-4924-8455-35249C3BCC4D  \n",
       "6   8229f609-93a7-4c8e-8ec7-37b18fa65679  \n",
       "7   8B9ED608-C9C6-4EAF-82A3-1DF9FFFD0EA4  \n",
       "8   C792B3A4-EC33-48C2-A911-A2AAB946B94F  \n",
       "9   45c8dfaa-e20f-44df-8f01-445c0f055e42  \n",
       "10                             978203293  \n",
       "11  8ce21b61-d2b1-453e-ab78-fe0ba06088d5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createDate             0\n",
       "displayImageUrl     2778\n",
       "displayName          138\n",
       "hasProfanity           0\n",
       "hasSpoilers            0\n",
       "isSuperReviewer        0\n",
       "isVerified             0\n",
       "rating                 0\n",
       "review                 0\n",
       "score                  0\n",
       "timeFromCreation       0\n",
       "updateDate             0\n",
       "user.accountLink    2390\n",
       "user.displayName     138\n",
       "user.realm             0\n",
       "user.userId            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Review column and score column is not having null values. This is the column which we can use as per conditions. Hence, no need of impution.\n",
      "----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "message=\"Review column and score column is not having null values. This is the column which we can use as per conditions. Hence, no need of impution.\"\n",
    "print_line=\"----------------------------------------------------------------------------------------------------------------------------\"\n",
    "print(print_line)\n",
    "print(message)\n",
    "print(print_line)\n",
    "message=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Ultimate Test data on which final prediction is to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ultimate_TestData = pd.read_csv(\"test-1566619745327.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92876</td>\n",
       "      <td>Was good. Nothing like the original but I beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92877</td>\n",
       "      <td>I absolutely loved it! A wonderful rendition o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92878</td>\n",
       "      <td>I love the movie! Good job director! \\nI appre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92879</td>\n",
       "      <td>GREAT MOVIE!!  Same as the original, but the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92880</td>\n",
       "      <td>Realistic.   Fantastic special effects</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewID                                             review\n",
       "0     92876  Was good. Nothing like the original but I beli...\n",
       "1     92877  I absolutely loved it! A wonderful rendition o...\n",
       "2     92878  I love the movie! Good job director! \\nI appre...\n",
       "3     92879  GREAT MOVIE!!  Same as the original, but the c...\n",
       "4     92880             Realistic.   Fantastic special effects"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ultimate_TestData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ultimate_TestData.drop('ReviewID',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was good. Nothing like the original but I beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I absolutely loved it! A wonderful rendition o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the movie! Good job director! \\nI appre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREAT MOVIE!!  Same as the original, but the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Realistic.   Fantastic special effects</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  Was good. Nothing like the original but I beli...\n",
       "1  I absolutely loved it! A wonderful rendition o...\n",
       "2  I love the movie! Good job director! \\nI appre...\n",
       "3  GREAT MOVIE!!  Same as the original, but the c...\n",
       "4             Realistic.   Fantastic special effects"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ultimate_TestData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ultimate_TestData.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "We don't see any Review as null. Hence, we are all good.\n",
      "----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "message=\"We don't see any Review as null. Hence, we are all good.\"\n",
    "print(print_line)\n",
    "print(message)\n",
    "print(print_line)\n",
    "message=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the target variable for classification to input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df['targetSentiment']=[0 if x>3 else 1 for x in movie_df['score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the columns which deemed to be ignored as per instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['createDate', 'displayImageUrl', 'displayName', 'hasProfanity',\n",
       "       'hasSpoilers', 'isSuperReviewer', 'isVerified', 'rating', 'review',\n",
       "       'score', 'timeFromCreation', 'updateDate', 'user.accountLink',\n",
       "       'user.displayName', 'user.realm', 'user.userId', 'targetSentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_except_review_target = ['createDate', 'displayImageUrl', 'displayName', 'hasProfanity',\n",
    "       'hasSpoilers', 'isSuperReviewer', 'isVerified', 'rating','timeFromCreation', 'updateDate', 'user.accountLink',\n",
    "       'user.displayName', 'user.realm', 'user.userId','score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.drop(columns_except_review_target,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment\n",
       "0  I liked most that the animation made the anima...                0\n",
       "1         Amazing! So realistic and incredible music                0\n",
       "2  Classic. Good remake. Loved it. Glover was out...                0\n",
       "3  Nice animation/CGI but completely lacking the ...                1\n",
       "4       Good but go mainly for the sentimental value                0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>I took my 8 year old grandson. We both enjoyed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>The animated version is still number one.  All...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Wonderful movie. I love this movie with the re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>My 3 stars is for the special effects alone. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>By trying to be realistic the movie loses its ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  targetSentiment\n",
       "2995  I took my 8 year old grandson. We both enjoyed...                0\n",
       "2996  The animated version is still number one.  All...                1\n",
       "2997  Wonderful movie. I love this movie with the re...                0\n",
       "2998  My 3 stars is for the special effects alone. I...                1\n",
       "2999  By trying to be realistic the movie loses its ...                1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do the contraction operation\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "# Function to do normalize operation\n",
    "def normalize_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    #https://docs.python.org/2/library/unicodedata.html\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to add tag in sentences having did not, could not etc till punctuation\n",
    "def add_splTAG_after_not(sentence=None):\n",
    "    transformed = re.sub(r'\\b(?:not|never)\\b[\\w\\s]+[^\\w\\s]',\n",
    "       lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1negtagneg\\2', match.group(0)), \n",
    "       sentence,\n",
    "       flags=re.IGNORECASE)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "# Function to transform the word to its lemma form\n",
    "def tokenize_lemma_clean(dataframe,column_name,dest_column_name,StopWord_Removal=False,Only_2Len_word_Removal=False):\n",
    "    # text cleaning and pre-processing   \n",
    "    for index,row in dataframe.iterrows():  \n",
    "        row[column_name]=row[column_name].lower()\n",
    "\n",
    "        doc=nlp(row[column_name])\n",
    "\n",
    "        ## 1st pass of removing only 2 letter words or stop words\n",
    "        if(Only_2Len_word_Removal):\n",
    "            #may use STOP words for conjuction detection\n",
    "            #remove words with <=2 chars\n",
    "            clean_tokens1 = [token for token in doc if len(token.text)>2]\n",
    "            \n",
    "        if(StopWord_Removal):\n",
    "        # remove stop words\n",
    "            if(Only_2Len_word_Removal):\n",
    "                clean_tokens1 = [token for token in clean_tokens1 if not token.is_stop]\n",
    "            else:\n",
    "                clean_tokens1 = [token for token in doc if not token.is_stop]\n",
    "       \n",
    "        ## 2nd pass of removing non-alpha words \n",
    "        if ((StopWord_Removal==True) or ( Only_2Len_word_Removal==True)):\n",
    "            clean_tokens2_bool = [token.is_alpha for token in clean_tokens1]\n",
    "            #clean_tokens2_bool = [token.is_alpha for token in doc]\n",
    "            clean_tokens2=list(compress(clean_tokens1,clean_tokens2_bool))\n",
    "        else:\n",
    "            ## if only Lemma operation is needed\n",
    "            clean_tokens2 = [token for token in doc]\n",
    "        \n",
    "        ## 3rd pass of detecting special NOT tag added earlier for did not type of sentences\n",
    "        ## Where ever encounter the spl tag addtition, keep it in the review comment for later processing\n",
    "        ## Hence ignore it from Lemma operation\n",
    "        ## Also for the added word in spltag, keep the lemma word, so that its easier to create a dictionary \n",
    "        ## of antonymns rather than having issues with past or future tense of same words/verbs\n",
    "        not_string = re.compile('negtagneg.*')\n",
    "        \n",
    "        clean_tokens3=[]\n",
    "        for token in clean_tokens2:\n",
    "            if(not_string.match(token.text)):\n",
    "                neg_whole_word=\"\"\n",
    "                neg_whole_word=token.text\n",
    "                word_for_lemma = neg_whole_word.split('negtagneg')\n",
    "                neg_doc = nlp(word_for_lemma[1])\n",
    "                neg_lemma_word=\"\"\n",
    "                for tkn in neg_doc:\n",
    "                    if (len(tkn)>2): #remove 2 letter words, which will be stopwords added with negation\n",
    "                        neg_lemma_word = tkn.lemma_\n",
    "                \n",
    "                clean_tokens3.append(\"negtagneg\"+neg_lemma_word)\n",
    "            else:\n",
    "                clean_tokens3.append(token.lemma_)\n",
    "\n",
    "        clean_text=' '.join(clean_tokens3)\n",
    "\n",
    "        ## Add the cleaned text to the dataframe column desired\n",
    "        #dataframe.at[index,column_name]=clean_text\n",
    "        #print(\"clean text:\", clean_text)\n",
    "        dataframe.at[index,dest_column_name]=clean_text\n",
    "\n",
    "        \n",
    "# Function to keep specific POS tag words \n",
    "# whichever POS tag words are needed can be passed in X1 ...X4\n",
    "def pos_to_keep_and_count(dataframe,column_name,dest_column_name,dest_count_coulmn_name,Take_Count=False, x1=None,x2=None,x3=None,x4=None):\n",
    "\n",
    "    for index,row in dataframe.iterrows():\n",
    "        doc=nlp(row[column_name])\n",
    "        \n",
    "        ## words which have different POS tags based on context / placement\n",
    "        ambiguous_pos_words = ['like','love']\n",
    "\n",
    "        # POS specific words\n",
    "        pos_tokens = [token.text for token in doc if ((token.pos_ in [x1,x2,x3,x4])or(token.text in ambiguous_pos_words ))]\n",
    "        \n",
    "\n",
    "        pos_text=' '.join(pos_tokens)\n",
    "        \n",
    "        if(Take_Count):\n",
    "            # add # of POS tags in desired column nameS\n",
    "            dataframe.at[index,dest_count_coulmn_name]=len(pos_tokens)\n",
    "\n",
    "        dataframe.at[index,dest_column_name]=pos_text\n",
    "\n",
    "        \n",
    "# Change to antonyms for words suceeding the NOT spl tags        \n",
    "def antonyms_change(dataframe,column_name,dest_column_name,word=None,antonym=None):\n",
    "    # text cleaning and pre-processing\n",
    "    #print(\"coming inside antonymns\")\n",
    "    for index,row in dataframe.iterrows():\n",
    "\n",
    "        doc=row[column_name].split()\n",
    "\n",
    "        not_string = re.compile(\"^\"+word+\"$\")\n",
    "\n",
    "        reverse_tokens=[]\n",
    "\n",
    "        for token in doc:\n",
    "            if(not_string.match(token)):\n",
    "                if(len(antonym)>1):\n",
    "                    reverse_tokens.append(antonym)\n",
    "            else:\n",
    "                reverse_tokens.append(token)\n",
    "\n",
    "        clean_text=' '.join(reverse_tokens)\n",
    "\n",
    "        dataframe.at[index,dest_column_name]=clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary list of all the negations replacement\n",
    "all_antonymns_dict = {\n",
    "'negtagneglike':'dislike',\n",
    "'negtagneggood':'bad',\n",
    "'negtagnegoriginal':'fake',\n",
    "'negtagnegreal':'counterfeit',\n",
    "'negtagnegfunny':'melancholy',\n",
    "'negtagnegspectacular':'paltry',\n",
    "'negtagnegimpress':'depress',\n",
    "'negtagneginteresting':'dull',   \n",
    "'negtagnegremember':'forget',\n",
    "'negtagnegnear':'far',\n",
    "'negtagnegfeel':'apathy',\n",
    "'negtagnegadd':'miss',\n",
    "'negtagnegkeep':'remove',\n",
    "'negtagnegthink':'forget',\n",
    "'negtagnegjustice':'injustice',\n",
    "'negtagnegadd':'diminish',\n",
    "'negtagnegway':'away',\n",
    "'negtagnegmake':'destroy',\n",
    "'negtagnegunlike':'unlike',\n",
    "'negtagnegunnecessary':'unnecessary',\n",
    "'negtagnegamazing': 'boring',\n",
    "'negtagneganimation': 'mockery',\n",
    "'negtagnegcartoon': 'spoof',\n",
    "'negtagnegbelieve': 'disbelieve',\n",
    "'negtagnegbetter': 'worse',\n",
    "'negtagnegbig':'small',\n",
    "'negtagnegcare':'disregard',\n",
    "'negtagnegbad': 'bad',\n",
    "'negtagnegbring':'repulse',\n",
    "'negtagnegbuy':'relinquish',\n",
    "'negtagneglion':'',\n",
    "'negtagnegking':'',\n",
    "'negtagnegall':'',\n",
    "'negtagnegas':'',\n",
    "'negtagnegand':'',\n",
    "'negtagnega':'',\n",
    "'negtagnegbe':'',\n",
    "'negtagnegbut':'',\n",
    "'negtagnegfor':'',\n",
    "'negtagneghave':'',\n",
    "'negtagnegi':'',\n",
    "'negtagnegis':'',\n",
    "'negtagnegin':'',\n",
    "'negtagnegabove':'',\n",
    "'negtagnegit':'',\n",
    "'negtagnegmovie':'',\n",
    "'negtagnegmuch':'',\n",
    "'negtagnegnot':'',\n",
    "'negtagnegof':'',\n",
    "'negtagnegon':'',\n",
    "'negtagnegthe':'',\n",
    "'negtagnegthat':'',\n",
    "'negtagnegthis':'',\n",
    "'negtagnegwas':'',\n",
    "'negtagnegis':'',\n",
    "'negtagnegare':'',\n",
    "'negtagnegwith':'',\n",
    "'negtagnegof':'',\n",
    "'negtagnegif':'',\n",
    "'negtagnegversion':'',\n",
    "'negtagnegneed':'',\n",
    "'negtagnegremake':'',\n",
    "'negtagnegput':'',\n",
    "'negtagnegnala':'',\n",
    "'negtagnegme':'',\n",
    "'negtagnegbeyonce':'',\n",
    "'negtagneganything':'',\n",
    "'negtagnegbeing':'',\n",
    "'negtagneg-PRON-':'',\n",
    "'negtagnegdo':'',\n",
    "'negtagneg':'',\n",
    "'negtagnegsome':'',\n",
    "'negtagnegstuff':'',\n",
    "'negtagnegthing':'',\n",
    "'negtagnegout':'',\n",
    "'negtagneganimal':'',\n",
    "'negtagneganimate': '',\n",
    "'negtagnegfilm': '',\n",
    "'negtagnegany':'',\n",
    "'negnegtagneganyone':'',\n",
    "'negtagnegattention':'',\n",
    "'negtagnegaudience':'',\n",
    "'negtagnegbeauty':'',\n",
    "'negtagnegbecause':'',\n",
    "'negtagnegbefore':'',\n",
    "'negtagnegbegin':'',\n",
    "'negtagnegthan':'',\n",
    "'negtagnegbother':'',\n",
    "'negtagnegcan':'',\n",
    "'negtagnegdisney':'',\n",
    "'negtagnegeither':'',\n",
    "'negtagnegever':'',\n",
    "'negtagnegnow':'',\n",
    "'negtagnegalso':'',\n",
    "'negtagnegagain':'',\n",
    "'negtagnegthere':'',\n",
    "'negtagnegthose':'',\n",
    "'negtagnegvery':'',\n",
    "'negtagnegwould':'',\n",
    "'negtagnegwhy':'',\n",
    "'negtagnegyear':'',\n",
    "'negtagnegtimon':'',\n",
    "'negtagnegtoo':'',\n",
    "'negtagnegunder':'',\n",
    "'negtagneguntil':'',\n",
    "'negtagnegcharacter':'',\n",
    "'negtagnegwhich':'',\n",
    "'negtagnegwho':'',\n",
    "'negtagnegwhere':'',\n",
    "'negtagnegwhen':'',\n",
    "'negtagnegabout':'',\n",
    "'negtagneglet':'',\n",
    "'negtagnegleast':'',\n",
    "'negtagnegagain':'',\n",
    "'negtagnegactor':'',\n",
    "'negtagnegactually':'',\n",
    "'negtagneghyena':'',\n",
    "'negtagnegapart':'',\n",
    "\n",
    "\n",
    "} ## TODO: will add more negative words for replacement\n",
    "\n",
    "\n",
    "# substitute words \n",
    "# will be using for changing spltagged words for negatives to antonymns\n",
    "def substitute_words(dataframe,source_column_name,dest_column_name,dict_rep_words=all_antonymns_dict):\n",
    "    try:\n",
    "        dataframe.insert(2,source_column_name,dataframe[clean_after_lemma_n_stop_2len_word])\n",
    "        print (\"Creating a column for the cleaned antonyms process\")\n",
    "    except:\n",
    "        print (\"Error: May be the column already exits\")\n",
    "\n",
    "    print(\"Negation replacement in progress ...\")\n",
    "        \n",
    "    for key in dict_rep_words.keys():\n",
    "        antonyms_change(dataframe,source_column_name,dest_column_name,key,dict_rep_words[key])\n",
    "        \n",
    "    print(\"Negation replacement all Done!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Feature Extraction (non-lexical) from review comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features from the review content \n",
    "# #Words\n",
    "# #Capital Words\n",
    "# #Question Marks \n",
    "# #Exclaimation Marks\n",
    "# #NOT words\n",
    "# #Emojis\n",
    "\n",
    "class Spl_Chars_Counts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet,flags=re.IGNORECASE))\n",
    "\n",
    "    def count_CAPS_regex(self, pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
    "        count_love = X.apply(lambda x: self.count_regex(r'love|great|superb|amazing|awesome|beautiful|fantastic|marvellous|magnificent|stunning|spectacular|wonderful|remarkable|fabulous|incredible|astounding|astonishing|unbelievable', x))\n",
    "        count_shit = X.apply(lambda x: self.count_regex(r'hate|dislik|ugly|rubbish|trash|garbage|awful|boring|slow|waste|weird|forgettable|distast|lack|shit|ass|dung|fuck|dick|cunt|bitch|bastard|ruin', x))\n",
    "        count_capital_words = X.apply(lambda x: self.count_CAPS_regex(r'\\b[A-Z]{2,}\\b', x))\n",
    "        count_quest_marks = X.apply(lambda x: self.count_regex(r'\\?', x))\n",
    "        count_excl_marks = X.apply(lambda x: self.count_regex(r'!', x))\n",
    "        count_NOT_words = X.apply(lambda x: self.count_regex(r'\\bN*oo*t*\\b', x))\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "                           'count_words': count_words\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_love' : count_love\n",
    "                           , 'count_shit' : count_shit\n",
    "                           , 'count_quest_marks': count_quest_marks\n",
    "                           , 'count_excl_marks': count_excl_marks\n",
    "                           , 'count_NOT_words': count_NOT_words\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to check polarity of words/ phrases/ sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_polarity(dataframe,column_name,dest_column_name,MULT_Factor):\n",
    "    for index,row in dataframe.iterrows():\n",
    "        review=TextBlob(row[column_name])\n",
    "\n",
    "        dataframe.at[index,dest_column_name]=MULT_Factor*review.polarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate average of w2v for list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_w2v_vector(w2v_dict, review):\n",
    "    list_of_word_vectors = [w2v_dict[w] for w in review if w in w2v_dict.vocab.keys()]\n",
    "    \n",
    "    if len(list_of_word_vectors) == 0:\n",
    "        result = [0.0]*SIZE\n",
    "    else:\n",
    "        result = np.sum(list_of_word_vectors, axis=0) / len(list_of_word_vectors)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Starting Processing of Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### copying train data till this moment and safe-keeping; Using the copied variable for processing ahead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = movie_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns name for orginal review text\n",
    "review = 'review'\n",
    "review_polar = 'review_polar'\n",
    "\n",
    "## for 1st stage of cleaning data\n",
    "clean_spll_contr_accnt_NOT = 'spll_contr_accnt_NOT'\n",
    "\n",
    "## for 2nd stage of cleaning data\n",
    "clean_after_lemma_only = 'lemma_only'\n",
    "clean_after_lemma_n_stop = 'lemma_stop'\n",
    "clean_after_lemma_only_2len_word = 'lemma_2len_word'\n",
    "clean_after_lemma_n_stop_2len_word = 'lemma_stop_2len_word'\n",
    "\n",
    "## for 3rd stage of cleaning data\n",
    "clean_after_antonymns = 'antonymns_upd'\n",
    "\n",
    "## for POS tags\n",
    "adj_count = 'adj_count'\n",
    "adj_polar ='adj_polar'\n",
    "adj_text='adj_text'\n",
    "\n",
    "\n",
    "adv_verb_count = 'adv_verb_count'\n",
    "adv_verb_polar = 'adv_verb_polar'\n",
    "adv_verb_text = 'adv_verb_text'\n",
    "\n",
    "\n",
    "## target column\n",
    "targetSentiment = 'targetSentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment\n",
       "0  I liked most that the animation made the anima...                0\n",
       "1         Amazing! So realistic and incredible music                0\n",
       "2  Classic. Good remake. Loved it. Glover was out...                0\n",
       "3  Nice animation/CGI but completely lacking the ...                1\n",
       "4       Good but go mainly for the sentimental value                0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_data[clean_spll_contr_accnt_NOT] = [TextBlob(text).correct() for text in total_data['review']]\n",
    "\n",
    "## TextBlob Spell correction changing many words into different words, which is not desired.\n",
    "## Hence not using it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[clean_spll_contr_accnt_NOT] = [expand_contractions(re.sub('’', \"'\", text)) for text in total_data['review']]\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "Ultimate_TestData[clean_spll_contr_accnt_NOT] = [expand_contractions(re.sub('’', \"'\", text)) for text in Ultimate_TestData['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>spll_contr_accnt_NOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                spll_contr_accnt_NOT  \n",
       "0  I liked most that the animation made the anima...  \n",
       "1         Amazing! So realistic and incredible music  \n",
       "2  Classic. Good remake. Loved it. Glover was out...  \n",
       "3  Nice animation/CGI but completely lacking the ...  \n",
       "4       Good but go mainly for the sentimental value  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>spll_contr_accnt_NOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was good. Nothing like the original but I beli...</td>\n",
       "      <td>Was good. Nothing like the original but I beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I absolutely loved it! A wonderful rendition o...</td>\n",
       "      <td>I absolutely loved it! A wonderful rendition o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the movie! Good job director! \\nI appre...</td>\n",
       "      <td>I love the movie! Good job director! \\nI appre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREAT MOVIE!!  Same as the original, but the c...</td>\n",
       "      <td>GREAT MOVIE!!  Same as the original, but the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Realistic.   Fantastic special effects</td>\n",
       "      <td>Realistic.   Fantastic special effects</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Was good. Nothing like the original but I beli...   \n",
       "1  I absolutely loved it! A wonderful rendition o...   \n",
       "2  I love the movie! Good job director! \\nI appre...   \n",
       "3  GREAT MOVIE!!  Same as the original, but the c...   \n",
       "4             Realistic.   Fantastic special effects   \n",
       "\n",
       "                                spll_contr_accnt_NOT  \n",
       "0  Was good. Nothing like the original but I beli...  \n",
       "1  I absolutely loved it! A wonderful rendition o...  \n",
       "2  I love the movie! Good job director! \\nI appre...  \n",
       "3  GREAT MOVIE!!  Same as the original, but the c...  \n",
       "4             Realistic.   Fantastic special effects  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ultimate_TestData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalize accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[clean_spll_contr_accnt_NOT] = [normalize_accented_chars(text) for text in total_data[clean_spll_contr_accnt_NOT]]\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "Ultimate_TestData[clean_spll_contr_accnt_NOT] = [normalize_accented_chars(text) for text in Ultimate_TestData[clean_spll_contr_accnt_NOT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>spll_contr_accnt_NOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                spll_contr_accnt_NOT  \n",
       "0  I liked most that the animation made the anima...  \n",
       "1         Amazing! So realistic and incredible music  \n",
       "2  Classic. Good remake. Loved it. Glover was out...  \n",
       "3  Nice animation/CGI but completely lacking the ...  \n",
       "4       Good but go mainly for the sentimental value  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Add NOT_ after not is encountered till punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data[clean_spll_contr_accnt_NOT] = [add_splTAG_after_not(text) for text in total_data[clean_spll_contr_accnt_NOT]]\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "Ultimate_TestData[clean_spll_contr_accnt_NOT] = [add_splTAG_after_not(text) for text in Ultimate_TestData[clean_spll_contr_accnt_NOT]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>spll_contr_accnt_NOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                spll_contr_accnt_NOT  \n",
       "0  I liked most that the animation made the anima...  \n",
       "1         Amazing! So realistic and incredible music  \n",
       "2  Classic. Good remake. Loved it. Glover was out...  \n",
       "3  Nice animation/CGI but completely lacking the ...  \n",
       "4       Good but go mainly for the sentimental value  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Do lemma and cleaning of stop words etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_lemma_clean(total_data,clean_spll_contr_accnt_NOT,clean_after_lemma_n_stop_2len_word,StopWord_Removal=True,Only_2Len_word_Removal=False)\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "tokenize_lemma_clean(Ultimate_TestData,clean_spll_contr_accnt_NOT,clean_after_lemma_n_stop_2len_word,StopWord_Removal=True,Only_2Len_word_Removal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>spll_contr_accnt_NOT</th>\n",
       "      <th>lemma_stop_2len_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>like animation animal look real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                spll_contr_accnt_NOT  \\\n",
       "0  I liked most that the animation made the anima...   \n",
       "1         Amazing! So realistic and incredible music   \n",
       "2  Classic. Good remake. Loved it. Glover was out...   \n",
       "3  Nice animation/CGI but completely lacking the ...   \n",
       "4       Good but go mainly for the sentimental value   \n",
       "\n",
       "                                lemma_stop_2len_word  \n",
       "0                    like animation animal look real  \n",
       "1                 amazing realistic incredible music  \n",
       "2        classic good remake love glover outstanding  \n",
       "3  nice animation cgi completely lack disney styl...  \n",
       "4                      good mainly sentimental value  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words succeeding the NOT tags with Antonyms or remove if useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before going ahead with identifying POS words, change the negation succeeding words to antonymns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a column for the cleaned antonyms process\n",
      "Negation replacement in progress ...\n",
      "Negation replacement all Done!\n",
      "Creating a column for the cleaned antonyms process\n",
      "Negation replacement in progress ...\n",
      "Negation replacement all Done!\n"
     ]
    }
   ],
   "source": [
    "substitute_words(total_data,source_column_name=clean_after_antonymns,dest_column_name=clean_after_antonymns,dict_rep_words=all_antonymns_dict)\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "substitute_words(Ultimate_TestData,source_column_name=clean_after_antonymns,dest_column_name=clean_after_antonymns,dict_rep_words=all_antonymns_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>spll_contr_accnt_NOT</th>\n",
       "      <th>lemma_stop_2len_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>like animation animal look real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                       antonymns_upd  \\\n",
       "0                    like animation animal look real   \n",
       "1                 amazing realistic incredible music   \n",
       "2        classic good remake love glover outstanding   \n",
       "3  nice animation cgi completely lack disney styl...   \n",
       "4                      good mainly sentimental value   \n",
       "\n",
       "                                spll_contr_accnt_NOT  \\\n",
       "0  I liked most that the animation made the anima...   \n",
       "1         Amazing! So realistic and incredible music   \n",
       "2  Classic. Good remake. Loved it. Glover was out...   \n",
       "3  Nice animation/CGI but completely lacking the ...   \n",
       "4       Good but go mainly for the sentimental value   \n",
       "\n",
       "                                lemma_stop_2len_word  \n",
       "0                    like animation animal look real  \n",
       "1                 amazing realistic incredible music  \n",
       "2        classic good remake love glover outstanding  \n",
       "3  nice animation cgi completely lack disney styl...  \n",
       "4                      good mainly sentimental value  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>spll_contr_accnt_NOT</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>lemma_stop_2len_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was good. Nothing like the original but I beli...</td>\n",
       "      <td>was good. nothing like the original but i beli...</td>\n",
       "      <td>good like original believe point</td>\n",
       "      <td>good like original believe point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I absolutely loved it! A wonderful rendition o...</td>\n",
       "      <td>i absolutely loved it! a wonderful rendition o...</td>\n",
       "      <td>absolutely love wonderful rendition original n...</td>\n",
       "      <td>absolutely love wonderful rendition original n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the movie! Good job director! \\nI appre...</td>\n",
       "      <td>i love the movie! good job director! \\ni appre...</td>\n",
       "      <td>love movie good job director appreciate work g...</td>\n",
       "      <td>love movie good job director appreciate work g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREAT MOVIE!!  Same as the original, but the c...</td>\n",
       "      <td>great movie!!  same as the original, but the c...</td>\n",
       "      <td>great movie original cinematography huge diffe...</td>\n",
       "      <td>great movie original cinematography huge diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Realistic.   Fantastic special effects</td>\n",
       "      <td>realistic.   fantastic special effects</td>\n",
       "      <td>realistic fantastic special effect</td>\n",
       "      <td>realistic fantastic special effect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Was good. Nothing like the original but I beli...   \n",
       "1  I absolutely loved it! A wonderful rendition o...   \n",
       "2  I love the movie! Good job director! \\nI appre...   \n",
       "3  GREAT MOVIE!!  Same as the original, but the c...   \n",
       "4             Realistic.   Fantastic special effects   \n",
       "\n",
       "                                spll_contr_accnt_NOT  \\\n",
       "0  was good. nothing like the original but i beli...   \n",
       "1  i absolutely loved it! a wonderful rendition o...   \n",
       "2  i love the movie! good job director! \\ni appre...   \n",
       "3  great movie!!  same as the original, but the c...   \n",
       "4             realistic.   fantastic special effects   \n",
       "\n",
       "                                       antonymns_upd  \\\n",
       "0                   good like original believe point   \n",
       "1  absolutely love wonderful rendition original n...   \n",
       "2  love movie good job director appreciate work g...   \n",
       "3  great movie original cinematography huge diffe...   \n",
       "4                 realistic fantastic special effect   \n",
       "\n",
       "                                lemma_stop_2len_word  \n",
       "0                   good like original believe point  \n",
       "1  absolutely love wonderful rendition original n...  \n",
       "2  love movie good job director appreciate work g...  \n",
       "3  great movie original cinematography huge diffe...  \n",
       "4                 realistic fantastic special effect  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ultimate_TestData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    total_data.drop([clean_after_lemma_n_stop_2len_word,clean_spll_contr_accnt_NOT],axis=1,inplace=True)\n",
    "    Ultimate_TestData.drop([clean_after_lemma_n_stop_2len_word,clean_spll_contr_accnt_NOT],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                       antonymns_upd  \n",
       "0                    like animation animal look real  \n",
       "1                 amazing realistic incredible music  \n",
       "2        classic good remake love glover outstanding  \n",
       "3  nice animation cgi completely lack disney styl...  \n",
       "4                      good mainly sentimental value  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>antonymns_upd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was good. Nothing like the original but I beli...</td>\n",
       "      <td>good like original believe point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I absolutely loved it! A wonderful rendition o...</td>\n",
       "      <td>absolutely love wonderful rendition original n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the movie! Good job director! \\nI appre...</td>\n",
       "      <td>love movie good job director appreciate work g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREAT MOVIE!!  Same as the original, but the c...</td>\n",
       "      <td>great movie original cinematography huge diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Realistic.   Fantastic special effects</td>\n",
       "      <td>realistic fantastic special effect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Was good. Nothing like the original but I beli...   \n",
       "1  I absolutely loved it! A wonderful rendition o...   \n",
       "2  I love the movie! Good job director! \\nI appre...   \n",
       "3  GREAT MOVIE!!  Same as the original, but the c...   \n",
       "4             Realistic.   Fantastic special effects   \n",
       "\n",
       "                                       antonymns_upd  \n",
       "0                   good like original believe point  \n",
       "1  absolutely love wonderful rendition original n...  \n",
       "2  love movie good job director appreciate work g...  \n",
       "3  great movie original cinematography huge diffe...  \n",
       "4                 realistic fantastic special effect  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ultimate_TestData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the POS tags and counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Getting ADJ tags and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_to_keep_and_count(dataframe,column_name,dest_column_name,dest_count_coulmn_name,Take_Count=False,\n",
    "pos_to_keep_and_count(dataframe=total_data,column_name=clean_after_antonymns,dest_column_name=adj_text,dest_count_coulmn_name=adj_count,Take_Count=True,x1='ADJ')\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "pos_to_keep_and_count(dataframe=Ultimate_TestData,column_name=clean_after_antonymns,dest_column_name=adj_text,dest_count_coulmn_name=adj_count,Take_Count=True,x1='ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adj_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>like real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>amazing realistic incredible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>classic good love outstanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>nice humour original undue real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good sentimental</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                       antonymns_upd  adj_count  \\\n",
       "0                    like animation animal look real        2.0   \n",
       "1                 amazing realistic incredible music        3.0   \n",
       "2        classic good remake love glover outstanding        4.0   \n",
       "3  nice animation cgi completely lack disney styl...        5.0   \n",
       "4                      good mainly sentimental value        2.0   \n",
       "\n",
       "                          adj_text  \n",
       "0                        like real  \n",
       "1     amazing realistic incredible  \n",
       "2    classic good love outstanding  \n",
       "3  nice humour original undue real  \n",
       "4                 good sentimental  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Getting ADV,VERB tags and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_keep_and_count(dataframe=total_data,column_name=clean_after_antonymns,dest_column_name=adv_verb_text,dest_count_coulmn_name=adv_verb_count,Take_Count=True,x1='ADV',x2='VERB')\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "pos_to_keep_and_count(dataframe=Ultimate_TestData,column_name=clean_after_antonymns,dest_column_name=adv_verb_text,dest_count_coulmn_name=adv_verb_count,Take_Count=True,x1='ADV',x2='VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adj_text</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>adv_verb_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>like real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>like look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>amazing realistic incredible</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>classic good love outstanding</td>\n",
       "      <td>1.0</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>nice humour original undue real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>completely lack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good sentimental</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mainly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                       antonymns_upd  adj_count  \\\n",
       "0                    like animation animal look real        2.0   \n",
       "1                 amazing realistic incredible music        3.0   \n",
       "2        classic good remake love glover outstanding        4.0   \n",
       "3  nice animation cgi completely lack disney styl...        5.0   \n",
       "4                      good mainly sentimental value        2.0   \n",
       "\n",
       "                          adj_text  adv_verb_count    adv_verb_text  \n",
       "0                        like real             2.0        like look  \n",
       "1     amazing realistic incredible             0.0                   \n",
       "2    classic good love outstanding             1.0             love  \n",
       "3  nice humour original undue real             2.0  completely lack  \n",
       "4                 good sentimental             1.0           mainly  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting polarity of review comments, adj text, adv-verb text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_polarity(dataframe,column_name,dest_column_name,MULT_Factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_polarity(total_data,review,review_polar,1)\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "review_polarity(Ultimate_TestData,review,review_polar,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_polarity(total_data,adj_text,adj_polar,1)\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "review_polarity(Ultimate_TestData,adj_text,adj_polar,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_polarity(total_data,adv_verb_text,adv_verb_polar,1)\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "review_polarity(Ultimate_TestData,adv_verb_text,adv_verb_polar,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adj_text</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>adv_verb_text</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>like real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>like look</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>amazing realistic incredible</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>classic good love outstanding</td>\n",
       "      <td>1.0</td>\n",
       "      <td>love</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>nice humour original undue real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>completely lack</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good sentimental</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mainly</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                       antonymns_upd  adj_count  \\\n",
       "0                    like animation animal look real        2.0   \n",
       "1                 amazing realistic incredible music        3.0   \n",
       "2        classic good remake love glover outstanding        4.0   \n",
       "3  nice animation cgi completely lack disney styl...        5.0   \n",
       "4                      good mainly sentimental value        2.0   \n",
       "\n",
       "                          adj_text  adv_verb_count    adv_verb_text  \\\n",
       "0                        like real             2.0        like look   \n",
       "1     amazing realistic incredible             0.0                    \n",
       "2    classic good love outstanding             1.0             love   \n",
       "3  nice humour original undue real             2.0  completely lack   \n",
       "4                 good sentimental             1.0           mainly   \n",
       "\n",
       "   review_polar  adj_polar  adv_verb_polar  \n",
       "0      0.433333   0.200000        0.000000  \n",
       "1      0.605556   0.555556        0.000000  \n",
       "2      0.516667   0.466667        0.500000  \n",
       "3      0.318750   0.391667        0.100000  \n",
       "4      0.205556   0.225000        0.166667  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adj_text</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>adv_verb_text</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was good. Nothing like the original but I beli...</td>\n",
       "      <td>good like original believe point</td>\n",
       "      <td>3.0</td>\n",
       "      <td>good like original</td>\n",
       "      <td>1.0</td>\n",
       "      <td>like</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I absolutely loved it! A wonderful rendition o...</td>\n",
       "      <td>absolutely love wonderful rendition original n...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>love wonderful original like</td>\n",
       "      <td>6.0</td>\n",
       "      <td>absolutely love not compare let like</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the movie! Good job director! \\nI appre...</td>\n",
       "      <td>love movie good job director appreciate work g...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>love good great</td>\n",
       "      <td>2.0</td>\n",
       "      <td>love work</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREAT MOVIE!!  Same as the original, but the c...</td>\n",
       "      <td>great movie original cinematography huge diffe...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>great original huge</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Realistic.   Fantastic special effects</td>\n",
       "      <td>realistic fantastic special effect</td>\n",
       "      <td>3.0</td>\n",
       "      <td>realistic fantastic special</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Was good. Nothing like the original but I beli...   \n",
       "1  I absolutely loved it! A wonderful rendition o...   \n",
       "2  I love the movie! Good job director! \\nI appre...   \n",
       "3  GREAT MOVIE!!  Same as the original, but the c...   \n",
       "4             Realistic.   Fantastic special effects   \n",
       "\n",
       "                                       antonymns_upd  adj_count  \\\n",
       "0                   good like original believe point        3.0   \n",
       "1  absolutely love wonderful rendition original n...        4.0   \n",
       "2  love movie good job director appreciate work g...        3.0   \n",
       "3  great movie original cinematography huge diffe...        3.0   \n",
       "4                 realistic fantastic special effect        3.0   \n",
       "\n",
       "                       adj_text  adv_verb_count  \\\n",
       "0            good like original             1.0   \n",
       "1  love wonderful original like             6.0   \n",
       "2               love good great             2.0   \n",
       "3           great original huge             0.0   \n",
       "4   realistic fantastic special             0.0   \n",
       "\n",
       "                          adv_verb_text  review_polar  adj_polar  \\\n",
       "0                                  like      0.537500   0.537500   \n",
       "1  absolutely love not compare let like      0.712500   0.625000   \n",
       "2                             love work      0.766667   0.666667   \n",
       "3                                            0.500000   0.525000   \n",
       "4                                            0.307937   0.307937   \n",
       "\n",
       "   adv_verb_polar  \n",
       "0             0.0  \n",
       "1             0.5  \n",
       "2             0.5  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ultimate_TestData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the length, words and special chars presence from review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = Spl_Chars_Counts()\n",
    "df_extra_feat = tc.fit_transform(total_data[review])\n",
    "\n",
    "## Repeat the same for the final test data\n",
    "tc_FinalOut = Spl_Chars_Counts()\n",
    "df_extra_feat_FinalOut = tc_FinalOut.fit_transform(Ultimate_TestData[review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_capital_words  count_love  count_shit  \\\n",
       "0           12                    0           0           0   \n",
       "1            6                    0           2           0   \n",
       "2            8                    0           2           1   \n",
       "3           27                    1           0           1   \n",
       "4            8                    0           0           0   \n",
       "\n",
       "   count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                  0                 0                0             0  \n",
       "1                  0                 1                0             0  \n",
       "2                  0                 0                0             0  \n",
       "3                  1                 0                0             0  \n",
       "4                  0                 0                0             0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extra_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_capital_words  count_love  count_shit  \\\n",
       "0           13                    0           0           0   \n",
       "1           28                    0           2           0   \n",
       "2           20                    0           2           0   \n",
       "3           13                    2           1           0   \n",
       "4            4                    0           1           0   \n",
       "\n",
       "   count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                  0                 0                0             0  \n",
       "1                  0                 1                0             0  \n",
       "2                  0                 2                0             0  \n",
       "3                  0                 4                0             0  \n",
       "4                  0                 0                0             0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extra_feat_FinalOut.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make the final data frame after feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([total_data,df_extra_feat],axis=1)\n",
    "\n",
    "FinalOut_final_data = pd.concat([Ultimate_TestData,df_extra_feat_FinalOut],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adj_text</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>adv_verb_text</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I liked most that the animation made the anima...</td>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>like real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>like look</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing! So realistic and incredible music</td>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>amazing realistic incredible</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classic. Good remake. Loved it. Glover was out...</td>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>classic good love outstanding</td>\n",
       "      <td>1.0</td>\n",
       "      <td>love</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice animation/CGI but completely lacking the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>nice humour original undue real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>completely lack</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good but go mainly for the sentimental value</td>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good sentimental</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mainly</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  targetSentiment  \\\n",
       "0  I liked most that the animation made the anima...                0   \n",
       "1         Amazing! So realistic and incredible music                0   \n",
       "2  Classic. Good remake. Loved it. Glover was out...                0   \n",
       "3  Nice animation/CGI but completely lacking the ...                1   \n",
       "4       Good but go mainly for the sentimental value                0   \n",
       "\n",
       "                                       antonymns_upd  adj_count  \\\n",
       "0                    like animation animal look real        2.0   \n",
       "1                 amazing realistic incredible music        3.0   \n",
       "2        classic good remake love glover outstanding        4.0   \n",
       "3  nice animation cgi completely lack disney styl...        5.0   \n",
       "4                      good mainly sentimental value        2.0   \n",
       "\n",
       "                          adj_text  adv_verb_count    adv_verb_text  \\\n",
       "0                        like real             2.0        like look   \n",
       "1     amazing realistic incredible             0.0                    \n",
       "2    classic good love outstanding             1.0             love   \n",
       "3  nice humour original undue real             2.0  completely lack   \n",
       "4                 good sentimental             1.0           mainly   \n",
       "\n",
       "   review_polar  adj_polar  adv_verb_polar  count_words  count_capital_words  \\\n",
       "0      0.433333   0.200000        0.000000           12                    0   \n",
       "1      0.605556   0.555556        0.000000            6                    0   \n",
       "2      0.516667   0.466667        0.500000            8                    0   \n",
       "3      0.318750   0.391667        0.100000           27                    1   \n",
       "4      0.205556   0.225000        0.166667            8                    0   \n",
       "\n",
       "   count_love  count_shit  count_quest_marks  count_excl_marks  \\\n",
       "0           0           0                  0                 0   \n",
       "1           2           0                  0                 1   \n",
       "2           2           1                  0                 0   \n",
       "3           0           1                  1                 0   \n",
       "4           0           0                  0                 0   \n",
       "\n",
       "   count_NOT_words  count_emojis  \n",
       "0                0             0  \n",
       "1                0             0  \n",
       "2                0             0  \n",
       "3                0             0  \n",
       "4                0             0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adj_text</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>adv_verb_text</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was good. Nothing like the original but I beli...</td>\n",
       "      <td>good like original believe point</td>\n",
       "      <td>3.0</td>\n",
       "      <td>good like original</td>\n",
       "      <td>1.0</td>\n",
       "      <td>like</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I absolutely loved it! A wonderful rendition o...</td>\n",
       "      <td>absolutely love wonderful rendition original n...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>love wonderful original like</td>\n",
       "      <td>6.0</td>\n",
       "      <td>absolutely love not compare let like</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the movie! Good job director! \\nI appre...</td>\n",
       "      <td>love movie good job director appreciate work g...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>love good great</td>\n",
       "      <td>2.0</td>\n",
       "      <td>love work</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREAT MOVIE!!  Same as the original, but the c...</td>\n",
       "      <td>great movie original cinematography huge diffe...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>great original huge</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Realistic.   Fantastic special effects</td>\n",
       "      <td>realistic fantastic special effect</td>\n",
       "      <td>3.0</td>\n",
       "      <td>realistic fantastic special</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Was good. Nothing like the original but I beli...   \n",
       "1  I absolutely loved it! A wonderful rendition o...   \n",
       "2  I love the movie! Good job director! \\nI appre...   \n",
       "3  GREAT MOVIE!!  Same as the original, but the c...   \n",
       "4             Realistic.   Fantastic special effects   \n",
       "\n",
       "                                       antonymns_upd  adj_count  \\\n",
       "0                   good like original believe point        3.0   \n",
       "1  absolutely love wonderful rendition original n...        4.0   \n",
       "2  love movie good job director appreciate work g...        3.0   \n",
       "3  great movie original cinematography huge diffe...        3.0   \n",
       "4                 realistic fantastic special effect        3.0   \n",
       "\n",
       "                       adj_text  adv_verb_count  \\\n",
       "0            good like original             1.0   \n",
       "1  love wonderful original like             6.0   \n",
       "2               love good great             2.0   \n",
       "3           great original huge             0.0   \n",
       "4   realistic fantastic special             0.0   \n",
       "\n",
       "                          adv_verb_text  review_polar  adj_polar  \\\n",
       "0                                  like      0.537500   0.537500   \n",
       "1  absolutely love not compare let like      0.712500   0.625000   \n",
       "2                             love work      0.766667   0.666667   \n",
       "3                                            0.500000   0.525000   \n",
       "4                                            0.307937   0.307937   \n",
       "\n",
       "   adv_verb_polar  count_words  count_capital_words  count_love  count_shit  \\\n",
       "0             0.0           13                    0           0           0   \n",
       "1             0.5           28                    0           2           0   \n",
       "2             0.5           20                    0           2           0   \n",
       "3             0.0           13                    2           1           0   \n",
       "4             0.0            4                    0           1           0   \n",
       "\n",
       "   count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                  0                 0                0             0  \n",
       "1                  0                 1                0             0  \n",
       "2                  0                 2                0             0  \n",
       "3                  0                 4                0             0  \n",
       "4                  0                 0                0             0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalOut_final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing unecessary text data columns : adj_text,  adv_verb_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    final_data.drop([adj_text,adv_verb_text,review],axis=1,inplace=True)\n",
    "    FinalOut_final_data.drop([adj_text,adv_verb_text,review],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   targetSentiment                                      antonymns_upd  \\\n",
       "0                0                    like animation animal look real   \n",
       "1                0                 amazing realistic incredible music   \n",
       "2                0        classic good remake love glover outstanding   \n",
       "3                1  nice animation cgi completely lack disney styl...   \n",
       "4                0                      good mainly sentimental value   \n",
       "\n",
       "   adj_count  adv_verb_count  review_polar  adj_polar  adv_verb_polar  \\\n",
       "0        2.0             2.0      0.433333   0.200000        0.000000   \n",
       "1        3.0             0.0      0.605556   0.555556        0.000000   \n",
       "2        4.0             1.0      0.516667   0.466667        0.500000   \n",
       "3        5.0             2.0      0.318750   0.391667        0.100000   \n",
       "4        2.0             1.0      0.205556   0.225000        0.166667   \n",
       "\n",
       "   count_words  count_capital_words  count_love  count_shit  \\\n",
       "0           12                    0           0           0   \n",
       "1            6                    0           2           0   \n",
       "2            8                    0           2           1   \n",
       "3           27                    1           0           1   \n",
       "4            8                    0           0           0   \n",
       "\n",
       "   count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                  0                 0                0             0  \n",
       "1                  0                 1                0             0  \n",
       "2                  0                 0                0             0  \n",
       "3                  1                 0                0             0  \n",
       "4                  0                 0                0             0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 copies of data\n",
    "### 1. for having non-lexical features\n",
    "### 2. for having text + non-lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_nontext = final_data.drop([clean_after_antonymns],axis=1)\n",
    "FinalOut_final_data_nontext = FinalOut_final_data.drop([clean_after_antonymns],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_text = final_data.copy(deep=True)\n",
    "FinalOut_final_data_text = FinalOut_final_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good like original believe point</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolutely love wonderful rendition original n...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love movie good job director appreciate work g...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great movie original cinematography huge diffe...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realistic fantastic special effect</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       antonymns_upd  adj_count  \\\n",
       "0                   good like original believe point        3.0   \n",
       "1  absolutely love wonderful rendition original n...        4.0   \n",
       "2  love movie good job director appreciate work g...        3.0   \n",
       "3  great movie original cinematography huge diffe...        3.0   \n",
       "4                 realistic fantastic special effect        3.0   \n",
       "\n",
       "   adv_verb_count  review_polar  adj_polar  adv_verb_polar  count_words  \\\n",
       "0             1.0      0.537500   0.537500             0.0           13   \n",
       "1             6.0      0.712500   0.625000             0.5           28   \n",
       "2             2.0      0.766667   0.666667             0.5           20   \n",
       "3             0.0      0.500000   0.525000             0.0           13   \n",
       "4             0.0      0.307937   0.307937             0.0            4   \n",
       "\n",
       "   count_capital_words  count_love  count_shit  count_quest_marks  \\\n",
       "0                    0           0           0                  0   \n",
       "1                    0           2           0                  0   \n",
       "2                    0           2           0                  0   \n",
       "3                    2           1           0                  0   \n",
       "4                    0           1           0                  0   \n",
       "\n",
       "   count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                 0                0             0  \n",
       "1                 1                0             0  \n",
       "2                 2                0             0  \n",
       "3                 4                0             0  \n",
       "4                 0                0             0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalOut_final_data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good like original believe point</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absolutely love wonderful rendition original n...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love movie good job director appreciate work g...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great movie original cinematography huge diffe...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realistic fantastic special effect</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.307937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       antonymns_upd  adj_count  \\\n",
       "0                   good like original believe point        3.0   \n",
       "1  absolutely love wonderful rendition original n...        4.0   \n",
       "2  love movie good job director appreciate work g...        3.0   \n",
       "3  great movie original cinematography huge diffe...        3.0   \n",
       "4                 realistic fantastic special effect        3.0   \n",
       "\n",
       "   adv_verb_count  review_polar  adj_polar  adv_verb_polar  count_words  \\\n",
       "0             1.0      0.537500   0.537500             0.0           13   \n",
       "1             6.0      0.712500   0.625000             0.5           28   \n",
       "2             2.0      0.766667   0.666667             0.5           20   \n",
       "3             0.0      0.500000   0.525000             0.0           13   \n",
       "4             0.0      0.307937   0.307937             0.0            4   \n",
       "\n",
       "   count_capital_words  count_love  count_shit  count_quest_marks  \\\n",
       "0                    0           0           0                  0   \n",
       "1                    0           2           0                  0   \n",
       "2                    0           2           0                  0   \n",
       "3                    2           1           0                  0   \n",
       "4                    0           1           0                  0   \n",
       "\n",
       "   count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                 0                0             0  \n",
       "1                 1                0             0  \n",
       "2                 2                0             0  \n",
       "3                 4                0             0  \n",
       "4                 0                0             0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalOut_final_data_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame to store all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = pd.DataFrame(columns=['Model_Name','Train_Accuracy','F1_0Class','F1_1Class','Test_Accuracy','F1_0Class_test','F1_1Class_test','Features','Comments'])\n",
    "model_summary_index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>F1_0Class</th>\n",
       "      <th>F1_1Class</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>F1_0Class_test</th>\n",
       "      <th>F1_1Class_test</th>\n",
       "      <th>Features</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model_Name, Train_Accuracy, F1_0Class, F1_1Class, Test_Accuracy, F1_0Class_test, F1_1Class_test, Features, Comments]\n",
       "Index: []"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML models to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Logistic Regression\n",
    "\n",
    "def logistic_regr(X_train_sm,y_train_sm,X_test,C_list,Penalty,Features_name,TESTDATA=True):\n",
    "    global model_summary_index\n",
    "    for C in C_list:\n",
    "        for penalty in Penalty:\n",
    "            logisticRegr = LogisticRegression(penalty = penalty, C = C,random_state = 0)\n",
    "            logisticRegr.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "            train_predicted_classes = logisticRegr.predict(X_train_sm)\n",
    "            train_accuracy = accuracy_score(y_train_sm,train_predicted_classes)\n",
    "\n",
    "\n",
    "            test_predicted_classes = logisticRegr.predict(X_test)\n",
    "\n",
    "            train_accuracy = accuracy_score(y_train_sm,train_predicted_classes)\n",
    "            train_f1_0 = f1_score(y_train_sm,train_predicted_classes,pos_label=0)\n",
    "            train_f1_1 = f1_score(y_train_sm,train_predicted_classes,pos_label=1)\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            print(\"-------------------------------------------------------------------------------------\")\n",
    "            print(\"C : \",C, \"Penalty : \",penalty)\n",
    "            print(\"TRAIN DATA ACCURACY\",train_accuracy)\n",
    "            print(\"\\nTrain data f1-score for class '0'\",train_f1_0)\n",
    "            print(\"\\nTrain data f1-score for class '1'\",train_f1_1)\n",
    "            \n",
    "            model_summary.loc[model_summary_index,'Model_Name']='Logistic'\n",
    "            model_summary.loc[model_summary_index,'Train_Accuracy']=train_accuracy\n",
    "\n",
    "            model_summary.loc[model_summary_index,'F1_0Class']=train_f1_0\n",
    "            model_summary.loc[model_summary_index,'F1_1Class']=train_f1_1\n",
    "\n",
    "            if(TESTDATA==True):\n",
    "                test_accuracy = accuracy_score(y_test,test_predicted_classes)\n",
    "                test_f1_0 = f1_score(y_test,test_predicted_classes,pos_label=0)\n",
    "                test_f1_1 = f1_score(y_test,test_predicted_classes,pos_label=1)\n",
    "            \n",
    "                print(\"TEST DATA ACCURACY\",test_accuracy)\n",
    "                print(\"\\nTest data f1-score for class '0'\",test_f1_0)\n",
    "                print(\"\\nTest data f1-score for class '1'\",test_f1_1)\n",
    "\n",
    "                model_summary.loc[model_summary_index,'Test_Accuracy'] = test_accuracy\n",
    "                model_summary.loc[model_summary_index,'F1_0Class_test']=test_f1_0\n",
    "                model_summary.loc[model_summary_index,'F1_1Class_test']=test_f1_1\n",
    "                                     \n",
    "\n",
    "            model_summary.loc[model_summary_index,'Features'] = Features_name\n",
    "            model_summary.loc[model_summary_index,'Comments'] = \" C : \" + str(C) + \"| Penalty : \" + str(penalty)\n",
    "\n",
    "            model_summary_index=model_summary_index+1\n",
    "    \n",
    "    #return the last one as default\n",
    "    return logisticRegr\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "## Function for XGB model\n",
    "\n",
    "def xgb_model(X_train_sm,y_train_sm,X_test,param_grid,Features_name,TESTDATA=True):\n",
    "    global model_summary_index\n",
    "    \n",
    "    XGB = XGBClassifier(n_jobs=-1)\n",
    "    CV_XGB = GridSearchCV(estimator=XGB, param_grid=param_grid, cv= 10)\n",
    "\n",
    "\n",
    "    CV_XGB.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "    train_predictions_xgb =  CV_XGB.predict(X_train_sm)\n",
    "    test_predictions_xgb =  CV_XGB.predict(X_test.values)\n",
    "                                     \n",
    "    train_accuracy = accuracy_score(y_train_sm,train_predictions_xgb)\n",
    "    train_f1_0 = f1_score(y_train_sm,train_predictions_xgb,pos_label=0)\n",
    "    train_f1_1 = f1_score(y_train_sm,train_predictions_xgb,pos_label=1)\n",
    " \n",
    "\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(\"CV_XGB.best_score_ \",CV_XGB.best_score_)\n",
    "    print(\"CV_XGB.best_params_ \",CV_XGB.best_params_)\n",
    "\n",
    "    print(\"TRAIN DATA ACCURACY\",train_accuracy)\n",
    "    print(\"\\nTrain data f1-score for class '0'\",train_f1_0)\n",
    "    print(\"\\nTrain data f1-score for class '1'\",train_f1_1)\n",
    "\n",
    "                                     \n",
    "    model_summary.loc[model_summary_index,'Model_Name']='XGBoost'\n",
    "    model_summary.loc[model_summary_index,'Train_Accuracy']=train_accuracy\n",
    "\n",
    "    model_summary.loc[model_summary_index,'F1_0Class']=train_f1_0\n",
    "    model_summary.loc[model_summary_index,'F1_1Class']=train_f1_1\n",
    "                                     \n",
    "    if(TESTDATA==True):\n",
    "                                     \n",
    "        test_accuracy = accuracy_score(y_test,test_predictions_xgb)\n",
    "        test_f1_0 = f1_score(y_test,test_predictions_xgb,pos_label=0)\n",
    "        test_f1_1 = f1_score(y_test,test_predictions_xgb,pos_label=1)\n",
    "        print(\"TEST DATA ACCURACY\",test_accuracy)\n",
    "        print(\"\\nTest data f1-score for class '0'\",test_f1_0)\n",
    "        print(\"\\nTest data f1-score for class '1'\",test_f1_1)\n",
    "                                     \n",
    "        model_summary.loc[model_summary_index,'Test_Accuracy'] = test_accuracy\n",
    "        model_summary.loc[model_summary_index,'F1_0Class_test']= test_f1_0\n",
    "        model_summary.loc[model_summary_index,'F1_1Class_test']= test_f1_1\n",
    "\n",
    "    model_summary.loc[model_summary_index,'Features'] = Features_name\n",
    "    model_summary.loc[model_summary_index,'Comments'] = \" Best_Score_ : \" + str(CV_XGB.best_score_) + \"| Best_Param_ : \" + str(CV_XGB.best_params_)\n",
    "\n",
    "    model_summary_index=model_summary_index+1\n",
    "    \n",
    "    return CV_XGB\n",
    "    \n",
    "\n",
    "## Function for RFC model\n",
    "def rfc_model(X_train_sm,y_train_sm,X_test,param_grid,Features_name,TESTDATA=True):\n",
    "    global model_summary_index\n",
    "    \n",
    "    rfc_grid = RandomForestClassifier(n_jobs=-1, max_features='sqrt')\n",
    "    rfc_cv_grid = RandomizedSearchCV(estimator = rfc_grid, param_distributions = param_grid, cv = 3, n_iter=10)\n",
    "\n",
    "    rfc_cv_grid.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "    train_predictions_rf = rfc_cv_grid.predict(X_train_sm)\n",
    "    test_predictions_rf = rfc_cv_grid.predict(X_test)\n",
    "                             \n",
    "    train_accuracy = accuracy_score(y_train_sm,train_predictions_rf)\n",
    "    train_f1_0 = f1_score(y_train_sm,train_predictions_rf,pos_label=0)\n",
    "    train_f1_1 = f1_score(y_train_sm,train_predictions_rf,pos_label=1)\n",
    "\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    print(\"rfc_cv_grid.best_score_ \",rfc_cv_grid.best_score_)\n",
    "    print(\"rfc_cv_grid.best_params_ \",rfc_cv_grid.best_params_)\n",
    "\n",
    "    print(\"TRAIN DATA ACCURACY\",train_accuracy)\n",
    "    print(\"\\nTrain data f1-score for class '0'\",train_f1_0)\n",
    "    print(\"\\nTrain data f1-score for class '1'\",train_f1_1)\n",
    "                             \n",
    "    model_summary.loc[model_summary_index,'Model_Name']='RandomForest'\n",
    "    model_summary.loc[model_summary_index,'Train_Accuracy']=train_accuracy\n",
    "\n",
    "    model_summary.loc[model_summary_index,'F1_0Class']=train_f1_0\n",
    "    model_summary.loc[model_summary_index,'F1_1Class']=train_f1_1\n",
    "\n",
    "    if(TESTDATA==True):\n",
    "        test_accuracy = accuracy_score(y_test,test_predictions_rf)\n",
    "        test_f1_0 =f1_score(y_test,test_predictions_rf,pos_label=0)\n",
    "        test_f1_1 =f1_score(y_test,test_predictions_rf,pos_label=1)\n",
    "                             \n",
    "        print(\"TEST DATA ACCURACY\",test_accuracy)\n",
    "        print(\"\\nTest data f1-score for class '0'\",test_f1_0)\n",
    "        print(\"\\nTest data f1-score for class '1'\",test_f1_1)\n",
    "\n",
    "        model_summary.loc[model_summary_index,'Test_Accuracy'] = test_accuracy\n",
    "        model_summary.loc[model_summary_index,'F1_0Class_test']=test_f1_0\n",
    "        model_summary.loc[model_summary_index,'F1_1Class_test']=test_f1_1\n",
    "\n",
    "    model_summary.loc[model_summary_index,'Features'] = Features_name\n",
    "    model_summary.loc[model_summary_index,'Comments'] = \" Best_Score_ : \" + str(rfc_cv_grid.best_score_) + \"| Best_Param_ : \" + str(rfc_cv_grid.best_params_)\n",
    "\n",
    "    model_summary_index=model_summary_index+1  \n",
    "\n",
    "    return rfc_cv_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model building with non-text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   targetSentiment  adj_count  adv_verb_count  review_polar  adj_polar  \\\n",
       "0                0        2.0             2.0      0.433333   0.200000   \n",
       "1                0        3.0             0.0      0.605556   0.555556   \n",
       "2                0        4.0             1.0      0.516667   0.466667   \n",
       "3                1        5.0             2.0      0.318750   0.391667   \n",
       "4                0        2.0             1.0      0.205556   0.225000   \n",
       "\n",
       "   adv_verb_polar  count_words  count_capital_words  count_love  count_shit  \\\n",
       "0        0.000000           12                    0           0           0   \n",
       "1        0.000000            6                    0           2           0   \n",
       "2        0.500000            8                    0           2           1   \n",
       "3        0.100000           27                    1           0           1   \n",
       "4        0.166667            8                    0           0           0   \n",
       "\n",
       "   count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                  0                 0                0             0  \n",
       "1                  0                 1                0             0  \n",
       "2                  0                 0                0             0  \n",
       "3                  1                 0                0             0  \n",
       "4                  0                 0                0             0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_nontext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_data_nontext.drop([targetSentiment], axis = 1)\n",
    "y = y = final_data_nontext[targetSentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "## For future use\n",
    "X_train_ntext_fut = X_train.copy(deep=True)\n",
    "X_test_ntext_fut = X_test.copy(deep=True)\n",
    "\n",
    "y_train_ntext_fut = y_train.copy(deep=True)\n",
    "y_test_ntext_fut = y_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE the train data as there is data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (2400, 13)\n",
      "y_train.shape = (2400,)\n",
      "X_train_sm.shape = (3468, 13)\n",
      "y_train_sm.shape = (3468,)\n",
      "sum(y_train_sm) = 1734\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train.shape = {X_train.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")\n",
    "print(f\"X_train_sm.shape = {X_train_sm.shape}\")\n",
    "print(f\"y_train_sm.shape = {y_train_sm.shape}\")\n",
    "print(f\"sum(y_train_sm) = {sum(y_train_sm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.5\n",
      "\n",
      "Train data f1-score for class '0' 0.0\n",
      "\n",
      "Train data f1-score for class '1' 0.6666666666666666\n",
      "TEST DATA ACCURACY 0.27\n",
      "\n",
      "Test data f1-score for class '0' 0.0\n",
      "\n",
      "Test data f1-score for class '1' 0.4251968503937008\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7696078431372549\n",
      "\n",
      "Train data f1-score for class '0' 0.7614213197969544\n",
      "\n",
      "Train data f1-score for class '1' 0.7772511848341231\n",
      "TEST DATA ACCURACY 0.7833333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8391089108910891\n",
      "\n",
      "Test data f1-score for class '1' 0.6683673469387756\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7632641291810842\n",
      "\n",
      "Train data f1-score for class '0' 0.7578885284576821\n",
      "\n",
      "Train data f1-score for class '1' 0.7684062059238365\n",
      "TEST DATA ACCURACY 0.7766666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8337468982630272\n",
      "\n",
      "Test data f1-score for class '1' 0.6598984771573604\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7724913494809689\n",
      "\n",
      "Train data f1-score for class '0' 0.7685538281020827\n",
      "\n",
      "Train data f1-score for class '1' 0.7762971363765239\n",
      "TEST DATA ACCURACY 0.7966666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8493827160493828\n",
      "\n",
      "Test data f1-score for class '1' 0.6871794871794873\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7797001153402537\n",
      "\n",
      "Train data f1-score for class '0' 0.7768691588785046\n",
      "\n",
      "Train data f1-score for class '1' 0.7824601366742596\n",
      "TEST DATA ACCURACY 0.8083333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.857849196538937\n",
      "\n",
      "Test data f1-score for class '1' 0.7058823529411764\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.779123414071511\n",
      "\n",
      "Train data f1-score for class '0' 0.776285046728972\n",
      "\n",
      "Train data f1-score for class '1' 0.7818906605922551\n",
      "TEST DATA ACCURACY 0.8083333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.857849196538937\n",
      "\n",
      "Test data f1-score for class '1' 0.7058823529411764\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7768166089965398\n",
      "\n",
      "Train data f1-score for class '0' 0.7734192037470726\n",
      "\n",
      "Train data f1-score for class '1' 0.7801136363636362\n",
      "TEST DATA ACCURACY 0.815\n",
      "\n",
      "Test data f1-score for class '0' 0.8634686346863468\n",
      "\n",
      "Test data f1-score for class '1' 0.7131782945736435\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7765282583621684\n",
      "\n",
      "Train data f1-score for class '0' 0.7733255337818075\n",
      "\n",
      "Train data f1-score for class '1' 0.7796417401194199\n",
      "TEST DATA ACCURACY 0.8166666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.8645320197044336\n",
      "\n",
      "Test data f1-score for class '1' 0.7164948453608248\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7756632064590542\n",
      "\n",
      "Train data f1-score for class '0' 0.77211482132396\n",
      "\n",
      "Train data f1-score for class '1' 0.7791027825099375\n",
      "TEST DATA ACCURACY 0.8133333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8620689655172414\n",
      "\n",
      "Test data f1-score for class '1' 0.7113402061855669\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7753748558246828\n",
      "\n",
      "Train data f1-score for class '0' 0.7718887262079063\n",
      "\n",
      "Train data f1-score for class '1' 0.7787560352172678\n",
      "TEST DATA ACCURACY 0.8133333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8620689655172414\n",
      "\n",
      "Test data f1-score for class '1' 0.7113402061855669\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7756632064590542\n",
      "\n",
      "Train data f1-score for class '0' 0.77211482132396\n",
      "\n",
      "Train data f1-score for class '1' 0.7791027825099375\n",
      "TEST DATA ACCURACY 0.8133333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8620689655172414\n",
      "\n",
      "Test data f1-score for class '1' 0.7113402061855669\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7753748558246828\n",
      "\n",
      "Train data f1-score for class '0' 0.7718887262079063\n",
      "\n",
      "Train data f1-score for class '1' 0.7787560352172678\n",
      "TEST DATA ACCURACY 0.8133333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8620689655172414\n",
      "\n",
      "Test data f1-score for class '1' 0.7113402061855669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_list = [0.001,0.01,0.1,1,10,100]\n",
    "Penalty = ['l1','l2']\n",
    "\n",
    "logistic_regr(X_train_sm,y_train_sm,X_test,C_list,Penalty,Features_name=\"Non-Text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trex\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "CV_XGB.best_score_  0.8526528258362168\n",
      "CV_XGB.best_params_  {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}\n",
      "TRAIN DATA ACCURACY 0.9873125720876585\n",
      "\n",
      "Train data f1-score for class '0' 0.9873997709049255\n",
      "\n",
      "Train data f1-score for class '1' 0.9872241579558653\n",
      "TEST DATA ACCURACY 0.7933333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8571428571428571\n",
      "\n",
      "Test data f1-score for class '1' 0.6265060240963856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=-1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'colsample_bytree': array([0.5, 0.6, 0.7, 0.8, 0.9]), 'n_estimators': [100], 'max_depth': [10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a grid over parameters of interest\n",
    "param_grid = {\n",
    "    'colsample_bytree': np.linspace(0.5, 0.9, 5),\n",
    "    'n_estimators':[100],\n",
    "    'max_depth': [10]\n",
    "}\n",
    "\n",
    "xgb_model(X_train_sm,y_train_sm,X_test,param_grid,\"Non-Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "rfc_cv_grid.best_score_  0.8243944636678201\n",
      "rfc_cv_grid.best_params_  {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 16, 'class_weight': 'balanced'}\n",
      "TRAIN DATA ACCURACY 0.9065743944636678\n",
      "\n",
      "Train data f1-score for class '0' 0.9055944055944055\n",
      "\n",
      "Train data f1-score for class '1' 0.9075342465753425\n",
      "TEST DATA ACCURACY 0.7866666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8472553699284009\n",
      "\n",
      "Test data f1-score for class '1' 0.6464088397790054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [10, 25, 50, 75, 100], 'max_depth': [10, 12, 14, 16, 18, 20], 'min_samples_leaf': [5, 10, 15, 20], 'class_weight': ['balanced', 'balanced_subsample']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use a grid over parameters of interest\n",
    "## n_estimators is the number of trees in the forest\n",
    "## max_depth is how deep each tree can be\n",
    "## min_sample_leaf is the minimum samples required in each leaf node for the root node to split\n",
    "## \"A node will only be split if in each of it's leaf nodes there should be min_sample_leaf\"\n",
    "\n",
    "param_grid = {\"n_estimators\" : [10, 25, 50, 75, 100],\n",
    "           \"max_depth\" : [10, 12, 14, 16, 18, 20],\n",
    "           \"min_samples_leaf\" : [5, 10, 15, 20],\n",
    "           \"class_weight\" : ['balanced','balanced_subsample']}\n",
    "\n",
    "\n",
    "rfc_model(X_train_sm,y_train_sm,X_test,param_grid,\"Non-Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model building with Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Using TF-IDF only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   targetSentiment                                      antonymns_upd  \\\n",
       "0                0                    like animation animal look real   \n",
       "1                0                 amazing realistic incredible music   \n",
       "2                0        classic good remake love glover outstanding   \n",
       "3                1  nice animation cgi completely lack disney styl...   \n",
       "4                0                      good mainly sentimental value   \n",
       "\n",
       "   adj_count  adv_verb_count  review_polar  adj_polar  adv_verb_polar  \\\n",
       "0        2.0             2.0      0.433333   0.200000        0.000000   \n",
       "1        3.0             0.0      0.605556   0.555556        0.000000   \n",
       "2        4.0             1.0      0.516667   0.466667        0.500000   \n",
       "3        5.0             2.0      0.318750   0.391667        0.100000   \n",
       "4        2.0             1.0      0.205556   0.225000        0.166667   \n",
       "\n",
       "   count_words  count_capital_words  count_love  count_shit  \\\n",
       "0           12                    0           0           0   \n",
       "1            6                    0           2           0   \n",
       "2            8                    0           2           1   \n",
       "3           27                    1           0           1   \n",
       "4            8                    0           0           0   \n",
       "\n",
       "   count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                  0                 0                0             0  \n",
       "1                  0                 1                0             0  \n",
       "2                  0                 0                0             0  \n",
       "3                  1                 0                0             0  \n",
       "4                  0                 0                0             0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(final_data_text[clean_after_antonymns])\n",
    "y = final_data_text[targetSentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking both unigram and bigram\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=10000000,\n",
    "                                 min_df=0.001,\n",
    "                                 use_idf=True, ngram_range=(1,2))\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(X_train[clean_after_antonymns])\n",
    "\n",
    "X_train_tfidf_matrix= tfidf_vectorizer.transform(X_train[clean_after_antonymns])\n",
    "X_test_tfidf_matrix = tfidf_vectorizer.transform(X_test[clean_after_antonymns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2400x2259 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 29473 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<600x2259 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7272 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'absolute', 'absolutely', 'absolutely amazing', 'absolutely beautiful', 'absolutely love', 'absolutely stunning', 'accurate', 'achieve', 'achievement', 'act', 'acting', 'acting bad', 'acting feel', 'action', 'action adaptation', 'action film', 'action movie', 'action remake', 'action version', 'actor', 'actor play', 'actor sound', 'actor voice', 'actual', 'actual animal', 'actually', 'adaptation', 'adaptation original', 'add', 'add little', 'add new', 'addition', 'additionally', 'admission', 'admit', 'adorable', 'adore', 'adult', 'adult child', 'affect', 'africa', 'african', 'age', 'ago', 'agree', 'agree critic', 'aladdin', 'alike', 'alive', 'allow', 'alright', 'alteration', 'amazing', 'amazing animal', 'amazing animation', 'amazing bring', 'amazing cgi', 'amazing feel', 'amazing good', 'amazing graphic', 'amazing great', 'amazing job', 'amazing look', 'amazing love', 'amazing movie', 'amazing real', 'amazing realistic', 'amazing story', 'amazing visual', 'amazing voice', 'amazingly', 'amazingly realistic', 'anger', 'animal', 'animal absolutely', 'animal action', 'animal expression', 'animal face', 'animal great', 'animal life', 'animal little', 'animal look', 'animal planet', 'animal real', 'animal realistic', 'animal scenery', 'animal talk', 'animate', 'animate character', 'animate classic', 'animate feature', 'animate film', 'animate lion', 'animate movie', 'animate original', 'animate version', 'animate well', 'animated', 'animated film', 'animation', 'animation amazing', 'animation animal', 'animation awesome', 'animation beautiful', 'animation good', 'animation great', 'animation incredible', 'animation movie', 'animation music', 'animation new', 'animation real', 'animation realistic', 'animation unbelievable', 'animator', 'annoy', 'annoying', 'answer', 'apart', 'apathy', 'apathy dislike', 'appeal', 'appear', 'appreciate', 'art', 'artist', 'aside', 'ask', 'asleep', 'aspect', 'attempt', 'attention', 'attitude', 'audience', 'audio', 'available', 'avoid', 'aware', 'away', 'awesome', 'awesome animation', 'awesome enjoy', 'awesome great', 'awesome job', 'awesome love', 'awesome movie', 'awful', 'awkward', 'baby', 'background', 'bad', 'bad fake', 'bad good', 'bad movie', 'bad negtagnegvoice', 'bad review', 'balance', 'barely', 'base', 'basically', 'be', 'be glad', 'bear', 'beast', 'beast reference', 'beat', 'beautiful', 'beautiful animal', 'beautiful cgi', 'beautiful movie', 'beautiful music', 'beautiful scene', 'beautiful scenery', 'beautiful story', 'beautifully', 'beauty', 'beauty beast', 'begin', 'beginning', 'believable', 'believe', 'beloved', 'beloved classic', 'best', 'bet', 'better', 'better leave', 'better movie', 'better original', 'beyonc', 'beyonc voice', 'beyonce', 'beyonce donald', 'beyonce nala', 'beyonce singe', 'beyonce song', 'beyonce thing', 'beyonce voice', 'big', 'big fan', 'big screen', 'billy', 'billy eichner', 'bird', 'bit', 'bit slow', 'bland', 'blend', 'blow', 'body', 'book', 'bore', 'bore time', 'bored', 'boring', 'box', 'boy', 'bravo', 'breathtake', 'brilliant', 'bring', 'bring childhood', 'bring good', 'bring great', 'bring life', 'bring lot', 'bring memory', 'bring movie', 'bring tear', 'broadway', 'brother', 'buena', 'bunch', 'butcher', 'butcher prepare', 'buy', 'buy come', 'call', 'captivate', 'capture', 'carbon', 'carbon copy', 'care', 'cartoon', 'cartoon good', 'cartoon movie', 'cartoon version', 'cartoon well', 'case', 'cash', 'cash grab', 'cast', 'casting', 'cat', 'cause', 'celebrity', 'certain', 'certain scene', 'certainly', 'cg', 'cgi', 'cgi amazing', 'cgi animal', 'cgi come', 'cgi effect', 'cgi emotion', 'cgi great', 'cgi incredible', 'cgi look', 'cgi see', 'chance', 'change', 'change character', 'change little', 'change original', 'change scar', 'channel', 'character', 'character design', 'character development', 'character emotion', 'character face', 'character great', 'character know', 'character lack', 'character look', 'character real', 'character scene', 'character song', 'character talk', 'character timon', 'character voice', 'charm', 'charm original', 'check', 'chemistry', 'child', 'child great', 'child love', 'child movie', 'child watch', 'childhood', 'childhood classic', 'childhood favorite', 'childhood love', 'childhood memory', 'childhood movie', 'children', 'chill', 'chiwetel', 'chiwetel ejiofor', 'choice', 'choose', 'cinema', 'cinematic', 'cinematography', 'cinematography amazing', 'cinematography great', 'circle', 'circle life', 'classic', 'classic bring', 'classic movie', 'classic story', 'clearly', 'clever', 'close', 'close original', 'closely', 'closer', 'closer original', 'cold', 'color', 'colorful', 'colour', 'come', 'come dvd', 'come life', 'come movie', 'comedy', 'comic', 'comic relief', 'compare', 'compare original', 'comparison', 'comparison original', 'compelling', 'complaint', 'complete', 'completely', 'completely ruin', 'computer', 'computer animation', 'computer generate', 'computer graphic', 'con', 'connect', 'connection', 'connection character', 'consider', 'content', 'contrast', 'convey', 'convey emotion', 'cool', 'copy', 'copy original', 'counterfeit', 'couple', 'course', 'craft', 'crap', 'create', 'creation', 'creative', 'creativity', 'creature', 'credit', 'creepy', 'critic', 'crude', 'cry', 'cry mufasa', 'cub', 'curious', 'current', 'cut', 'cut lot', 'cute', 'cute movie', 'dad', 'dance', 'dare', 'dark', 'date', 'daughter', 'daughter love', 'day', 'de', 'de la', 'dead', 'deal', 'death', 'decent', 'decide', 'def', 'definite', 'definitely', 'definitely recommend', 'definitely worth', 'deliver', 'depress', 'depth', 'deserve', 'design', 'desire', 'despite', 'destroy', 'detail', 'development', 'dialog', 'dialogue', 'die', 'difference', 'different', 'different cartoon', 'different original', 'difficult', 'digital', 'diminish', 'diminish negtagnegnew', 'direct', 'direction', 'director', 'disagree', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disbelieve', 'discovery', 'discovery channel', 'dislike', 'dislike beyonce', 'dislike fake', 'dislike movie', 'dislike negtagneghow', 'dislikes', 'disney', 'disney classic', 'disney film', 'disney great', 'disney live', 'disney movie', 'disney need', 'disney original', 'disney remake', 'disney take', 'display', 'disregard', 'distraction', 'documentary', 'dollar', 'donald', 'donald glover', 'downplay', 'drag', 'drama', 'dramatic', 'draw', 'dry', 'duet', 'dull', 'dvd', 'earl', 'earl jone', 'early', 'earth', 'easy', 'eat', 'ed', 'edge', 'effect', 'effect amazing', 'effect great', 'effective', 'effort', 'eichner', 'eichner seth', 'eichner timon', 'ejiofor', 'element', 'eliminate', 'elton', 'elton john', 'elton johns', 'emote', 'emotion', 'emotion animal', 'emotion cartoon', 'emotion character', 'emotion face', 'emotion heart', 'emotion not', 'emotion original', 'emotion voice', 'emotional', 'emotionally', 'emotionless', 'emphasis', 'end', 'ending', 'energy', 'engage', 'enhance', 'enjoy', 'enjoy film', 'enjoy great', 'enjoy live', 'enjoy movie', 'enjoy music', 'enjoy original', 'enjoy remake', 'enjoyable', 'enjoyable movie', 'enjoyment', 'entertain', 'entertaining', 'entertaining movie', 'entertainment', 'enthusiasm', 'entire', 'entire family', 'entire movie', 'environment', 'epic', 'equally', 'es', 'especially', 'event', 'exact', 'exact copy', 'exact original', 'exact remake', 'exactly', 'exactly like', 'exactly original', 'example', 'exceed', 'exceed expectation', 'excellent', 'excellent graphic', 'excellent movie', 'excellent remake', 'exception', 'exceptional', 'excite', 'excited', 'exciting', 'execute', 'exist', 'expect', 'expect good', 'expectation', 'experience', 'explain', 'express', 'express emotion', 'expression', 'expression character', 'expression emotion', 'expression like', 'expression look', 'expression voice', 'expressionless', 'extent', 'extra', 'extremely', 'eye', 'fabulous', 'face', 'facial', 'facial emotion', 'facial expression', 'fact', 'factor', 'fail', 'fail capture', 'fairly', 'faithful', 'faithful original', 'fake', 'fake negtagnegstill', 'fall', 'fall asleep', 'fall flat', 'fall love', 'fall short', 'familiar', 'family', 'family enjoy', 'family love', 'family movie', 'fan', 'fan lion', 'fan original', 'fantastic', 'fantastic job', 'fantastic movie', 'far', 'far good', 'far well', 'fast', 'father', 'favorite', 'favorite disney', 'favorite movie', 'favorite part', 'favorite scene', 'favorite song', 'favreau', 'favreaus', 'feature', 'feel', 'feel bit', 'feel emotion', 'feel flat', 'feel good', 'feel like', 'feel love', 'feel movie', 'feel rush', 'feeling', 'female', 'fight', 'fight scene', 'figure', 'fill', 'film', 'film classic', 'film feel', 'film great', 'film making', 'film original', 'film see', 'filme', 'final', 'finally', 'find', 'fine', 'finish', 'fit', 'flat', 'flat bore', 'flaw', 'flawless', 'flop', 'flow', 'focus', 'follow', 'follow animate', 'follow cartoon', 'follow original', 'follow story', 'force', 'forever', 'forget', 'form', 'forward', 'frame', 'frame frame', 'frame remake', 'fresh', 'friend', 'frustrating', 'fun', 'fun family', 'fun movie', 'fun original', 'fun watch', 'funniest', 'funny', 'funny entertaining', 'funny moment', 'funny movie', 'funny part', 'future', 'game', 'garbage', 'general', 'generally', 'generate', 'generation', 'genius', 'genuinely', 'geo', 'geographic', 'get', 'get bore', 'get well', 'girl', 'give', 'give star', 'glad', 'glad see', 'glover', 'glover beyonce', 'glover simba', 'go', 'go happen', 'go home', 'go movie', 'go to', 'god', 'goldberg', 'good', 'good animate', 'good animation', 'good cgi', 'good disney', 'good enjoy', 'good family', 'good film', 'good graphic', 'good job', 'good kid', 'good like', 'good lion', 'good love', 'good memory', 'good movie', 'good music', 'good negtagneggreat', 'good original', 'good part', 'good quality', 'good remake', 'good scene', 'good song', 'good story', 'good thing', 'good time', 'good voice', 'good watch', 'good way', 'good worse', 'gorgeous', 'grab', 'grab disney', 'grand', 'grand daughter', 'grandchild', 'granddaughter', 'granddaughter love', 'grandkid', 'grandkid love', 'grandson', 'grandson love', 'graphic', 'graphic amazing', 'graphic animal', 'graphic awesome', 'graphic good', 'graphic great', 'great', 'great animal', 'great animation', 'great cast', 'great casting', 'great cgi', 'great change', 'great character', 'great cinematography', 'great computer', 'great disney', 'great family', 'great film', 'great humor', 'great job', 'great look', 'great memory', 'great movie', 'great music', 'great not', 'great remake', 'great song', 'great story', 'great storyline', 'great technology', 'great time', 'great visual', 'great voice', 'greatness', 'group', 'grow', 'grow lion', 'grow original', 'grow watch', 'guess', 'guest', 'guy', 'hakuna', 'hakuna matata', 'half', 'half star', 'han', 'han zimmer', 'hand', 'happen', 'happy', 'hard', 'hard believe', 'hard work', 'hat', 'hate', 'have', 'have see', 'head', 'hear', 'heart', 'heart original', 'heart soul', 'heartfelt', 'heartwarme', 'help', 'hero', 'high', 'high hope', 'highly', 'highly recommend', 'hilarious', 'hire', 'history', 'hit', 'hold', 'hold true', 'hole', 'hollow', 'hollywood', 'home', 'home watch', 'honest', 'honestly', 'hope', 'hope remake', 'horrible', 'horribly', 'hour', 'huge', 'human', 'humor', 'humor original', 'humorous', 'humour', 'hurt', 'husband', 'husband love', 'hyena', 'hype', 'hyper', 'hyper realistic', 'iconic', 'idea', 'identical', 'identical original', 'idiot', 'ill', 'image', 'imagery', 'imagine', 'imax', 'impact', 'impactful', 'important', 'important scene', 'impossible', 'impressed', 'impressive', 'improve', 'improvement', 'include', 'incredible', 'incredible movie', 'incredible music', 'incredibly', 'incredibly realistic', 'inspiration', 'instead', 'intense', 'interact', 'interaction', 'interest', 'interesting', 'invest', 'involve', 'iron', 'iron man', 'issue', 'jam', 'jam earl', 'jame', 'jame earl', 'jeremy', 'jeremy iron', 'job', 'job remake', 'john', 'john oliver', 'johns', 'joke', 'jon', 'jon favreau', 'jone', 'jone mufasa', 'jone sound', 'jone voice', 'journey', 'joy', 'jump', 'jungle', 'jungle book', 'justice', 'keep', 'keep original', 'keep true', 'key', 'key moment', 'kid', 'kid adult', 'kid enjoy', 'kid get', 'kid good', 'kid grow', 'kid like', 'kid love', 'kid movie', 'kid overall', 'kid watch', 'kill', 'kill movie', 'kind', 'kind bore', 'kinda', 'king', 'king child', 'king disney', 'king favorite', 'king film', 'king good', 'king great', 'king like', 'king live', 'king movie', 'king new', 'king original', 'king remake', 'king scene', 'know', 'know exactly', 'know go', 'know story', 'la', 'la original', 'lack', 'lack depth', 'lack emotion', 'lack emotional', 'lack energy', 'lack expression', 'lack facial', 'lack heart', 'lack lot', 'lack passion', 'lackluster', 'landscape', 'lane', 'language', 'largely', 'lastly', 'late', 'laugh', 'laugh cry', 'laughter', 'lazy', 'lead', 'learn', 'leave', 'leave theater', 'length', 'lesson', 'let', 'level', 'life', 'life animal', 'life like', 'life version', 'lifeless', 'lifelike', 'light', 'lighting', 'like', 'like actor', 'like aladdin', 'like animal', 'like animate', 'like better', 'like cartoon', 'like change', 'like character', 'like emotion', 'like fact', 'like follow', 'like go', 'like good', 'like great', 'like jungle', 'like kid', 'like little', 'like live', 'like lot', 'like movie', 'like music', 'like new', 'like original', 'like prepare', 'like real', 'like realistic', 'like see', 'like story', 'like take', 'like version', 'like voice', 'like watch', 'likely', 'line', 'line animate', 'line change', 'line great', 'line love', 'line movie', 'line original', 'lion', 'lion cub', 'lion face', 'lion king', 'lion look', 'list', 'listen', 'literally', 'little', 'little bit', 'little different', 'little emotion', 'little kid', 'little long', 'little scary', 'little thing', 'little violent', 'live', 'live action', 'live animal', 'live version', 'lol', 'long', 'long time', 'longer', 'look', 'look amazing', 'look animal', 'look beautiful', 'look forward', 'look great', 'look life', 'look like', 'look real', 'look realistic', 'looking', 'lose', 'lot', 'lot fun', 'lot memory', 'lot part', 'lot people', 'lot voice', 'loud', 'love', 'love amazing', 'love animation', 'love beyonce', 'love bring', 'love cgi', 'love character', 'love classic', 'love cry', 'love definitely', 'love disney', 'love fact', 'love family', 'love favorite', 'love feel', 'love funny', 'love good', 'love graphic', 'love great', 'love keep', 'love kid', 'love like', 'love lion', 'love love', 'love minute', 'love movie', 'love music', 'love negtagnegsure', 'love new', 'love not', 'love original', 'love real', 'love realistic', 'love recommend', 'love remake', 'love second', 'love see', 'love song', 'love stay', 'love stick', 'love story', 'love take', 'love time', 'love tonight', 'love true', 'love version', 'love watch', 'love way', 'love year', 'lovely', 'mad', 'magic', 'magic original', 'magnificent', 'main', 'main character', 'mainly', 'major', 'make', 'make movie', 'making', 'man', 'manage', 'mark', 'marvel', 'masterpiece', 'matata', 'match', 'match original', 'material', 'matter', 'maybe', 'mean', 'meaning', 'mediocre', 'medium', 'meh', 'melancholy', 'member', 'memorable', 'memory', 'memory child', 'memory childhood', 'mess', 'message', 'middle', 'mind', 'minor', 'minute', 'minute late', 'minute movie', 'miscast', 'miss', 'miss lot', 'miss original', 'mockery', 'model', 'modern', 'mom', 'moment', 'money', 'monotone', 'moral', 'mother', 'motion', 'mouse', 'mouth', 'move', 'movement', 'movie', 'movie absolutely', 'movie age', 'movie amazing', 'movie animal', 'movie animation', 'movie awesome', 'movie bad', 'movie beautiful', 'movie beautifully', 'movie begin', 'movie better', 'movie beyonce', 'movie bore', 'movie bring', 'movie cartoon', 'movie cgi', 'movie change', 'movie child', 'movie classic', 'movie come', 'movie compare', 'movie complete', 'movie completely', 'movie definitely', 'movie different', 'movie dislike', 'movie disney', 'movie enjoy', 'movie exactly', 'movie excellent', 'movie family', 'movie far', 'movie favorite', 'movie feel', 'movie get', 'movie glad', 'movie go', 'movie good', 'movie graphic', 'movie great', 'movie grow', 'movie have', 'movie highly', 'movie kid', 'movie lack', 'movie life', 'movie like', 'movie lion', 'movie little', 'movie live', 'movie long', 'movie look', 'movie love', 'movie minute', 'movie miss', 'movie movie', 'movie new', 'movie not', 'movie ok', 'movie original', 'movie overall', 'movie people', 'movie pretty', 'movie real', 'movie realistic', 'movie recommend', 'movie remake', 'movie remind', 'movie review', 'movie see', 'movie slow', 'movie song', 'movie star', 'movie start', 'movie stay', 'movie stick', 'movie take', 'movie theater', 'movie think', 'movie time', 'movie true', 'movie visually', 'movie voice', 'movie watch', 'movie way', 'movie well', 'movie wish', 'movie year', 'movie young', 'moving', 'mufasa', 'mufasa die', 'mufasas', 'mufasas death', 'multiple', 'music', 'music amazing', 'music animation', 'music awesome', 'music good', 'music great', 'music voice', 'music wonderful', 'musical', 'musical number', 'nada', 'nail', 'nala', 'nala sound', 'nalas', 'nat', 'nat geo', 'nathan', 'nathan lane', 'national', 'national geographic', 'natural', 'naturally', 'nature', 'nature documentary', 'near', 'nearly', 'need', 'need remake', 'negative', 'negative review', 'negtagnegable', 'negtagnegact', 'negtagnegaction', 'negtagnegafter', 'negtagnegage', 'negtagneganyone', 'negtagnegcapture', 'negtagnegcast', 'negtagnegcgi', 'negtagnegchange', 'negtagnegchild', 'negtagnegchoose', 'negtagnegclassic', 'negtagnegclose', 'negtagnegcome', 'negtagnegcompare', 'negtagnegcompare fake', 'negtagnegcompletely', 'negtagnegcomputer', 'negtagnegconnect', 'negtagnegcould', 'negtagnegcritic', 'negtagnegdifferent', 'negtagnegdisappoint', 'negtagnegdisplay', 'negtagnegdocumentary', 'negtagnegdown', 'negtagnegemotion', 'negtagnegemotional', 'negtagnegend', 'negtagnegenjoy', 'negtagnegenough', 'negtagnegeven', 'negtagnegeveryone', 'negtagnegexact', 'negtagnegexact negtagnegsame', 'negtagnegexcite', 'negtagnegexpectation', 'negtagnegexpression', 'negtagnegface', 'negtagnegfan', 'negtagnegfan fake', 'negtagnegfew', 'negtagnegfirst', 'negtagnegfit', 'negtagnegflow', 'negtagnegfollow', 'negtagnegfrom', 'negtagnegfrom fake', 'negtagnegfull', 'negtagnegget', 'negtagneggive', 'negtagneggo', 'negtagneggreat', 'negtagneghead', 'negtagneghear', 'negtagnegheart', 'negtagnegheart fake', 'negtagneghelp', 'negtagneghit', 'negtagneghold', 'negtagneghow', 'negtagneghuman', 'negtagneghumor', 'negtagneghype', 'negtagneginto', 'negtagnegjust', 'negtagnegkid', 'negtagnegknow', 'negtagnegknow negtagneghow', 'negtagneglack', 'negtagneglack negtagnegemotion', 'negtagneglife', 'negtagneglittle', 'negtagneglive', 'negtagneglive fake', 'negtagneglive negtagnegaction', 'negtagneglook', 'negtagneglove', 'negtagnegmagic', 'negtagnegmagic fake', 'negtagnegmany', 'negtagnegmatch', 'negtagnegmean', 'negtagnegmind', 'negtagnegmiss', 'negtagnegmoment', 'negtagnegmoney', 'negtagnegmore', 'negtagnegmufasa', 'negtagnegmusic', 'negtagnegnecessary', 'negtagnegnee', 'negtagnegnew', 'negtagnegold', 'negtagnegone', 'negtagnegonly', 'negtagnegother', 'negtagnegover', 'negtagnegoverall', 'negtagnegpart', 'negtagnegpay', 'negtagnegpeople', 'negtagnegperformance', 'negtagnegperson', 'negtagnegplay', 'negtagnegpossible', 'negtagnegprepare', 'negtagnegpumbaa', 'negtagnegpurchase', 'negtagnegquality', 'negtagnegquite', 'negtagnegread', 'negtagnegrealistic', 'negtagnegreally', 'negtagnegrecommend', 'negtagnegrefund', 'negtagnegreview', 'negtagnegright', 'negtagnegrole', 'negtagnegsame', 'negtagnegsay', 'negtagnegscar', 'negtagnegscene', 'negtagnegsee', 'negtagnegsee destroy', 'negtagnegsee fake', 'negtagnegsee negtagnegfirst', 'negtagnegseem', 'negtagnegshould', 'negtagnegshow', 'negtagnegsimba', 'negtagnegsing', 'negtagnegsinge', 'negtagnegsmall', 'negtagnegsong', 'negtagnegsound', 'negtagnegspecial', 'negtagnegspend', 'negtagnegstart', 'negtagnegstill', 'negtagnegstop', 'negtagnegstory', 'negtagnegsure', 'negtagnegtake', 'negtagnegtalk', 'negtagnegtell', 'negtagnegterrible', 'negtagnegtheater', 'negtagnegthrough', 'negtagnegticket', 'negtagnegtime', 'negtagnegtouch', 'negtagnegtry', 'negtagneguse', 'negtagnegusually', 'negtagnegvoice', 'negtagnegvoice negtagnegact', 'negtagnegwait', 'negtagnegwant', 'negtagnegwaste', 'negtagnegwaste negtagnegmoney', 'negtagnegwatch', 'negtagnegwell', 'negtagnegwhat', 'negtagnegwhat negtagnegcritic', 'negtagnegwhole', 'negtagnegwill', 'negtagnegwithout', 'negtagnegwork', 'negtagnegwrong', 'negtagnegyet', 'negtagnegyoung', 'new', 'new cast', 'new generation', 'new joke', 'new lion', 'new movie', 'new original', 'new scene', 'new song', 'new version', 'nice', 'nice see', 'non', 'nostalgia', 'nostalgic', 'not', 'not change', 'not feel', 'not fit', 'not know', 'not like', 'not think', 'not wait', 'not well', 'note', 'notice', 'number', 'nyla', 'obvious', 'obviously', 'odd', 'offer', 'og', 'oh', 'ok', 'ok prefer', 'ok special', 'okay', 'old', 'old enjoy', 'old granddaughter', 'old grandson', 'old kid', 'old love', 'old movie', 'old son', 'old version', 'oliver', 'omg', 'one', 'open', 'open scene', 'opening', 'opinion', 'original', 'original add', 'original amazing', 'original animate', 'original animated', 'original animation', 'original awesome', 'original bad', 'original better', 'original cartoon', 'original change', 'original classic', 'original closely', 'original come', 'original daughter', 'original definitely', 'original disney', 'original enjoy', 'original favorite', 'original film', 'original fun', 'original good', 'original graphic', 'original great', 'original heart', 'original hold', 'original kid', 'original like', 'original lion', 'original lot', 'original love', 'original miss', 'original movie', 'original music', 'original negtagnegsee', 'original new', 'original script', 'original song', 'original soundtrack', 'original story', 'original storyline', 'original think', 'original time', 'original unlike', 'original version', 'original visually', 'original voice', 'original way', 'original well', 'originality', 'outstanding', 'over', 'overall', 'overall good', 'overall great', 'overall movie', 'pace', 'pacing', 'paltry', 'parent', 'part', 'part movie', 'part original', 'particular', 'particularly', 'pass', 'passion', 'past', 'pay', 'people', 'perfect', 'perfection', 'perfectly', 'performance', 'period', 'person', 'personality', 'personally', 'phenomenal', 'phone', 'photo', 'photo realistic', 'photography', 'photorealism', 'photorealistic', 'pick', 'picture', 'picture quality', 'piece', 'place', 'plan', 'planet', 'planet earth', 'play', 'playing', 'pleasantly', 'pleased', 'pleasure', 'plot', 'plus', 'point', 'pointless', 'political', 'poor', 'poorly', 'popular', 'portray', 'positive', 'possible', 'potential', 'power', 'powerful', 'practically', 'predictable', 'prefer', 'prefer animate', 'prefer original', 'prepare', 'prepare song', 'present', 'presentation', 'pretty', 'pretty close', 'pretty cool', 'pretty good', 'pretty story', 'preview', 'previous', 'price', 'price admission', 'pride', 'pride rock', 'probably', 'problem', 'produce', 'production', 'project', 'provide', 'pull', 'pumba', 'pumba good', 'pumba hilarious', 'pumba timon', 'pumbaa', 'pumbaa timon', 'punch', 'purchase', 'push', 'quality', 'quantity', 'que', 'queen', 'question', 'questionable', 'quickly', 'quiet', 'quote', 'rafiki', 'raise', 'rate', 'rate star', 'rating', 'raw', 'raw emotion', 'reach', 'reaction', 'read', 'read script', 'ready', 'real', 'real amazing', 'real animal', 'real emotion', 'real great', 'real life', 'real look', 'real scenery', 'realism', 'realism animal', 'realistic', 'realistic animal', 'realistic cgi', 'realistic character', 'realistic feel', 'realistic graphic', 'realistic look', 'realistic looking', 'realistic version', 'reality', 'realization', 'reason', 'reboot', 'recent', 'recognizable', 'recommend', 'recommend age', 'recommend movie', 'recommend see', 'record', 'recreate', 'recreation', 'recreation original', 'redo', 'redone', 'reference', 'refreshing', 'regret', 'rehash', 'reimagine', 'reimagining', 'relationship', 'release', 'relief', 'relinquish', 'relive', 'relive childhood', 'remain', 'remake', 'remake add', 'remake animate', 'remake animation', 'remake cartoon', 'remake change', 'remake classic', 'remake disney', 'remake film', 'remake go', 'remake good', 'remake lion', 'remake movie', 'remake original', 'remake stay', 'remake think', 'remarkable', 'remember', 'remind', 'remind childhood', 'remixe', 'remove', 'rendition', 'replace', 'replace original', 'repulse', 'rest', 'result', 'retelling', 'return', 'review', 'rewrite', 'riff', 'right', 'rip', 'roar', 'rock', 'rogan', 'rogan pumba', 'rogen', 'rogen pumba', 'role', 'room', 'ruin', 'run', 'rush', 'sad', 'sad funny', 'sadly', 'sake', 'save', 'say', 'scale', 'scar', 'scar prepare', 'scar song', 'scar voice', 'scare', 'scary', 'scary little', 'scene', 'scene animate', 'scene cut', 'scene dialogue', 'scene feel', 'scene movie', 'scene original', 'scene overall', 'scene rafiki', 'scene remake', 'scene scene', 'scene simba', 'scenery', 'scenery amazing', 'scenery beautiful', 'scenery music', 'scheme', 'school', 'score', 'scream', 'screen', 'screenplay', 'script', 'seat', 'second', 'second time', 'see', 'see animate', 'see kid', 'see life', 'see lion', 'see movie', 'see original', 'see real', 'see theater', 'see time', 'see twice', 'self', 'sell', 'senior', 'sense', 'sequel', 'sequence', 'serengeti', 'seriously', 'set', 'set apart', 'seth', 'seth rogan', 'seth rogen', 'shakespearean', 'share', 'shoot', 'shoot shot', 'short', 'shorten', 'shot', 'shot remake', 'shot shot', 'show', 'show emotion', 'sick', 'simba', 'simba nala', 'simbas', 'similar', 'similar original', 'simply', 'sing', 'singe', 'singe song', 'singer', 'singing', 'single', 'sit', 'situation', 'skip', 'sleep', 'slight', 'slightly', 'slow', 'small', 'smart', 'smile', 'solid', 'something', 'somewhat', 'son', 'son enjoy', 'song', 'song feel', 'song great', 'song like', 'song movie', 'song new', 'soo', 'soon', 'sorry', 'soul', 'soul original', 'soulless', 'sound', 'sound like', 'soundtrack', 'source', 'source material', 'speak', 'special', 'special effect', 'spectacle', 'spectacular', 'speech', 'spend', 'spin', 'spirit', 'spoiler', 'spoof', 'spot', 'stage', 'stale', 'stampede', 'stand', 'star', 'start', 'start finish', 'stay', 'stay original', 'stay true', 'steal', 'step', 'stick', 'stick original', 'stick script', 'stick story', 'stop', 'story', 'story animation', 'story feel', 'story good', 'story great', 'story life', 'story like', 'story line', 'story little', 'story love', 'story music', 'story new', 'story original', 'story tell', 'story wise', 'storyline', 'strength', 'strong', 'stuff', 'stun', 'stunning', 'stunning movie', 'stupid', 'style', 'subpar', 'subtle', 'suck', 'suffer', 'super', 'superb', 'support', 'suppose', 'sure', 'surprise', 'surprised', 'surprisingly', 'sweet', 'switch', 'take', 'take away', 'take childhood', 'take daughter', 'take fun', 'take granddaughter', 'take kid', 'take year', 'talent', 'talented', 'talk', 'talk animal', 'talk look', 'talk sing', 'teach', 'tear', 'tear eye', 'technical', 'technically', 'technological', 'technology', 'tell', 'tell real', 'tell story', 'term', 'terrible', 'terrible movie', 'terrible voice', 'terrific', 'thank', 'theater', 'theatre', 'theatrical', 'thing', 'thing love', 'thing miss', 'thing movie', 'thing original', 'think', 'think beyonce', 'think cgi', 'think disney', 'think good', 'think great', 'think little', 'think movie', 'think original', 'think watch', 'think well', 'thoroughly', 'thoroughly enjoy', 'throne', 'thumb', 'ticket', 'time', 'time favorite', 'time feel', 'time love', 'time see', 'time watch', 'time year', 'timeless', 'timon', 'timon funny', 'timon pumba', 'timon pumbaa', 'timone', 'timone pumba', 'tired', 'to', 'today', 'tone', 'tonight', 'totally', 'touch', 'touching', 'track', 'traditional', 'transition', 'trash', 'true', 'true animate', 'true life', 'true original', 'true story', 'truly', 'truly enjoy', 'try', 'try movie', 'tune', 'turn', 'tweak', 'twice', 'twist', 'unbelievable', 'uncanny', 'uncanny valley', 'understand', 'understandable', 'underwhelme', 'underwhelming', 'undoubtedly', 'unfortunately', 'unimpressed', 'uninspired', 'unique', 'unlike', 'unnecessary', 'unnecessary remake', 'update', 'use', 'useless', 'usually', 'valley', 'value', 'version', 'version better', 'version feel', 'version good', 'version great', 'version lack', 'version lion', 'version movie', 'version music', 'version new', 'version original', 'version time', 'version watch', 'version well', 'vfx', 'vibe', 'vibrant', 'vibrant color', 'video', 'view', 'viewer', 'villain', 'villain song', 'violence', 'violent', 'virtually', 'visual', 'visual amazing', 'visual effect', 'visual great', 'visual impressive', 'visual movie', 'visual stunning', 'visually', 'visually amazing', 'visually beautiful', 'visually impressive', 'visually stunning', 'vocal', 'vocal performance', 'voice', 'voice act', 'voice acting', 'voice actor', 'voice bad', 'voice cast', 'voice character', 'voice great', 'voice lack', 'voice mufasa', 'voice nala', 'voice not', 'voice over', 'voice performance', 'voice simba', 'voice talent', 'voice work', 'wait', 'wait dvd', 'wait king', 'wait watch', 'walk', 'want', 'want enjoy', 'want money', 'warthog', 'waste', 'waste money', 'watch', 'watch animal', 'watch animate', 'watch cartoon', 'watch cgi', 'watch discovery', 'watch disney', 'watch enjoy', 'watch lion', 'watch movie', 'watch national', 'watch nature', 'watch new', 'watch original', 'watch planet', 'watch preview', 'watch real', 'watch time', 'water', 'way', 'way better', 'way emotion', 'way look', 'way movie', 'way watch', 'weak', 'week', 'weird', 'well', 'well expect', 'well graphic', 'well job', 'well original', 'whilst', 'whoopi', 'whoopi goldberg', 'wife', 'wild', 'wildlife', 'will', 'will not', 'win', 'wise', 'wish', 'wish movie', 'wish original', 'wonder', 'wonder original', 'wonderful', 'wonderful movie', 'wonderful remake', 'word', 'word word', 'work', 'world', 'worse', 'worse fake', 'worth', 'worth price', 'worth see', 'worth watch', 'worthy', 'would', 'wow', 'write', 'wrong', 'year', 'year ago', 'year old', 'yes', 'young', 'young child', 'young kid', 'youth', 'yr', 'yr old', 'zazu', 'zimmer']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making tfidf dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "X_test_tfidf_df  = pd.DataFrame(X_test_tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For future use\n",
    "X_train_tfidf_fut = X_train_tfidf_df.copy(deep=True)\n",
    "X_test_tfidf_fut = X_test_tfidf_df.copy(deep=True)\n",
    "\n",
    "y_train_tfidf_fut = y_train.copy(deep=True)\n",
    "y_test_tfidf_fut = y_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE the train data as there is data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train_tfidf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tfidf_df.shape = (2400, 2259)\n",
      "y_train.shape = (2400,)\n",
      "X_train_sm.shape = (3468, 2259)\n",
      "y_train_sm.shape = (3468,)\n",
      "sum(y_train_sm) = 1734\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_tfidf_df.shape = {X_train_tfidf_df.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")\n",
    "print(f\"X_train_sm.shape = {X_train_sm.shape}\")\n",
    "print(f\"y_train_sm.shape = {y_train_sm.shape}\")\n",
    "print(f\"sum(y_train_sm) = {sum(y_train_sm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.5\n",
      "\n",
      "Train data f1-score for class '0' 0.6666666666666666\n",
      "\n",
      "Train data f1-score for class '1' 0.0\n",
      "TEST DATA ACCURACY 0.73\n",
      "\n",
      "Test data f1-score for class '0' 0.8439306358381503\n",
      "\n",
      "Test data f1-score for class '1' 0.0\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8460207612456747\n",
      "\n",
      "Train data f1-score for class '0' 0.8388654194327096\n",
      "\n",
      "Train data f1-score for class '1' 0.8525676421866372\n",
      "TEST DATA ACCURACY 0.7866666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8439024390243903\n",
      "\n",
      "Test data f1-score for class '1' 0.6631578947368421\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.5\n",
      "\n",
      "Train data f1-score for class '0' 0.6666666666666666\n",
      "\n",
      "Train data f1-score for class '1' 0.0\n",
      "TEST DATA ACCURACY 0.73\n",
      "\n",
      "Test data f1-score for class '0' 0.8439306358381503\n",
      "\n",
      "Test data f1-score for class '1' 0.0\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8428489042675894\n",
      "\n",
      "Train data f1-score for class '0' 0.8286702294875825\n",
      "\n",
      "Train data f1-score for class '1' 0.8548601864181092\n",
      "TEST DATA ACCURACY 0.7616666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.8173690932311621\n",
      "\n",
      "Test data f1-score for class '1' 0.6570743405275778\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7047289504036909\n",
      "\n",
      "Train data f1-score for class '0' 0.6417074877536739\n",
      "\n",
      "Train data f1-score for class '1' 0.7488965179009318\n",
      "TEST DATA ACCURACY 0.62\n",
      "\n",
      "Test data f1-score for class '0' 0.6779661016949152\n",
      "\n",
      "Test data f1-score for class '1' 0.5365853658536586\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8647635524798154\n",
      "\n",
      "Train data f1-score for class '0' 0.8543025784405095\n",
      "\n",
      "Train data f1-score for class '1' 0.8738229755178908\n",
      "TEST DATA ACCURACY 0.7733333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.829145728643216\n",
      "\n",
      "Test data f1-score for class '1' 0.6633663366336634\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.8765859284890427\n",
      "\n",
      "Train data f1-score for class '0' 0.8699878493317132\n",
      "\n",
      "Train data f1-score for class '1' 0.8825466520307355\n",
      "TEST DATA ACCURACY 0.7833333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.841075794621027\n",
      "\n",
      "Test data f1-score for class '1' 0.6596858638743457\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9181084198385236\n",
      "\n",
      "Train data f1-score for class '0' 0.9143029571514786\n",
      "\n",
      "Train data f1-score for class '1' 0.9215902816123689\n",
      "TEST DATA ACCURACY 0.8133333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8640776699029125\n",
      "\n",
      "Test data f1-score for class '1' 0.702127659574468\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.9936562860438293\n",
      "\n",
      "Train data f1-score for class '0' 0.9936157864190366\n",
      "\n",
      "Train data f1-score for class '1' 0.9936962750716332\n",
      "TEST DATA ACCURACY 0.8066666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8632075471698113\n",
      "\n",
      "Test data f1-score for class '1' 0.6704545454545455\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9786620530565168\n",
      "\n",
      "Train data f1-score for class '0' 0.9783499122293738\n",
      "\n",
      "Train data f1-score for class '1' 0.9789653212052302\n",
      "TEST DATA ACCURACY 0.8216666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.8730723606168445\n",
      "\n",
      "Test data f1-score for class '1' 0.700280112044818\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.9979815455594002\n",
      "\n",
      "Train data f1-score for class '0' 0.9979774631609362\n",
      "\n",
      "Train data f1-score for class '1' 0.9979856115107913\n",
      "TEST DATA ACCURACY 0.8\n",
      "\n",
      "Test data f1-score for class '0' 0.8594847775175645\n",
      "\n",
      "Test data f1-score for class '1' 0.653179190751445\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9976931949250288\n",
      "\n",
      "Train data f1-score for class '0' 0.9976878612716763\n",
      "\n",
      "Train data f1-score for class '1' 0.9976985040276178\n",
      "TEST DATA ACCURACY 0.79\n",
      "\n",
      "Test data f1-score for class '0' 0.8510638297872339\n",
      "\n",
      "Test data f1-score for class '1' 0.6440677966101694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=X_test_tfidf_df\n",
    "\n",
    "C=[0.001,0.01,0.1,1,10,100]\n",
    "Penalty=['l1','l2']\n",
    "        \n",
    "logistic_regr(X_train_sm,y_train_sm,X_test,C_list,Penalty,Features_name=\"TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "CV_XGB.best_score_  0.8696655132641292\n",
      "CV_XGB.best_params_  {'colsample_bytree': 0.5, 'max_depth': 10, 'n_estimators': 100}\n",
      "TRAIN DATA ACCURACY 0.9466551326412919\n",
      "\n",
      "Train data f1-score for class '0' 0.9453471196454948\n",
      "\n",
      "Train data f1-score for class '1' 0.9479019994367783\n",
      "TEST DATA ACCURACY 0.7916666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8551564310544613\n",
      "\n",
      "Test data f1-score for class '1' 0.6290801186943621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=-1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'colsample_bytree': array([0.5, 0.6, 0.7, 0.8, 0.9]), 'n_estimators': [100], 'max_depth': [10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test_tfidf_df\n",
    "\n",
    "param_grid = {\n",
    "    'colsample_bytree': np.linspace(0.5, 0.9, 5),\n",
    "    'n_estimators':[100],\n",
    "    'max_depth': [10]\n",
    "}\n",
    "\n",
    "xgb_model(X_train_sm,y_train_sm,X_test,param_grid,\"TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "rfc_cv_grid.best_score_  0.8264129181084199\n",
      "rfc_cv_grid.best_params_  {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 18, 'class_weight': 'balanced'}\n",
      "TRAIN DATA ACCURACY 0.8601499423298731\n",
      "\n",
      "Train data f1-score for class '0' 0.8653151902249375\n",
      "\n",
      "Train data f1-score for class '1' 0.8545727136431785\n",
      "TEST DATA ACCURACY 0.805\n",
      "\n",
      "Test data f1-score for class '0' 0.8671963677639046\n",
      "\n",
      "Test data f1-score for class '1' 0.6332288401253918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [10, 25, 50, 75, 100], 'max_depth': [10, 12, 14, 16, 18, 20], 'min_samples_leaf': [5, 10, 15, 20], 'class_weight': ['balanced', 'balanced_subsample']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test_tfidf_df\n",
    "\n",
    "param_grid = {\"n_estimators\" : [10, 25, 50, 75, 100],\n",
    "           \"max_depth\" : [10, 12, 14, 16, 18, 20],\n",
    "           \"min_samples_leaf\" : [5, 10, 15, 20],\n",
    "           \"class_weight\" : ['balanced','balanced_subsample']}\n",
    "\n",
    "\n",
    "rfc_model(X_train_sm,y_train_sm,X_test,param_grid,\"TFIDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>F1_0Class</th>\n",
       "      <th>F1_1Class</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>F1_0Class_test</th>\n",
       "      <th>F1_1Class_test</th>\n",
       "      <th>Features</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.978662</td>\n",
       "      <td>0.97835</td>\n",
       "      <td>0.978965</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.873072</td>\n",
       "      <td>0.70028</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>C : 10| Penalty : l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.997982</td>\n",
       "      <td>0.997977</td>\n",
       "      <td>0.997986</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.859485</td>\n",
       "      <td>0.653179</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>C : 100| Penalty : l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.997693</td>\n",
       "      <td>0.997688</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>C : 100| Penalty : l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.946655</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>0.947902</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.855156</td>\n",
       "      <td>0.62908</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>Best_Score_ : 0.8696655132641292| Best_Param_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.86015</td>\n",
       "      <td>0.865315</td>\n",
       "      <td>0.854573</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.867196</td>\n",
       "      <td>0.633229</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>Best_Score_ : 0.8264129181084199| Best_Param_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model_Name Train_Accuracy F1_0Class F1_1Class Test_Accuracy  \\\n",
       "23      Logistic       0.978662   0.97835  0.978965      0.821667   \n",
       "24      Logistic       0.997982  0.997977  0.997986           0.8   \n",
       "25      Logistic       0.997693  0.997688  0.997699          0.79   \n",
       "26       XGBoost       0.946655  0.945347  0.947902      0.791667   \n",
       "27  RandomForest        0.86015  0.865315  0.854573         0.805   \n",
       "\n",
       "   F1_0Class_test F1_1Class_test Features  \\\n",
       "23       0.873072        0.70028    TFIDF   \n",
       "24       0.859485       0.653179    TFIDF   \n",
       "25       0.851064       0.644068    TFIDF   \n",
       "26       0.855156        0.62908    TFIDF   \n",
       "27       0.867196       0.633229    TFIDF   \n",
       "\n",
       "                                             Comments  \n",
       "23                               C : 10| Penalty : l2  \n",
       "24                              C : 100| Penalty : l1  \n",
       "25                              C : 100| Penalty : l2  \n",
       "26   Best_Score_ : 0.8696655132641292| Best_Param_...  \n",
       "27   Best_Score_ : 0.8264129181084199| Best_Param_...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_summary.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Using WordVec model for average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(final_data_text[clean_after_antonymns])\n",
    "y = final_data_text[targetSentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_column = 'clean_wordlist'\n",
    "\n",
    "X_train[w2v_column] = X_train[clean_after_antonymns].apply(lambda x : word_tokenize(x))\n",
    "X_test[w2v_column]  = X_test[clean_after_antonymns].apply(lambda x : word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(X_train[w2v_column], min_count=1, size=SIZE, window=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = X_train[w2v_column].apply(lambda x: compute_avg_w2v_vector(model_w2v.wv, x))\n",
    "X_test_w2v = X_test[w2v_column].apply(lambda x: compute_avg_w2v_vector(model_w2v.wv, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = pd.DataFrame(X_train_w2v.values.tolist(), index= X_train.index)\n",
    "X_test_w2v = pd.DataFrame(X_test_w2v.values.tolist(), index= X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>0.183881</td>\n",
       "      <td>-0.169687</td>\n",
       "      <td>0.068553</td>\n",
       "      <td>-0.110692</td>\n",
       "      <td>0.105639</td>\n",
       "      <td>0.325656</td>\n",
       "      <td>-0.586722</td>\n",
       "      <td>0.106893</td>\n",
       "      <td>-0.371388</td>\n",
       "      <td>0.221531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023244</td>\n",
       "      <td>-0.026766</td>\n",
       "      <td>-0.060066</td>\n",
       "      <td>0.156503</td>\n",
       "      <td>0.108143</td>\n",
       "      <td>-0.045359</td>\n",
       "      <td>-0.204211</td>\n",
       "      <td>-0.156074</td>\n",
       "      <td>0.113804</td>\n",
       "      <td>0.364212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.283563</td>\n",
       "      <td>-0.269853</td>\n",
       "      <td>0.107104</td>\n",
       "      <td>-0.171815</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.513725</td>\n",
       "      <td>-0.919484</td>\n",
       "      <td>0.164001</td>\n",
       "      <td>-0.586195</td>\n",
       "      <td>0.348959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033123</td>\n",
       "      <td>-0.040341</td>\n",
       "      <td>-0.095904</td>\n",
       "      <td>0.245537</td>\n",
       "      <td>0.170071</td>\n",
       "      <td>-0.072836</td>\n",
       "      <td>-0.324256</td>\n",
       "      <td>-0.249129</td>\n",
       "      <td>0.180026</td>\n",
       "      <td>0.570966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.327164</td>\n",
       "      <td>-0.322116</td>\n",
       "      <td>0.121223</td>\n",
       "      <td>-0.203038</td>\n",
       "      <td>0.204459</td>\n",
       "      <td>0.609814</td>\n",
       "      <td>-1.082735</td>\n",
       "      <td>0.199471</td>\n",
       "      <td>-0.687581</td>\n",
       "      <td>0.413540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038690</td>\n",
       "      <td>-0.051772</td>\n",
       "      <td>-0.118968</td>\n",
       "      <td>0.282341</td>\n",
       "      <td>0.197637</td>\n",
       "      <td>-0.087455</td>\n",
       "      <td>-0.370465</td>\n",
       "      <td>-0.290342</td>\n",
       "      <td>0.204353</td>\n",
       "      <td>0.676885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>0.240384</td>\n",
       "      <td>-0.232709</td>\n",
       "      <td>0.094598</td>\n",
       "      <td>-0.145582</td>\n",
       "      <td>0.143642</td>\n",
       "      <td>0.437827</td>\n",
       "      <td>-0.776909</td>\n",
       "      <td>0.142649</td>\n",
       "      <td>-0.498241</td>\n",
       "      <td>0.292258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027111</td>\n",
       "      <td>-0.037378</td>\n",
       "      <td>-0.082577</td>\n",
       "      <td>0.205298</td>\n",
       "      <td>0.144143</td>\n",
       "      <td>-0.060417</td>\n",
       "      <td>-0.269619</td>\n",
       "      <td>-0.205922</td>\n",
       "      <td>0.149443</td>\n",
       "      <td>0.486301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0.201392</td>\n",
       "      <td>-0.189111</td>\n",
       "      <td>0.076046</td>\n",
       "      <td>-0.120188</td>\n",
       "      <td>0.118545</td>\n",
       "      <td>0.361112</td>\n",
       "      <td>-0.648310</td>\n",
       "      <td>0.116479</td>\n",
       "      <td>-0.412550</td>\n",
       "      <td>0.243207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024406</td>\n",
       "      <td>-0.028154</td>\n",
       "      <td>-0.067524</td>\n",
       "      <td>0.172411</td>\n",
       "      <td>0.119581</td>\n",
       "      <td>-0.051147</td>\n",
       "      <td>-0.227212</td>\n",
       "      <td>-0.174390</td>\n",
       "      <td>0.127454</td>\n",
       "      <td>0.403037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "642   0.183881 -0.169687  0.068553 -0.110692  0.105639  0.325656 -0.586722   \n",
       "700   0.283563 -0.269853  0.107104 -0.171815  0.169000  0.513725 -0.919484   \n",
       "226   0.327164 -0.322116  0.121223 -0.203038  0.204459  0.609814 -1.082735   \n",
       "1697  0.240384 -0.232709  0.094598 -0.145582  0.143642  0.437827 -0.776909   \n",
       "1010  0.201392 -0.189111  0.076046 -0.120188  0.118545  0.361112 -0.648310   \n",
       "\n",
       "            7         8         9   ...        40        41        42  \\\n",
       "642   0.106893 -0.371388  0.221531  ... -0.023244 -0.026766 -0.060066   \n",
       "700   0.164001 -0.586195  0.348959  ... -0.033123 -0.040341 -0.095904   \n",
       "226   0.199471 -0.687581  0.413540  ... -0.038690 -0.051772 -0.118968   \n",
       "1697  0.142649 -0.498241  0.292258  ... -0.027111 -0.037378 -0.082577   \n",
       "1010  0.116479 -0.412550  0.243207  ... -0.024406 -0.028154 -0.067524   \n",
       "\n",
       "            43        44        45        46        47        48        49  \n",
       "642   0.156503  0.108143 -0.045359 -0.204211 -0.156074  0.113804  0.364212  \n",
       "700   0.245537  0.170071 -0.072836 -0.324256 -0.249129  0.180026  0.570966  \n",
       "226   0.282341  0.197637 -0.087455 -0.370465 -0.290342  0.204353  0.676885  \n",
       "1697  0.205298  0.144143 -0.060417 -0.269619 -0.205922  0.149443  0.486301  \n",
       "1010  0.172411  0.119581 -0.051147 -0.227212 -0.174390  0.127454  0.403037  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>0.168612</td>\n",
       "      <td>-0.162302</td>\n",
       "      <td>0.064409</td>\n",
       "      <td>-0.099651</td>\n",
       "      <td>0.100662</td>\n",
       "      <td>0.300770</td>\n",
       "      <td>-0.541592</td>\n",
       "      <td>0.097018</td>\n",
       "      <td>-0.344425</td>\n",
       "      <td>0.204304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014859</td>\n",
       "      <td>-0.025147</td>\n",
       "      <td>-0.058368</td>\n",
       "      <td>0.141521</td>\n",
       "      <td>0.096001</td>\n",
       "      <td>-0.042532</td>\n",
       "      <td>-0.187310</td>\n",
       "      <td>-0.144256</td>\n",
       "      <td>0.101915</td>\n",
       "      <td>0.333598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>0.363916</td>\n",
       "      <td>-0.356781</td>\n",
       "      <td>0.135181</td>\n",
       "      <td>-0.224359</td>\n",
       "      <td>0.219858</td>\n",
       "      <td>0.665855</td>\n",
       "      <td>-1.196247</td>\n",
       "      <td>0.212571</td>\n",
       "      <td>-0.770283</td>\n",
       "      <td>0.454368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039175</td>\n",
       "      <td>-0.049283</td>\n",
       "      <td>-0.127053</td>\n",
       "      <td>0.317253</td>\n",
       "      <td>0.222884</td>\n",
       "      <td>-0.096957</td>\n",
       "      <td>-0.424185</td>\n",
       "      <td>-0.317406</td>\n",
       "      <td>0.233460</td>\n",
       "      <td>0.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>0.289944</td>\n",
       "      <td>-0.272207</td>\n",
       "      <td>0.113232</td>\n",
       "      <td>-0.168885</td>\n",
       "      <td>0.172099</td>\n",
       "      <td>0.523432</td>\n",
       "      <td>-0.936660</td>\n",
       "      <td>0.171191</td>\n",
       "      <td>-0.596565</td>\n",
       "      <td>0.357377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033283</td>\n",
       "      <td>-0.048749</td>\n",
       "      <td>-0.102541</td>\n",
       "      <td>0.247917</td>\n",
       "      <td>0.168461</td>\n",
       "      <td>-0.076627</td>\n",
       "      <td>-0.325900</td>\n",
       "      <td>-0.252749</td>\n",
       "      <td>0.180521</td>\n",
       "      <td>0.584079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.174841</td>\n",
       "      <td>-0.165453</td>\n",
       "      <td>0.066581</td>\n",
       "      <td>-0.106118</td>\n",
       "      <td>0.103747</td>\n",
       "      <td>0.314307</td>\n",
       "      <td>-0.563515</td>\n",
       "      <td>0.102224</td>\n",
       "      <td>-0.359378</td>\n",
       "      <td>0.212750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020332</td>\n",
       "      <td>-0.026384</td>\n",
       "      <td>-0.059267</td>\n",
       "      <td>0.147594</td>\n",
       "      <td>0.104821</td>\n",
       "      <td>-0.044208</td>\n",
       "      <td>-0.196632</td>\n",
       "      <td>-0.151359</td>\n",
       "      <td>0.111006</td>\n",
       "      <td>0.349609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>0.144655</td>\n",
       "      <td>-0.138492</td>\n",
       "      <td>0.054004</td>\n",
       "      <td>-0.088768</td>\n",
       "      <td>0.086673</td>\n",
       "      <td>0.260756</td>\n",
       "      <td>-0.469064</td>\n",
       "      <td>0.084359</td>\n",
       "      <td>-0.298385</td>\n",
       "      <td>0.178680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017579</td>\n",
       "      <td>-0.021223</td>\n",
       "      <td>-0.051309</td>\n",
       "      <td>0.124317</td>\n",
       "      <td>0.086560</td>\n",
       "      <td>-0.038416</td>\n",
       "      <td>-0.163137</td>\n",
       "      <td>-0.126528</td>\n",
       "      <td>0.089121</td>\n",
       "      <td>0.290510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "1801  0.168612 -0.162302  0.064409 -0.099651  0.100662  0.300770 -0.541592   \n",
       "1190  0.363916 -0.356781  0.135181 -0.224359  0.219858  0.665855 -1.196247   \n",
       "1817  0.289944 -0.272207  0.113232 -0.168885  0.172099  0.523432 -0.936660   \n",
       "251   0.174841 -0.165453  0.066581 -0.106118  0.103747  0.314307 -0.563515   \n",
       "2505  0.144655 -0.138492  0.054004 -0.088768  0.086673  0.260756 -0.469064   \n",
       "\n",
       "            7         8         9   ...        40        41        42  \\\n",
       "1801  0.097018 -0.344425  0.204304  ... -0.014859 -0.025147 -0.058368   \n",
       "1190  0.212571 -0.770283  0.454368  ... -0.039175 -0.049283 -0.127053   \n",
       "1817  0.171191 -0.596565  0.357377  ... -0.033283 -0.048749 -0.102541   \n",
       "251   0.102224 -0.359378  0.212750  ... -0.020332 -0.026384 -0.059267   \n",
       "2505  0.084359 -0.298385  0.178680  ... -0.017579 -0.021223 -0.051309   \n",
       "\n",
       "            43        44        45        46        47        48        49  \n",
       "1801  0.141521  0.096001 -0.042532 -0.187310 -0.144256  0.101915  0.333598  \n",
       "1190  0.317253  0.222884 -0.096957 -0.424185 -0.317406  0.233460  0.745000  \n",
       "1817  0.247917  0.168461 -0.076627 -0.325900 -0.252749  0.180521  0.584079  \n",
       "251   0.147594  0.104821 -0.044208 -0.196632 -0.151359  0.111006  0.349609  \n",
       "2505  0.124317  0.086560 -0.038416 -0.163137 -0.126528  0.089121  0.290510  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For future use\n",
    "X_train_w2v_fut = X_train_w2v.copy(deep=True)\n",
    "X_test_w2v_fut = X_test_w2v.copy(deep=True)\n",
    "\n",
    "y_train_w2v_fut = y_train.copy(deep=True)\n",
    "y_test_w2v_fut = y_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE the train data as there is data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_w2v.shape = (2400, 50)\n",
      "y_train.shape = (2400,)\n",
      "X_train_sm.shape = (3468, 50)\n",
      "y_train_sm.shape = (3468,)\n",
      "sum(y_train_sm) = 1734\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_w2v.shape = {X_train_w2v.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")\n",
    "print(f\"X_train_sm.shape = {X_train_sm.shape}\")\n",
    "print(f\"y_train_sm.shape = {y_train_sm.shape}\")\n",
    "print(f\"sum(y_train_sm) = {sum(y_train_sm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.5\n",
      "\n",
      "Train data f1-score for class '0' 0.6666666666666666\n",
      "\n",
      "Train data f1-score for class '1' 0.0\n",
      "TEST DATA ACCURACY 0.73\n",
      "\n",
      "Test data f1-score for class '0' 0.8439306358381503\n",
      "\n",
      "Test data f1-score for class '1' 0.0\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.5135524798154556\n",
      "\n",
      "Train data f1-score for class '0' 0.666798340904602\n",
      "\n",
      "Train data f1-score for class '1' 0.0993059263214095\n",
      "TEST DATA ACCURACY 0.725\n",
      "\n",
      "Test data f1-score for class '0' 0.8387096774193548\n",
      "\n",
      "Test data f1-score for class '1' 0.06779661016949153\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.5\n",
      "\n",
      "Train data f1-score for class '0' 0.6666666666666666\n",
      "\n",
      "Train data f1-score for class '1' 0.0\n",
      "TEST DATA ACCURACY 0.73\n",
      "\n",
      "Test data f1-score for class '0' 0.8439306358381503\n",
      "\n",
      "Test data f1-score for class '1' 0.0\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.5867935409457901\n",
      "\n",
      "Train data f1-score for class '0' 0.6421972534332084\n",
      "\n",
      "Train data f1-score for class '1' 0.5110883657454794\n",
      "TEST DATA ACCURACY 0.64\n",
      "\n",
      "Test data f1-score for class '0' 0.7562076749435667\n",
      "\n",
      "Test data f1-score for class '1' 0.31210191082802546\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.6035178777393311\n",
      "\n",
      "Train data f1-score for class '0' 0.61212976022567\n",
      "\n",
      "Train data f1-score for class '1' 0.5945148923621351\n",
      "TEST DATA ACCURACY 0.6133333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.7135802469135804\n",
      "\n",
      "Test data f1-score for class '1' 0.40512820512820513\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.6066897347174164\n",
      "\n",
      "Train data f1-score for class '0' 0.6162070906021385\n",
      "\n",
      "Train data f1-score for class '1' 0.5966883500887049\n",
      "TEST DATA ACCURACY 0.6183333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.7190184049079755\n",
      "\n",
      "Test data f1-score for class '1' 0.40519480519480516\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.6023644752018454\n",
      "\n",
      "Train data f1-score for class '0' 0.6067864271457086\n",
      "\n",
      "Train data f1-score for class '1' 0.5978419364246136\n",
      "TEST DATA ACCURACY 0.6116666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.709838107098381\n",
      "\n",
      "Test data f1-score for class '1' 0.41309823677581864\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.6081314878892734\n",
      "\n",
      "Train data f1-score for class '0' 0.6122681883024251\n",
      "\n",
      "Train data f1-score for class '1' 0.6039055668901195\n",
      "TEST DATA ACCURACY 0.61\n",
      "\n",
      "Test data f1-score for class '0' 0.7075\n",
      "\n",
      "Test data f1-score for class '1' 0.415\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7497116493656286\n",
      "\n",
      "Train data f1-score for class '0' 0.7304347826086957\n",
      "\n",
      "Train data f1-score for class '1' 0.7664155005382131\n",
      "TEST DATA ACCURACY 0.7233333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.7909319899244333\n",
      "\n",
      "Test data f1-score for class '1' 0.5911330049261084\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.6441753171856978\n",
      "\n",
      "Train data f1-score for class '0' 0.6435586366262276\n",
      "\n",
      "Train data f1-score for class '1' 0.644789867587795\n",
      "TEST DATA ACCURACY 0.645\n",
      "\n",
      "Test data f1-score for class '0' 0.7347447073474471\n",
      "\n",
      "Test data f1-score for class '1' 0.4634760705289673\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7621107266435986\n",
      "\n",
      "Train data f1-score for class '0' 0.7425897035881435\n",
      "\n",
      "Train data f1-score for class '1' 0.7788796569284374\n",
      "TEST DATA ACCURACY 0.7516666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.8149068322981365\n",
      "\n",
      "Test data f1-score for class '1' 0.6227848101265823\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7332756632064591\n",
      "\n",
      "Train data f1-score for class '0' 0.7121070650482415\n",
      "\n",
      "Train data f1-score for class '1' 0.7515444533977975\n",
      "TEST DATA ACCURACY 0.7166666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.7831632653061225\n",
      "\n",
      "Test data f1-score for class '1' 0.5913461538461539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=X_test_w2v\n",
    "\n",
    "C=[0.001,0.01,0.1,1,10,100]\n",
    "Penalty=['l1','l2']\n",
    "        \n",
    "logistic_regr(X_train_sm,y_train_sm,X_test,C_list,Penalty,Features_name=\"Word2Vec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "CV_XGB.best_score_  0.8252595155709342\n",
      "CV_XGB.best_params_  {'colsample_bytree': 0.6, 'max_depth': 10, 'n_estimators': 100}\n",
      "TRAIN DATA ACCURACY 0.9976931949250288\n",
      "\n",
      "Train data f1-score for class '0' 0.997689196995956\n",
      "\n",
      "Train data f1-score for class '1' 0.9976971790443293\n",
      "TEST DATA ACCURACY 0.69\n",
      "\n",
      "Test data f1-score for class '0' 0.7876712328767124\n",
      "\n",
      "Test data f1-score for class '1' 0.42592592592592593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=-1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'colsample_bytree': array([0.5, 0.6, 0.7, 0.8, 0.9]), 'n_estimators': [100], 'max_depth': [10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test_w2v\n",
    "\n",
    "param_grid = {\n",
    "    'colsample_bytree': np.linspace(0.5, 0.9, 5),\n",
    "    'n_estimators':[100],\n",
    "    'max_depth': [10]\n",
    "}\n",
    "\n",
    "xgb_model(X_train_sm,y_train_sm,X_test,param_grid,\"Word2Vec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "rfc_cv_grid.best_score_  0.7834486735870819\n",
      "rfc_cv_grid.best_params_  {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 14, 'class_weight': 'balanced_subsample'}\n",
      "TRAIN DATA ACCURACY 0.9455017301038062\n",
      "\n",
      "Train data f1-score for class '0' 0.9435315207648641\n",
      "\n",
      "Train data f1-score for class '1' 0.9473390916689884\n",
      "TEST DATA ACCURACY 0.675\n",
      "\n",
      "Test data f1-score for class '0' 0.7719298245614036\n",
      "\n",
      "Test data f1-score for class '1' 0.43478260869565216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [10, 25, 50, 75, 100], 'max_depth': [10, 12, 14, 16, 18, 20], 'min_samples_leaf': [5, 10, 15, 20], 'class_weight': ['balanced', 'balanced_subsample']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test_w2v\n",
    "\n",
    "param_grid = {\"n_estimators\" : [10, 25, 50, 75, 100],\n",
    "           \"max_depth\" : [10, 12, 14, 16, 18, 20],\n",
    "           \"min_samples_leaf\" : [5, 10, 15, 20],\n",
    "           \"class_weight\" : ['balanced','balanced_subsample']}\n",
    "\n",
    "\n",
    "rfc_model(X_train_sm,y_train_sm,X_test,param_grid,\"Word2Vec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Using CountVectorizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(final_data_text[clean_after_antonymns])\n",
    "y = final_data_text[targetSentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking both unigram and bigram\n",
    "cvect = CountVectorizer(max_df=0.9, max_features=10000000,\n",
    "                                 min_df=0.001, ngram_range=(1,2))\n",
    "\n",
    "#cvect = CountVectorizer()\n",
    "cvect.fit(X_train[clean_after_antonymns])\n",
    "\n",
    "X_train_cvect_matrix= cvect.transform(X_train[clean_after_antonymns])\n",
    "X_test_cvect_matrix = cvect.transform(X_test[clean_after_antonymns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2400x2259 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29473 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvect_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'absolute', 'absolutely', 'absolutely amazing', 'absolutely beautiful', 'absolutely love', 'absolutely stunning', 'accurate', 'achieve', 'achievement', 'act', 'acting', 'acting bad', 'acting feel', 'action', 'action adaptation', 'action film', 'action movie', 'action remake', 'action version', 'actor', 'actor play', 'actor sound', 'actor voice', 'actual', 'actual animal', 'actually', 'adaptation', 'adaptation original', 'add', 'add little', 'add new', 'addition', 'additionally', 'admission', 'admit', 'adorable', 'adore', 'adult', 'adult child', 'affect', 'africa', 'african', 'age', 'ago', 'agree', 'agree critic', 'aladdin', 'alike', 'alive', 'allow', 'alright', 'alteration', 'amazing', 'amazing animal', 'amazing animation', 'amazing bring', 'amazing cgi', 'amazing feel', 'amazing good', 'amazing graphic', 'amazing great', 'amazing job', 'amazing look', 'amazing love', 'amazing movie', 'amazing real', 'amazing realistic', 'amazing story', 'amazing visual', 'amazing voice', 'amazingly', 'amazingly realistic', 'anger', 'animal', 'animal absolutely', 'animal action', 'animal expression', 'animal face', 'animal great', 'animal life', 'animal little', 'animal look', 'animal planet', 'animal real', 'animal realistic', 'animal scenery', 'animal talk', 'animate', 'animate character', 'animate classic', 'animate feature', 'animate film', 'animate lion', 'animate movie', 'animate original', 'animate version', 'animate well', 'animated', 'animated film', 'animation', 'animation amazing', 'animation animal', 'animation awesome', 'animation beautiful', 'animation good', 'animation great', 'animation incredible', 'animation movie', 'animation music', 'animation new', 'animation real', 'animation realistic', 'animation unbelievable', 'animator', 'annoy', 'annoying', 'answer', 'apart', 'apathy', 'apathy dislike', 'appeal', 'appear', 'appreciate', 'art', 'artist', 'aside', 'ask', 'asleep', 'aspect', 'attempt', 'attention', 'attitude', 'audience', 'audio', 'available', 'avoid', 'aware', 'away', 'awesome', 'awesome animation', 'awesome enjoy', 'awesome great', 'awesome job', 'awesome love', 'awesome movie', 'awful', 'awkward', 'baby', 'background', 'bad', 'bad fake', 'bad good', 'bad movie', 'bad negtagnegvoice', 'bad review', 'balance', 'barely', 'base', 'basically', 'be', 'be glad', 'bear', 'beast', 'beast reference', 'beat', 'beautiful', 'beautiful animal', 'beautiful cgi', 'beautiful movie', 'beautiful music', 'beautiful scene', 'beautiful scenery', 'beautiful story', 'beautifully', 'beauty', 'beauty beast', 'begin', 'beginning', 'believable', 'believe', 'beloved', 'beloved classic', 'best', 'bet', 'better', 'better leave', 'better movie', 'better original', 'beyonc', 'beyonc voice', 'beyonce', 'beyonce donald', 'beyonce nala', 'beyonce singe', 'beyonce song', 'beyonce thing', 'beyonce voice', 'big', 'big fan', 'big screen', 'billy', 'billy eichner', 'bird', 'bit', 'bit slow', 'bland', 'blend', 'blow', 'body', 'book', 'bore', 'bore time', 'bored', 'boring', 'box', 'boy', 'bravo', 'breathtake', 'brilliant', 'bring', 'bring childhood', 'bring good', 'bring great', 'bring life', 'bring lot', 'bring memory', 'bring movie', 'bring tear', 'broadway', 'brother', 'buena', 'bunch', 'butcher', 'butcher prepare', 'buy', 'buy come', 'call', 'captivate', 'capture', 'carbon', 'carbon copy', 'care', 'cartoon', 'cartoon good', 'cartoon movie', 'cartoon version', 'cartoon well', 'case', 'cash', 'cash grab', 'cast', 'casting', 'cat', 'cause', 'celebrity', 'certain', 'certain scene', 'certainly', 'cg', 'cgi', 'cgi amazing', 'cgi animal', 'cgi come', 'cgi effect', 'cgi emotion', 'cgi great', 'cgi incredible', 'cgi look', 'cgi see', 'chance', 'change', 'change character', 'change little', 'change original', 'change scar', 'channel', 'character', 'character design', 'character development', 'character emotion', 'character face', 'character great', 'character know', 'character lack', 'character look', 'character real', 'character scene', 'character song', 'character talk', 'character timon', 'character voice', 'charm', 'charm original', 'check', 'chemistry', 'child', 'child great', 'child love', 'child movie', 'child watch', 'childhood', 'childhood classic', 'childhood favorite', 'childhood love', 'childhood memory', 'childhood movie', 'children', 'chill', 'chiwetel', 'chiwetel ejiofor', 'choice', 'choose', 'cinema', 'cinematic', 'cinematography', 'cinematography amazing', 'cinematography great', 'circle', 'circle life', 'classic', 'classic bring', 'classic movie', 'classic story', 'clearly', 'clever', 'close', 'close original', 'closely', 'closer', 'closer original', 'cold', 'color', 'colorful', 'colour', 'come', 'come dvd', 'come life', 'come movie', 'comedy', 'comic', 'comic relief', 'compare', 'compare original', 'comparison', 'comparison original', 'compelling', 'complaint', 'complete', 'completely', 'completely ruin', 'computer', 'computer animation', 'computer generate', 'computer graphic', 'con', 'connect', 'connection', 'connection character', 'consider', 'content', 'contrast', 'convey', 'convey emotion', 'cool', 'copy', 'copy original', 'counterfeit', 'couple', 'course', 'craft', 'crap', 'create', 'creation', 'creative', 'creativity', 'creature', 'credit', 'creepy', 'critic', 'crude', 'cry', 'cry mufasa', 'cub', 'curious', 'current', 'cut', 'cut lot', 'cute', 'cute movie', 'dad', 'dance', 'dare', 'dark', 'date', 'daughter', 'daughter love', 'day', 'de', 'de la', 'dead', 'deal', 'death', 'decent', 'decide', 'def', 'definite', 'definitely', 'definitely recommend', 'definitely worth', 'deliver', 'depress', 'depth', 'deserve', 'design', 'desire', 'despite', 'destroy', 'detail', 'development', 'dialog', 'dialogue', 'die', 'difference', 'different', 'different cartoon', 'different original', 'difficult', 'digital', 'diminish', 'diminish negtagnegnew', 'direct', 'direction', 'director', 'disagree', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disbelieve', 'discovery', 'discovery channel', 'dislike', 'dislike beyonce', 'dislike fake', 'dislike movie', 'dislike negtagneghow', 'dislikes', 'disney', 'disney classic', 'disney film', 'disney great', 'disney live', 'disney movie', 'disney need', 'disney original', 'disney remake', 'disney take', 'display', 'disregard', 'distraction', 'documentary', 'dollar', 'donald', 'donald glover', 'downplay', 'drag', 'drama', 'dramatic', 'draw', 'dry', 'duet', 'dull', 'dvd', 'earl', 'earl jone', 'early', 'earth', 'easy', 'eat', 'ed', 'edge', 'effect', 'effect amazing', 'effect great', 'effective', 'effort', 'eichner', 'eichner seth', 'eichner timon', 'ejiofor', 'element', 'eliminate', 'elton', 'elton john', 'elton johns', 'emote', 'emotion', 'emotion animal', 'emotion cartoon', 'emotion character', 'emotion face', 'emotion heart', 'emotion not', 'emotion original', 'emotion voice', 'emotional', 'emotionally', 'emotionless', 'emphasis', 'end', 'ending', 'energy', 'engage', 'enhance', 'enjoy', 'enjoy film', 'enjoy great', 'enjoy live', 'enjoy movie', 'enjoy music', 'enjoy original', 'enjoy remake', 'enjoyable', 'enjoyable movie', 'enjoyment', 'entertain', 'entertaining', 'entertaining movie', 'entertainment', 'enthusiasm', 'entire', 'entire family', 'entire movie', 'environment', 'epic', 'equally', 'es', 'especially', 'event', 'exact', 'exact copy', 'exact original', 'exact remake', 'exactly', 'exactly like', 'exactly original', 'example', 'exceed', 'exceed expectation', 'excellent', 'excellent graphic', 'excellent movie', 'excellent remake', 'exception', 'exceptional', 'excite', 'excited', 'exciting', 'execute', 'exist', 'expect', 'expect good', 'expectation', 'experience', 'explain', 'express', 'express emotion', 'expression', 'expression character', 'expression emotion', 'expression like', 'expression look', 'expression voice', 'expressionless', 'extent', 'extra', 'extremely', 'eye', 'fabulous', 'face', 'facial', 'facial emotion', 'facial expression', 'fact', 'factor', 'fail', 'fail capture', 'fairly', 'faithful', 'faithful original', 'fake', 'fake negtagnegstill', 'fall', 'fall asleep', 'fall flat', 'fall love', 'fall short', 'familiar', 'family', 'family enjoy', 'family love', 'family movie', 'fan', 'fan lion', 'fan original', 'fantastic', 'fantastic job', 'fantastic movie', 'far', 'far good', 'far well', 'fast', 'father', 'favorite', 'favorite disney', 'favorite movie', 'favorite part', 'favorite scene', 'favorite song', 'favreau', 'favreaus', 'feature', 'feel', 'feel bit', 'feel emotion', 'feel flat', 'feel good', 'feel like', 'feel love', 'feel movie', 'feel rush', 'feeling', 'female', 'fight', 'fight scene', 'figure', 'fill', 'film', 'film classic', 'film feel', 'film great', 'film making', 'film original', 'film see', 'filme', 'final', 'finally', 'find', 'fine', 'finish', 'fit', 'flat', 'flat bore', 'flaw', 'flawless', 'flop', 'flow', 'focus', 'follow', 'follow animate', 'follow cartoon', 'follow original', 'follow story', 'force', 'forever', 'forget', 'form', 'forward', 'frame', 'frame frame', 'frame remake', 'fresh', 'friend', 'frustrating', 'fun', 'fun family', 'fun movie', 'fun original', 'fun watch', 'funniest', 'funny', 'funny entertaining', 'funny moment', 'funny movie', 'funny part', 'future', 'game', 'garbage', 'general', 'generally', 'generate', 'generation', 'genius', 'genuinely', 'geo', 'geographic', 'get', 'get bore', 'get well', 'girl', 'give', 'give star', 'glad', 'glad see', 'glover', 'glover beyonce', 'glover simba', 'go', 'go happen', 'go home', 'go movie', 'go to', 'god', 'goldberg', 'good', 'good animate', 'good animation', 'good cgi', 'good disney', 'good enjoy', 'good family', 'good film', 'good graphic', 'good job', 'good kid', 'good like', 'good lion', 'good love', 'good memory', 'good movie', 'good music', 'good negtagneggreat', 'good original', 'good part', 'good quality', 'good remake', 'good scene', 'good song', 'good story', 'good thing', 'good time', 'good voice', 'good watch', 'good way', 'good worse', 'gorgeous', 'grab', 'grab disney', 'grand', 'grand daughter', 'grandchild', 'granddaughter', 'granddaughter love', 'grandkid', 'grandkid love', 'grandson', 'grandson love', 'graphic', 'graphic amazing', 'graphic animal', 'graphic awesome', 'graphic good', 'graphic great', 'great', 'great animal', 'great animation', 'great cast', 'great casting', 'great cgi', 'great change', 'great character', 'great cinematography', 'great computer', 'great disney', 'great family', 'great film', 'great humor', 'great job', 'great look', 'great memory', 'great movie', 'great music', 'great not', 'great remake', 'great song', 'great story', 'great storyline', 'great technology', 'great time', 'great visual', 'great voice', 'greatness', 'group', 'grow', 'grow lion', 'grow original', 'grow watch', 'guess', 'guest', 'guy', 'hakuna', 'hakuna matata', 'half', 'half star', 'han', 'han zimmer', 'hand', 'happen', 'happy', 'hard', 'hard believe', 'hard work', 'hat', 'hate', 'have', 'have see', 'head', 'hear', 'heart', 'heart original', 'heart soul', 'heartfelt', 'heartwarme', 'help', 'hero', 'high', 'high hope', 'highly', 'highly recommend', 'hilarious', 'hire', 'history', 'hit', 'hold', 'hold true', 'hole', 'hollow', 'hollywood', 'home', 'home watch', 'honest', 'honestly', 'hope', 'hope remake', 'horrible', 'horribly', 'hour', 'huge', 'human', 'humor', 'humor original', 'humorous', 'humour', 'hurt', 'husband', 'husband love', 'hyena', 'hype', 'hyper', 'hyper realistic', 'iconic', 'idea', 'identical', 'identical original', 'idiot', 'ill', 'image', 'imagery', 'imagine', 'imax', 'impact', 'impactful', 'important', 'important scene', 'impossible', 'impressed', 'impressive', 'improve', 'improvement', 'include', 'incredible', 'incredible movie', 'incredible music', 'incredibly', 'incredibly realistic', 'inspiration', 'instead', 'intense', 'interact', 'interaction', 'interest', 'interesting', 'invest', 'involve', 'iron', 'iron man', 'issue', 'jam', 'jam earl', 'jame', 'jame earl', 'jeremy', 'jeremy iron', 'job', 'job remake', 'john', 'john oliver', 'johns', 'joke', 'jon', 'jon favreau', 'jone', 'jone mufasa', 'jone sound', 'jone voice', 'journey', 'joy', 'jump', 'jungle', 'jungle book', 'justice', 'keep', 'keep original', 'keep true', 'key', 'key moment', 'kid', 'kid adult', 'kid enjoy', 'kid get', 'kid good', 'kid grow', 'kid like', 'kid love', 'kid movie', 'kid overall', 'kid watch', 'kill', 'kill movie', 'kind', 'kind bore', 'kinda', 'king', 'king child', 'king disney', 'king favorite', 'king film', 'king good', 'king great', 'king like', 'king live', 'king movie', 'king new', 'king original', 'king remake', 'king scene', 'know', 'know exactly', 'know go', 'know story', 'la', 'la original', 'lack', 'lack depth', 'lack emotion', 'lack emotional', 'lack energy', 'lack expression', 'lack facial', 'lack heart', 'lack lot', 'lack passion', 'lackluster', 'landscape', 'lane', 'language', 'largely', 'lastly', 'late', 'laugh', 'laugh cry', 'laughter', 'lazy', 'lead', 'learn', 'leave', 'leave theater', 'length', 'lesson', 'let', 'level', 'life', 'life animal', 'life like', 'life version', 'lifeless', 'lifelike', 'light', 'lighting', 'like', 'like actor', 'like aladdin', 'like animal', 'like animate', 'like better', 'like cartoon', 'like change', 'like character', 'like emotion', 'like fact', 'like follow', 'like go', 'like good', 'like great', 'like jungle', 'like kid', 'like little', 'like live', 'like lot', 'like movie', 'like music', 'like new', 'like original', 'like prepare', 'like real', 'like realistic', 'like see', 'like story', 'like take', 'like version', 'like voice', 'like watch', 'likely', 'line', 'line animate', 'line change', 'line great', 'line love', 'line movie', 'line original', 'lion', 'lion cub', 'lion face', 'lion king', 'lion look', 'list', 'listen', 'literally', 'little', 'little bit', 'little different', 'little emotion', 'little kid', 'little long', 'little scary', 'little thing', 'little violent', 'live', 'live action', 'live animal', 'live version', 'lol', 'long', 'long time', 'longer', 'look', 'look amazing', 'look animal', 'look beautiful', 'look forward', 'look great', 'look life', 'look like', 'look real', 'look realistic', 'looking', 'lose', 'lot', 'lot fun', 'lot memory', 'lot part', 'lot people', 'lot voice', 'loud', 'love', 'love amazing', 'love animation', 'love beyonce', 'love bring', 'love cgi', 'love character', 'love classic', 'love cry', 'love definitely', 'love disney', 'love fact', 'love family', 'love favorite', 'love feel', 'love funny', 'love good', 'love graphic', 'love great', 'love keep', 'love kid', 'love like', 'love lion', 'love love', 'love minute', 'love movie', 'love music', 'love negtagnegsure', 'love new', 'love not', 'love original', 'love real', 'love realistic', 'love recommend', 'love remake', 'love second', 'love see', 'love song', 'love stay', 'love stick', 'love story', 'love take', 'love time', 'love tonight', 'love true', 'love version', 'love watch', 'love way', 'love year', 'lovely', 'mad', 'magic', 'magic original', 'magnificent', 'main', 'main character', 'mainly', 'major', 'make', 'make movie', 'making', 'man', 'manage', 'mark', 'marvel', 'masterpiece', 'matata', 'match', 'match original', 'material', 'matter', 'maybe', 'mean', 'meaning', 'mediocre', 'medium', 'meh', 'melancholy', 'member', 'memorable', 'memory', 'memory child', 'memory childhood', 'mess', 'message', 'middle', 'mind', 'minor', 'minute', 'minute late', 'minute movie', 'miscast', 'miss', 'miss lot', 'miss original', 'mockery', 'model', 'modern', 'mom', 'moment', 'money', 'monotone', 'moral', 'mother', 'motion', 'mouse', 'mouth', 'move', 'movement', 'movie', 'movie absolutely', 'movie age', 'movie amazing', 'movie animal', 'movie animation', 'movie awesome', 'movie bad', 'movie beautiful', 'movie beautifully', 'movie begin', 'movie better', 'movie beyonce', 'movie bore', 'movie bring', 'movie cartoon', 'movie cgi', 'movie change', 'movie child', 'movie classic', 'movie come', 'movie compare', 'movie complete', 'movie completely', 'movie definitely', 'movie different', 'movie dislike', 'movie disney', 'movie enjoy', 'movie exactly', 'movie excellent', 'movie family', 'movie far', 'movie favorite', 'movie feel', 'movie get', 'movie glad', 'movie go', 'movie good', 'movie graphic', 'movie great', 'movie grow', 'movie have', 'movie highly', 'movie kid', 'movie lack', 'movie life', 'movie like', 'movie lion', 'movie little', 'movie live', 'movie long', 'movie look', 'movie love', 'movie minute', 'movie miss', 'movie movie', 'movie new', 'movie not', 'movie ok', 'movie original', 'movie overall', 'movie people', 'movie pretty', 'movie real', 'movie realistic', 'movie recommend', 'movie remake', 'movie remind', 'movie review', 'movie see', 'movie slow', 'movie song', 'movie star', 'movie start', 'movie stay', 'movie stick', 'movie take', 'movie theater', 'movie think', 'movie time', 'movie true', 'movie visually', 'movie voice', 'movie watch', 'movie way', 'movie well', 'movie wish', 'movie year', 'movie young', 'moving', 'mufasa', 'mufasa die', 'mufasas', 'mufasas death', 'multiple', 'music', 'music amazing', 'music animation', 'music awesome', 'music good', 'music great', 'music voice', 'music wonderful', 'musical', 'musical number', 'nada', 'nail', 'nala', 'nala sound', 'nalas', 'nat', 'nat geo', 'nathan', 'nathan lane', 'national', 'national geographic', 'natural', 'naturally', 'nature', 'nature documentary', 'near', 'nearly', 'need', 'need remake', 'negative', 'negative review', 'negtagnegable', 'negtagnegact', 'negtagnegaction', 'negtagnegafter', 'negtagnegage', 'negtagneganyone', 'negtagnegcapture', 'negtagnegcast', 'negtagnegcgi', 'negtagnegchange', 'negtagnegchild', 'negtagnegchoose', 'negtagnegclassic', 'negtagnegclose', 'negtagnegcome', 'negtagnegcompare', 'negtagnegcompare fake', 'negtagnegcompletely', 'negtagnegcomputer', 'negtagnegconnect', 'negtagnegcould', 'negtagnegcritic', 'negtagnegdifferent', 'negtagnegdisappoint', 'negtagnegdisplay', 'negtagnegdocumentary', 'negtagnegdown', 'negtagnegemotion', 'negtagnegemotional', 'negtagnegend', 'negtagnegenjoy', 'negtagnegenough', 'negtagnegeven', 'negtagnegeveryone', 'negtagnegexact', 'negtagnegexact negtagnegsame', 'negtagnegexcite', 'negtagnegexpectation', 'negtagnegexpression', 'negtagnegface', 'negtagnegfan', 'negtagnegfan fake', 'negtagnegfew', 'negtagnegfirst', 'negtagnegfit', 'negtagnegflow', 'negtagnegfollow', 'negtagnegfrom', 'negtagnegfrom fake', 'negtagnegfull', 'negtagnegget', 'negtagneggive', 'negtagneggo', 'negtagneggreat', 'negtagneghead', 'negtagneghear', 'negtagnegheart', 'negtagnegheart fake', 'negtagneghelp', 'negtagneghit', 'negtagneghold', 'negtagneghow', 'negtagneghuman', 'negtagneghumor', 'negtagneghype', 'negtagneginto', 'negtagnegjust', 'negtagnegkid', 'negtagnegknow', 'negtagnegknow negtagneghow', 'negtagneglack', 'negtagneglack negtagnegemotion', 'negtagneglife', 'negtagneglittle', 'negtagneglive', 'negtagneglive fake', 'negtagneglive negtagnegaction', 'negtagneglook', 'negtagneglove', 'negtagnegmagic', 'negtagnegmagic fake', 'negtagnegmany', 'negtagnegmatch', 'negtagnegmean', 'negtagnegmind', 'negtagnegmiss', 'negtagnegmoment', 'negtagnegmoney', 'negtagnegmore', 'negtagnegmufasa', 'negtagnegmusic', 'negtagnegnecessary', 'negtagnegnee', 'negtagnegnew', 'negtagnegold', 'negtagnegone', 'negtagnegonly', 'negtagnegother', 'negtagnegover', 'negtagnegoverall', 'negtagnegpart', 'negtagnegpay', 'negtagnegpeople', 'negtagnegperformance', 'negtagnegperson', 'negtagnegplay', 'negtagnegpossible', 'negtagnegprepare', 'negtagnegpumbaa', 'negtagnegpurchase', 'negtagnegquality', 'negtagnegquite', 'negtagnegread', 'negtagnegrealistic', 'negtagnegreally', 'negtagnegrecommend', 'negtagnegrefund', 'negtagnegreview', 'negtagnegright', 'negtagnegrole', 'negtagnegsame', 'negtagnegsay', 'negtagnegscar', 'negtagnegscene', 'negtagnegsee', 'negtagnegsee destroy', 'negtagnegsee fake', 'negtagnegsee negtagnegfirst', 'negtagnegseem', 'negtagnegshould', 'negtagnegshow', 'negtagnegsimba', 'negtagnegsing', 'negtagnegsinge', 'negtagnegsmall', 'negtagnegsong', 'negtagnegsound', 'negtagnegspecial', 'negtagnegspend', 'negtagnegstart', 'negtagnegstill', 'negtagnegstop', 'negtagnegstory', 'negtagnegsure', 'negtagnegtake', 'negtagnegtalk', 'negtagnegtell', 'negtagnegterrible', 'negtagnegtheater', 'negtagnegthrough', 'negtagnegticket', 'negtagnegtime', 'negtagnegtouch', 'negtagnegtry', 'negtagneguse', 'negtagnegusually', 'negtagnegvoice', 'negtagnegvoice negtagnegact', 'negtagnegwait', 'negtagnegwant', 'negtagnegwaste', 'negtagnegwaste negtagnegmoney', 'negtagnegwatch', 'negtagnegwell', 'negtagnegwhat', 'negtagnegwhat negtagnegcritic', 'negtagnegwhole', 'negtagnegwill', 'negtagnegwithout', 'negtagnegwork', 'negtagnegwrong', 'negtagnegyet', 'negtagnegyoung', 'new', 'new cast', 'new generation', 'new joke', 'new lion', 'new movie', 'new original', 'new scene', 'new song', 'new version', 'nice', 'nice see', 'non', 'nostalgia', 'nostalgic', 'not', 'not change', 'not feel', 'not fit', 'not know', 'not like', 'not think', 'not wait', 'not well', 'note', 'notice', 'number', 'nyla', 'obvious', 'obviously', 'odd', 'offer', 'og', 'oh', 'ok', 'ok prefer', 'ok special', 'okay', 'old', 'old enjoy', 'old granddaughter', 'old grandson', 'old kid', 'old love', 'old movie', 'old son', 'old version', 'oliver', 'omg', 'one', 'open', 'open scene', 'opening', 'opinion', 'original', 'original add', 'original amazing', 'original animate', 'original animated', 'original animation', 'original awesome', 'original bad', 'original better', 'original cartoon', 'original change', 'original classic', 'original closely', 'original come', 'original daughter', 'original definitely', 'original disney', 'original enjoy', 'original favorite', 'original film', 'original fun', 'original good', 'original graphic', 'original great', 'original heart', 'original hold', 'original kid', 'original like', 'original lion', 'original lot', 'original love', 'original miss', 'original movie', 'original music', 'original negtagnegsee', 'original new', 'original script', 'original song', 'original soundtrack', 'original story', 'original storyline', 'original think', 'original time', 'original unlike', 'original version', 'original visually', 'original voice', 'original way', 'original well', 'originality', 'outstanding', 'over', 'overall', 'overall good', 'overall great', 'overall movie', 'pace', 'pacing', 'paltry', 'parent', 'part', 'part movie', 'part original', 'particular', 'particularly', 'pass', 'passion', 'past', 'pay', 'people', 'perfect', 'perfection', 'perfectly', 'performance', 'period', 'person', 'personality', 'personally', 'phenomenal', 'phone', 'photo', 'photo realistic', 'photography', 'photorealism', 'photorealistic', 'pick', 'picture', 'picture quality', 'piece', 'place', 'plan', 'planet', 'planet earth', 'play', 'playing', 'pleasantly', 'pleased', 'pleasure', 'plot', 'plus', 'point', 'pointless', 'political', 'poor', 'poorly', 'popular', 'portray', 'positive', 'possible', 'potential', 'power', 'powerful', 'practically', 'predictable', 'prefer', 'prefer animate', 'prefer original', 'prepare', 'prepare song', 'present', 'presentation', 'pretty', 'pretty close', 'pretty cool', 'pretty good', 'pretty story', 'preview', 'previous', 'price', 'price admission', 'pride', 'pride rock', 'probably', 'problem', 'produce', 'production', 'project', 'provide', 'pull', 'pumba', 'pumba good', 'pumba hilarious', 'pumba timon', 'pumbaa', 'pumbaa timon', 'punch', 'purchase', 'push', 'quality', 'quantity', 'que', 'queen', 'question', 'questionable', 'quickly', 'quiet', 'quote', 'rafiki', 'raise', 'rate', 'rate star', 'rating', 'raw', 'raw emotion', 'reach', 'reaction', 'read', 'read script', 'ready', 'real', 'real amazing', 'real animal', 'real emotion', 'real great', 'real life', 'real look', 'real scenery', 'realism', 'realism animal', 'realistic', 'realistic animal', 'realistic cgi', 'realistic character', 'realistic feel', 'realistic graphic', 'realistic look', 'realistic looking', 'realistic version', 'reality', 'realization', 'reason', 'reboot', 'recent', 'recognizable', 'recommend', 'recommend age', 'recommend movie', 'recommend see', 'record', 'recreate', 'recreation', 'recreation original', 'redo', 'redone', 'reference', 'refreshing', 'regret', 'rehash', 'reimagine', 'reimagining', 'relationship', 'release', 'relief', 'relinquish', 'relive', 'relive childhood', 'remain', 'remake', 'remake add', 'remake animate', 'remake animation', 'remake cartoon', 'remake change', 'remake classic', 'remake disney', 'remake film', 'remake go', 'remake good', 'remake lion', 'remake movie', 'remake original', 'remake stay', 'remake think', 'remarkable', 'remember', 'remind', 'remind childhood', 'remixe', 'remove', 'rendition', 'replace', 'replace original', 'repulse', 'rest', 'result', 'retelling', 'return', 'review', 'rewrite', 'riff', 'right', 'rip', 'roar', 'rock', 'rogan', 'rogan pumba', 'rogen', 'rogen pumba', 'role', 'room', 'ruin', 'run', 'rush', 'sad', 'sad funny', 'sadly', 'sake', 'save', 'say', 'scale', 'scar', 'scar prepare', 'scar song', 'scar voice', 'scare', 'scary', 'scary little', 'scene', 'scene animate', 'scene cut', 'scene dialogue', 'scene feel', 'scene movie', 'scene original', 'scene overall', 'scene rafiki', 'scene remake', 'scene scene', 'scene simba', 'scenery', 'scenery amazing', 'scenery beautiful', 'scenery music', 'scheme', 'school', 'score', 'scream', 'screen', 'screenplay', 'script', 'seat', 'second', 'second time', 'see', 'see animate', 'see kid', 'see life', 'see lion', 'see movie', 'see original', 'see real', 'see theater', 'see time', 'see twice', 'self', 'sell', 'senior', 'sense', 'sequel', 'sequence', 'serengeti', 'seriously', 'set', 'set apart', 'seth', 'seth rogan', 'seth rogen', 'shakespearean', 'share', 'shoot', 'shoot shot', 'short', 'shorten', 'shot', 'shot remake', 'shot shot', 'show', 'show emotion', 'sick', 'simba', 'simba nala', 'simbas', 'similar', 'similar original', 'simply', 'sing', 'singe', 'singe song', 'singer', 'singing', 'single', 'sit', 'situation', 'skip', 'sleep', 'slight', 'slightly', 'slow', 'small', 'smart', 'smile', 'solid', 'something', 'somewhat', 'son', 'son enjoy', 'song', 'song feel', 'song great', 'song like', 'song movie', 'song new', 'soo', 'soon', 'sorry', 'soul', 'soul original', 'soulless', 'sound', 'sound like', 'soundtrack', 'source', 'source material', 'speak', 'special', 'special effect', 'spectacle', 'spectacular', 'speech', 'spend', 'spin', 'spirit', 'spoiler', 'spoof', 'spot', 'stage', 'stale', 'stampede', 'stand', 'star', 'start', 'start finish', 'stay', 'stay original', 'stay true', 'steal', 'step', 'stick', 'stick original', 'stick script', 'stick story', 'stop', 'story', 'story animation', 'story feel', 'story good', 'story great', 'story life', 'story like', 'story line', 'story little', 'story love', 'story music', 'story new', 'story original', 'story tell', 'story wise', 'storyline', 'strength', 'strong', 'stuff', 'stun', 'stunning', 'stunning movie', 'stupid', 'style', 'subpar', 'subtle', 'suck', 'suffer', 'super', 'superb', 'support', 'suppose', 'sure', 'surprise', 'surprised', 'surprisingly', 'sweet', 'switch', 'take', 'take away', 'take childhood', 'take daughter', 'take fun', 'take granddaughter', 'take kid', 'take year', 'talent', 'talented', 'talk', 'talk animal', 'talk look', 'talk sing', 'teach', 'tear', 'tear eye', 'technical', 'technically', 'technological', 'technology', 'tell', 'tell real', 'tell story', 'term', 'terrible', 'terrible movie', 'terrible voice', 'terrific', 'thank', 'theater', 'theatre', 'theatrical', 'thing', 'thing love', 'thing miss', 'thing movie', 'thing original', 'think', 'think beyonce', 'think cgi', 'think disney', 'think good', 'think great', 'think little', 'think movie', 'think original', 'think watch', 'think well', 'thoroughly', 'thoroughly enjoy', 'throne', 'thumb', 'ticket', 'time', 'time favorite', 'time feel', 'time love', 'time see', 'time watch', 'time year', 'timeless', 'timon', 'timon funny', 'timon pumba', 'timon pumbaa', 'timone', 'timone pumba', 'tired', 'to', 'today', 'tone', 'tonight', 'totally', 'touch', 'touching', 'track', 'traditional', 'transition', 'trash', 'true', 'true animate', 'true life', 'true original', 'true story', 'truly', 'truly enjoy', 'try', 'try movie', 'tune', 'turn', 'tweak', 'twice', 'twist', 'unbelievable', 'uncanny', 'uncanny valley', 'understand', 'understandable', 'underwhelme', 'underwhelming', 'undoubtedly', 'unfortunately', 'unimpressed', 'uninspired', 'unique', 'unlike', 'unnecessary', 'unnecessary remake', 'update', 'use', 'useless', 'usually', 'valley', 'value', 'version', 'version better', 'version feel', 'version good', 'version great', 'version lack', 'version lion', 'version movie', 'version music', 'version new', 'version original', 'version time', 'version watch', 'version well', 'vfx', 'vibe', 'vibrant', 'vibrant color', 'video', 'view', 'viewer', 'villain', 'villain song', 'violence', 'violent', 'virtually', 'visual', 'visual amazing', 'visual effect', 'visual great', 'visual impressive', 'visual movie', 'visual stunning', 'visually', 'visually amazing', 'visually beautiful', 'visually impressive', 'visually stunning', 'vocal', 'vocal performance', 'voice', 'voice act', 'voice acting', 'voice actor', 'voice bad', 'voice cast', 'voice character', 'voice great', 'voice lack', 'voice mufasa', 'voice nala', 'voice not', 'voice over', 'voice performance', 'voice simba', 'voice talent', 'voice work', 'wait', 'wait dvd', 'wait king', 'wait watch', 'walk', 'want', 'want enjoy', 'want money', 'warthog', 'waste', 'waste money', 'watch', 'watch animal', 'watch animate', 'watch cartoon', 'watch cgi', 'watch discovery', 'watch disney', 'watch enjoy', 'watch lion', 'watch movie', 'watch national', 'watch nature', 'watch new', 'watch original', 'watch planet', 'watch preview', 'watch real', 'watch time', 'water', 'way', 'way better', 'way emotion', 'way look', 'way movie', 'way watch', 'weak', 'week', 'weird', 'well', 'well expect', 'well graphic', 'well job', 'well original', 'whilst', 'whoopi', 'whoopi goldberg', 'wife', 'wild', 'wildlife', 'will', 'will not', 'win', 'wise', 'wish', 'wish movie', 'wish original', 'wonder', 'wonder original', 'wonderful', 'wonderful movie', 'wonderful remake', 'word', 'word word', 'work', 'world', 'worse', 'worse fake', 'worth', 'worth price', 'worth see', 'worth watch', 'worthy', 'would', 'wow', 'write', 'wrong', 'year', 'year ago', 'year old', 'yes', 'young', 'young child', 'young kid', 'youth', 'yr', 'yr old', 'zazu', 'zimmer']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(cvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making CountVectorize matrix as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvect_df = pd.DataFrame(X_train_cvect_matrix.toarray(), columns=cvect.get_feature_names())\n",
    "X_test_cvect_df  = pd.DataFrame(X_test_cvect_matrix.toarray(), columns=cvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely amazing</th>\n",
       "      <th>absolutely beautiful</th>\n",
       "      <th>absolutely love</th>\n",
       "      <th>absolutely stunning</th>\n",
       "      <th>accurate</th>\n",
       "      <th>achieve</th>\n",
       "      <th>achievement</th>\n",
       "      <th>...</th>\n",
       "      <th>year old</th>\n",
       "      <th>yes</th>\n",
       "      <th>young</th>\n",
       "      <th>young child</th>\n",
       "      <th>young kid</th>\n",
       "      <th>youth</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>zazu</th>\n",
       "      <th>zimmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolute  absolutely  absolutely amazing  absolutely beautiful  \\\n",
       "0     0         0           0                   0                     0   \n",
       "1     0         0           0                   0                     0   \n",
       "2     0         0           0                   0                     0   \n",
       "3     0         0           0                   0                     0   \n",
       "4     0         0           0                   0                     0   \n",
       "\n",
       "   absolutely love  absolutely stunning  accurate  achieve  achievement  ...  \\\n",
       "0                0                    0         0        0            0  ...   \n",
       "1                0                    0         0        0            0  ...   \n",
       "2                0                    0         0        0            0  ...   \n",
       "3                0                    0         0        0            0  ...   \n",
       "4                0                    0         0        0            0  ...   \n",
       "\n",
       "   year old  yes  young  young child  young kid  youth  yr  yr old  zazu  \\\n",
       "0         0    0      0            0          0      0   0       0     0   \n",
       "1         0    0      0            0          0      0   0       0     0   \n",
       "2         0    0      0            0          0      0   0       0     0   \n",
       "3         0    0      0            0          0      0   0       0     0   \n",
       "4         0    0      0            0          0      0   0       0     0   \n",
       "\n",
       "   zimmer  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 2259 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cvect_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For future use\n",
    "X_train_cvect_fut = X_train_cvect_df.copy(deep=True)\n",
    "X_test_cvect_fut = X_test_cvect_df.copy(deep=True)\n",
    "\n",
    "y_train_cvect_fut = y_train.copy(deep=True)\n",
    "y_test_cvect_fut = y_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE the train data as there is data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train_cvect_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_cvect_df.shape = (2400, 2259)\n",
      "y_train.shape = (2400,)\n",
      "X_train_sm.shape = (3468, 2259)\n",
      "y_train_sm.shape = (3468,)\n",
      "sum(y_train_sm) = 1734\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_cvect_df.shape = {X_train_cvect_df.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")\n",
    "print(f\"X_train_sm.shape = {X_train_sm.shape}\")\n",
    "print(f\"y_train_sm.shape = {y_train_sm.shape}\")\n",
    "print(f\"sum(y_train_sm) = {sum(y_train_sm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.5\n",
      "\n",
      "Train data f1-score for class '0' 0.6666666666666666\n",
      "\n",
      "Train data f1-score for class '1' 0.0\n",
      "TEST DATA ACCURACY 0.73\n",
      "\n",
      "Test data f1-score for class '0' 0.8439306358381503\n",
      "\n",
      "Test data f1-score for class '1' 0.0\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7742214532871973\n",
      "\n",
      "Train data f1-score for class '0' 0.7729776746883155\n",
      "\n",
      "Train data f1-score for class '1' 0.7754516776598795\n",
      "TEST DATA ACCURACY 0.73\n",
      "\n",
      "Test data f1-score for class '0' 0.8052884615384616\n",
      "\n",
      "Test data f1-score for class '1' 0.5597826086956521\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.6882929642445214\n",
      "\n",
      "Train data f1-score for class '0' 0.5888170406998859\n",
      "\n",
      "Train data f1-score for class '1' 0.7490132342697933\n",
      "TEST DATA ACCURACY 0.5583333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.6074074074074074\n",
      "\n",
      "Test data f1-score for class '1' 0.4952380952380952\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8189158016147635\n",
      "\n",
      "Train data f1-score for class '0' 0.7980707395498392\n",
      "\n",
      "Train data f1-score for class '1' 0.8358599059069524\n",
      "TEST DATA ACCURACY 0.7266666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.7918781725888325\n",
      "\n",
      "Test data f1-score for class '1' 0.6019417475728156\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7990196078431373\n",
      "\n",
      "Train data f1-score for class '0' 0.7762439807383628\n",
      "\n",
      "Train data f1-score for class '1' 0.8175870191049464\n",
      "TEST DATA ACCURACY 0.71\n",
      "\n",
      "Test data f1-score for class '0' 0.7757731958762888\n",
      "\n",
      "Test data f1-score for class '1' 0.589622641509434\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8731257208765859\n",
      "\n",
      "Train data f1-score for class '0' 0.8637770897832817\n",
      "\n",
      "Train data f1-score for class '1' 0.8812736103615758\n",
      "TEST DATA ACCURACY 0.765\n",
      "\n",
      "Test data f1-score for class '0' 0.8257107540173053\n",
      "\n",
      "Test data f1-score for class '1' 0.6393861892583119\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.919838523644752\n",
      "\n",
      "Train data f1-score for class '0' 0.9167165967645298\n",
      "\n",
      "Train data f1-score for class '1' 0.9227348526959421\n",
      "TEST DATA ACCURACY 0.7833333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8422330097087378\n",
      "\n",
      "Test data f1-score for class '1' 0.6542553191489362\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.941753171856978\n",
      "\n",
      "Train data f1-score for class '0' 0.9398809523809524\n",
      "\n",
      "Train data f1-score for class '1' 0.9435123042505592\n",
      "TEST DATA ACCURACY 0.795\n",
      "\n",
      "Test data f1-score for class '0' 0.8512696493349455\n",
      "\n",
      "Test data f1-score for class '1' 0.6702412868632709\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.9884659746251442\n",
      "\n",
      "Train data f1-score for class '0' 0.9883788495061012\n",
      "\n",
      "Train data f1-score for class '1' 0.9885518030910131\n",
      "TEST DATA ACCURACY 0.7766666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8393285371702638\n",
      "\n",
      "Test data f1-score for class '1' 0.6338797814207651\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9821222606689735\n",
      "\n",
      "Train data f1-score for class '0' 0.9819136522753792\n",
      "\n",
      "Train data f1-score for class '1' 0.9823261117445838\n",
      "TEST DATA ACCURACY 0.7783333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.842603550295858\n",
      "\n",
      "Test data f1-score for class '1' 0.6253521126760564\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.9945213379469435\n",
      "\n",
      "Train data f1-score for class '0' 0.9945070829719572\n",
      "\n",
      "Train data f1-score for class '1' 0.994535519125683\n",
      "TEST DATA ACCURACY 0.7633333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8325471698113208\n",
      "\n",
      "Test data f1-score for class '1' 0.5965909090909092\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9916378316032295\n",
      "\n",
      "Train data f1-score for class '0' 0.9915868871482449\n",
      "\n",
      "Train data f1-score for class '1' 0.9916881627973632\n",
      "TEST DATA ACCURACY 0.7633333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8317535545023697\n",
      "\n",
      "Test data f1-score for class '1' 0.601123595505618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=X_test_cvect_df\n",
    "\n",
    "C=[0.001,0.01,0.1,1,10,100]\n",
    "Penalty=['l1','l2']\n",
    "        \n",
    "logistic_regr(X_train_sm,y_train_sm,X_test,C_list,Penalty,Features_name=\"CountVect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "CV_XGB.best_score_  0.8281430219146482\n",
      "CV_XGB.best_params_  {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}\n",
      "TRAIN DATA ACCURACY 0.8976355247981546\n",
      "\n",
      "Train data f1-score for class '0' 0.8910708806382326\n",
      "\n",
      "Train data f1-score for class '1' 0.9034539026380202\n",
      "TEST DATA ACCURACY 0.7633333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8301435406698564\n",
      "\n",
      "Test data f1-score for class '1' 0.6098901098901099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=-1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'colsample_bytree': array([0.5, 0.6, 0.7, 0.8, 0.9]), 'n_estimators': [100], 'max_depth': [10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test_cvect_df\n",
    "\n",
    "param_grid = {\n",
    "    'colsample_bytree': np.linspace(0.5, 0.9, 5),\n",
    "    'n_estimators':[100],\n",
    "    'max_depth': [10]\n",
    "}\n",
    "\n",
    "xgb_model(X_train_sm,y_train_sm,X_test,param_grid,\"CountVect\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "rfc_cv_grid.best_score_  0.7713379469434832\n",
      "rfc_cv_grid.best_params_  {'n_estimators': 75, 'min_samples_leaf': 10, 'max_depth': 12, 'class_weight': 'balanced_subsample'}\n",
      "TRAIN DATA ACCURACY 0.8013264129181085\n",
      "\n",
      "Train data f1-score for class '0' 0.7812003810733565\n",
      "\n",
      "Train data f1-score for class '1' 0.8180617903353578\n",
      "TEST DATA ACCURACY 0.71\n",
      "\n",
      "Test data f1-score for class '0' 0.7846534653465347\n",
      "\n",
      "Test data f1-score for class '1' 0.5561224489795918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [10, 25, 50, 75, 100], 'max_depth': [10, 12, 14, 16, 18, 20], 'min_samples_leaf': [5, 10, 15, 20], 'class_weight': ['balanced', 'balanced_subsample']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test_cvect_df\n",
    "\n",
    "param_grid = {\"n_estimators\" : [10, 25, 50, 75, 100],\n",
    "           \"max_depth\" : [10, 12, 14, 16, 18, 20],\n",
    "           \"min_samples_leaf\" : [5, 10, 15, 20],\n",
    "           \"class_weight\" : ['balanced','balanced_subsample']}\n",
    "\n",
    "\n",
    "rfc_model(X_train_sm,y_train_sm,X_test,param_grid,\"CountVect\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mix Text and non-Text data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Non-text Features +  tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   targetSentiment                                      antonymns_upd  \\\n",
       "0                0                    like animation animal look real   \n",
       "1                0                 amazing realistic incredible music   \n",
       "2                0        classic good remake love glover outstanding   \n",
       "3                1  nice animation cgi completely lack disney styl...   \n",
       "4                0                      good mainly sentimental value   \n",
       "\n",
       "   adj_count  adv_verb_count  review_polar  adj_polar  adv_verb_polar  \\\n",
       "0        2.0             2.0      0.433333   0.200000        0.000000   \n",
       "1        3.0             0.0      0.605556   0.555556        0.000000   \n",
       "2        4.0             1.0      0.516667   0.466667        0.500000   \n",
       "3        5.0             2.0      0.318750   0.391667        0.100000   \n",
       "4        2.0             1.0      0.205556   0.225000        0.166667   \n",
       "\n",
       "   count_words  count_capital_words  count_love  count_shit  \\\n",
       "0           12                    0           0           0   \n",
       "1            6                    0           2           0   \n",
       "2            8                    0           2           1   \n",
       "3           27                    1           0           1   \n",
       "4            8                    0           0           0   \n",
       "\n",
       "   count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                  0                 0                0             0  \n",
       "1                  0                 1                0             0  \n",
       "2                  0                 0                0             0  \n",
       "3                  1                 0                0             0  \n",
       "4                  0                 0                0             0  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_data_text.drop(targetSentiment,axis=1)\n",
    "y = final_data_text[targetSentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       antonymns_upd  adj_count  \\\n",
       "0                    like animation animal look real        2.0   \n",
       "1                 amazing realistic incredible music        3.0   \n",
       "2        classic good remake love glover outstanding        4.0   \n",
       "3  nice animation cgi completely lack disney styl...        5.0   \n",
       "4                      good mainly sentimental value        2.0   \n",
       "\n",
       "   adv_verb_count  review_polar  adj_polar  adv_verb_polar  count_words  \\\n",
       "0             2.0      0.433333   0.200000        0.000000           12   \n",
       "1             0.0      0.605556   0.555556        0.000000            6   \n",
       "2             1.0      0.516667   0.466667        0.500000            8   \n",
       "3             2.0      0.318750   0.391667        0.100000           27   \n",
       "4             1.0      0.205556   0.225000        0.166667            8   \n",
       "\n",
       "   count_capital_words  count_love  count_shit  count_quest_marks  \\\n",
       "0                    0           0           0                  0   \n",
       "1                    0           2           0                  0   \n",
       "2                    0           2           1                  0   \n",
       "3                    1           0           1                  1   \n",
       "4                    0           0           0                  0   \n",
       "\n",
       "   count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                 0                0             0  \n",
       "1                 1                0             0  \n",
       "2                 0                0             0  \n",
       "3                 0                0             0  \n",
       "4                 0                0             0  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tfidf matrix\n",
    "\n",
    "# Taking both unigram and bigram\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=10000000,\n",
    "                                 min_df=0.01,\n",
    "                                 use_idf=True, ngram_range=(1,2))\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(X_train[clean_after_antonymns])\n",
    "\n",
    "X_train_tfidf_matrix= tfidf_vectorizer.transform(X_train[clean_after_antonymns])\n",
    "X_test_tfidf_matrix = tfidf_vectorizer.transform(X_test[clean_after_antonymns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2400x222 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<600x222 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4464 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely', 'absolutely love', 'act', 'acting', 'action', 'actor', 'add', 'age', 'amazing', 'animal', 'animal look', 'animate', 'animate version', 'animation', 'apathy', 'away', 'awesome', 'bad', 'beautiful', 'beautifully', 'better', 'beyonce', 'big', 'bit', 'bore', 'boring', 'bring', 'cartoon', 'cartoon version', 'cast', 'cgi', 'change', 'character', 'child', 'childhood', 'cinematography', 'classic', 'close', 'come', 'compare', 'cry', 'cute', 'daughter', 'definitely', 'destroy', 'different', 'disappointed', 'dislike', 'disney', 'disney movie', 'earl', 'earl jone', 'effect', 'emotion', 'emotional', 'enjoy', 'enjoy movie', 'enjoyable', 'entertaining', 'especially', 'exactly', 'excellent', 'expect', 'experience', 'expression', 'face', 'facial', 'fact', 'fake', 'fall', 'family', 'fantastic', 'far', 'favorite', 'feel', 'feel like', 'film', 'flat', 'follow', 'forget', 'fun', 'funny', 'get', 'give', 'glad', 'go', 'good', 'good movie', 'graphic', 'great', 'great job', 'great movie', 'grow', 'hard', 'have', 'heart', 'humor', 'incredible', 'job', 'jone', 'keep', 'kid', 'king', 'know', 'lack', 'lack emotion', 'laugh', 'leave', 'life', 'life like', 'like', 'like movie', 'like original', 'like watch', 'line', 'lion', 'lion king', 'little', 'live', 'live action', 'long', 'look', 'look real', 'lot', 'love', 'love movie', 'make', 'maybe', 'memory', 'minute', 'miss', 'moment', 'money', 'movie', 'movie good', 'movie great', 'movie love', 'mufasa', 'music', 'nala', 'need', 'negtagnegsee', 'new', 'nice', 'not', 'ok', 'old', 'opinion', 'original', 'original lion', 'original movie', 'overall', 'part', 'people', 'perfect', 'performance', 'play', 'point', 'prepare', 'pretty', 'pumba', 'real', 'real animal', 'realistic', 'recommend', 'remake', 'sad', 'say', 'scar', 'scene', 'scenery', 'script', 'see', 'seth', 'shot', 'show', 'simba', 'sing', 'singing', 'slow', 'song', 'sound', 'special', 'star', 'start', 'stay', 'stay true', 'stick', 'story', 'story line', 'storyline', 'stunning', 'take', 'talk', 'tell', 'theater', 'thing', 'think', 'time', 'timon', 'timon pumba', 'true', 'true original', 'truly', 'try', 'version', 'visual', 'visually', 'voice', 'voice actor', 'wait', 'want', 'watch', 'way', 'well', 'wish', 'wonderful', 'work', 'worth', 'year', 'year old', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2400x222 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names(),index=X_train.index)\n",
    "X_test_tfidf_df  = pd.DataFrame(X_test_tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names(),index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely love</th>\n",
       "      <th>act</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>add</th>\n",
       "      <th>age</th>\n",
       "      <th>amazing</th>\n",
       "      <th>animal</th>\n",
       "      <th>...</th>\n",
       "      <th>watch</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>wish</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      absolutely  absolutely love  act  acting  action  actor  add  age  \\\n",
       "642          0.0              0.0  0.0     0.0     0.0    0.0  0.0  0.0   \n",
       "700          0.0              0.0  0.0     0.0     0.0    0.0  0.0  0.0   \n",
       "226          0.0              0.0  0.0     0.0     0.0    0.0  0.0  0.0   \n",
       "1697         0.0              0.0  0.0     0.0     0.0    0.0  0.0  0.0   \n",
       "1010         0.0              0.0  0.0     0.0     0.0    0.0  0.0  0.0   \n",
       "\n",
       "       amazing    animal  ...  watch  way  well  wish  wonderful  work  worth  \\\n",
       "642   0.000000  0.000000  ...    0.0  0.0   0.0   0.0        0.0   0.0    0.0   \n",
       "700   0.000000  0.226768  ...    0.0  0.0   0.0   0.0        0.0   0.0    0.0   \n",
       "226   0.000000  0.000000  ...    0.0  0.0   0.0   0.0        0.0   0.0    0.0   \n",
       "1697  0.000000  0.000000  ...    0.0  0.0   0.0   0.0        0.0   0.0    0.0   \n",
       "1010  0.169833  0.000000  ...    0.0  0.0   0.0   0.0        0.0   0.0    0.0   \n",
       "\n",
       "      year  year old  young  \n",
       "642    0.0       0.0    0.0  \n",
       "700    0.0       0.0    0.0  \n",
       "226    0.0       0.0    0.0  \n",
       "1697   0.0       0.0    0.0  \n",
       "1010   0.0       0.0    0.0  \n",
       "\n",
       "[5 rows x 222 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_remaining = X_train.drop([clean_after_antonymns],axis=1)\n",
    "X_test_remaining = X_test.drop([clean_after_antonymns],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.393087</td>\n",
       "      <td>0.487273</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.427778</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.25</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.00</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      adj_count  adv_verb_count  review_polar  adj_polar  adv_verb_polar  \\\n",
       "642         5.0             2.0      0.393087   0.487273            0.00   \n",
       "700         8.0             2.0      0.427778   0.005952            0.25   \n",
       "226         1.0             1.0      0.875000   0.500000            0.50   \n",
       "1697        1.0             2.0      0.412500   0.400000            0.00   \n",
       "1010       11.0             8.0      0.230303   0.281481            0.00   \n",
       "\n",
       "      count_words  count_capital_words  count_love  count_shit  \\\n",
       "642            22                    0           2           1   \n",
       "700            39                    0           0           0   \n",
       "226             4                    0           1           0   \n",
       "1697           19                    0           1           0   \n",
       "1010           82                    0           1           2   \n",
       "\n",
       "      count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "642                   0                 1                0             0  \n",
       "700                   0                 0                1             0  \n",
       "226                   0                 1                0             0  \n",
       "1697                  4                 6                1             0  \n",
       "1010                  0                 0                1             0  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_remaining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    X_train_remaining.reset_index(drop=True,inplace=True)\n",
    "    X_test_remaining.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    y_train.reset_index(drop=True,inplace=True)\n",
    "    y_test.reset_index(drop=True,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_train = pd.concat([X_train_remaining,X_train_tfidf_df],axis=1)\n",
    "final_X_test = pd.concat([X_test_remaining,X_test_tfidf_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "final_X shape\n",
      "(2400, 235)\n",
      "(600, 235)\n",
      "-------------------------------------\n",
      "X _remaining shape\n",
      "(2400, 13)\n",
      "(600, 13)\n",
      "-------------------------------------\n",
      "X _tfidf shape\n",
      "(2400, 222)\n",
      "(600, 222)\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------\")\n",
    "print(\"final_X shape\")\n",
    "print(final_X_train.shape)\n",
    "print(final_X_test.shape)\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"X _remaining shape\")\n",
    "print(X_train_remaining.shape)\n",
    "print(X_test_remaining.shape)\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"X _tfidf shape\")\n",
    "print(X_train_tfidf_df.shape)\n",
    "print(X_test_tfidf_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE the train data as there is data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(final_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_X_train.shape = (2400, 235)\n",
      "y_train.shape = (2400,)\n",
      "X_train_sm.shape = (3468, 235)\n",
      "y_train_sm.shape = (3468,)\n",
      "sum(y_train_sm) = 1734\n"
     ]
    }
   ],
   "source": [
    "print(f\"final_X_train.shape = {final_X_train.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")\n",
    "print(f\"X_train_sm.shape = {X_train_sm.shape}\")\n",
    "print(f\"y_train_sm.shape = {y_train_sm.shape}\")\n",
    "print(f\"sum(y_train_sm) = {sum(y_train_sm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.5\n",
      "\n",
      "Train data f1-score for class '0' 0.0\n",
      "\n",
      "Train data f1-score for class '1' 0.6666666666666666\n",
      "TEST DATA ACCURACY 0.27\n",
      "\n",
      "Test data f1-score for class '0' 0.0\n",
      "\n",
      "Test data f1-score for class '1' 0.4251968503937008\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7707612456747405\n",
      "\n",
      "Train data f1-score for class '0' 0.7624738571855394\n",
      "\n",
      "Train data f1-score for class '1' 0.7784898300362217\n",
      "TEST DATA ACCURACY 0.7833333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8391089108910891\n",
      "\n",
      "Test data f1-score for class '1' 0.6683673469387756\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.763840830449827\n",
      "\n",
      "Train data f1-score for class '0' 0.758335792269106\n",
      "\n",
      "Train data f1-score for class '1' 0.7691006484352975\n",
      "TEST DATA ACCURACY 0.7766666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8341584158415842\n",
      "\n",
      "Test data f1-score for class '1' 0.6581632653061223\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7860438292964245\n",
      "\n",
      "Train data f1-score for class '0' 0.7818930041152262\n",
      "\n",
      "Train data f1-score for class '1' 0.7900396151669495\n",
      "TEST DATA ACCURACY 0.8033333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.854679802955665\n",
      "\n",
      "Test data f1-score for class '1' 0.6958762886597939\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.7892156862745098\n",
      "\n",
      "Train data f1-score for class '0' 0.7835356825584839\n",
      "\n",
      "Train data f1-score for class '1' 0.7946052261871311\n",
      "TEST DATA ACCURACY 0.805\n",
      "\n",
      "Test data f1-score for class '0' 0.8539325842696629\n",
      "\n",
      "Test data f1-score for class '1' 0.706766917293233\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8220876585928489\n",
      "\n",
      "Train data f1-score for class '0' 0.8165328575676479\n",
      "\n",
      "Train data f1-score for class '1' 0.8273159809683738\n",
      "TEST DATA ACCURACY 0.82\n",
      "\n",
      "Test data f1-score for class '0' 0.8682926829268293\n",
      "\n",
      "Test data f1-score for class '1' 0.7157894736842104\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.8682237600922722\n",
      "\n",
      "Train data f1-score for class '0' 0.864431919311777\n",
      "\n",
      "Train data f1-score for class '1' 0.8718092566619915\n",
      "TEST DATA ACCURACY 0.8233333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8729016786570744\n",
      "\n",
      "Test data f1-score for class '1' 0.7103825136612022\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8636101499423299\n",
      "\n",
      "Train data f1-score for class '0' 0.8589322994333434\n",
      "\n",
      "Train data f1-score for class '1' 0.8679877197878872\n",
      "TEST DATA ACCURACY 0.8166666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.8674698795180722\n",
      "\n",
      "Test data f1-score for class '1' 0.7027027027027027\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.8829296424452133\n",
      "\n",
      "Train data f1-score for class '0' 0.879739336492891\n",
      "\n",
      "Train data f1-score for class '1' 0.8859550561797752\n",
      "TEST DATA ACCURACY 0.7933333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8534278959810875\n",
      "\n",
      "Test data f1-score for class '1' 0.6497175141242938\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.878316032295271\n",
      "\n",
      "Train data f1-score for class '0' 0.8745541022592151\n",
      "\n",
      "Train data f1-score for class '1' 0.8818589025755879\n",
      "TEST DATA ACCURACY 0.8133333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.867612293144208\n",
      "\n",
      "Test data f1-score for class '1' 0.6836158192090396\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.8829296424452133\n",
      "\n",
      "Train data f1-score for class '0' 0.88023598820059\n",
      "\n",
      "Train data f1-score for class '1' 0.8855047941342358\n",
      "TEST DATA ACCURACY 0.7916666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8520710059171598\n",
      "\n",
      "Test data f1-score for class '1' 0.6478873239436621\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8814878892733564\n",
      "\n",
      "Train data f1-score for class '0' 0.8785101980490688\n",
      "\n",
      "Train data f1-score for class '1' 0.8843231072333241\n",
      "TEST DATA ACCURACY 0.79\n",
      "\n",
      "Test data f1-score for class '0' 0.8514150943396226\n",
      "\n",
      "Test data f1-score for class '1' 0.6420454545454546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=final_X_test\n",
    "\n",
    "C=[0.001,0.01,0.1,1,10,100]\n",
    "Penalty=['l1','l2']\n",
    "        \n",
    "logistic_regr(X_train_sm,y_train_sm,X_test,C_list,Penalty,Features_name=\"Non-Text+TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "CV_XGB.best_score_  0.8852364475201846\n",
      "CV_XGB.best_params_  {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}\n",
      "TRAIN DATA ACCURACY 0.9803921568627451\n",
      "\n",
      "Train data f1-score for class '0' 0.9804034582132564\n",
      "\n",
      "Train data f1-score for class '1' 0.9803808424697057\n",
      "TEST DATA ACCURACY 0.8383333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8893956670467502\n",
      "\n",
      "Test data f1-score for class '1' 0.6996904024767803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=-1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'colsample_bytree': array([0.5, 0.6, 0.7, 0.8, 0.9]), 'n_estimators': [100], 'max_depth': [10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = final_X_test\n",
    "\n",
    "param_grid = {\n",
    "    'colsample_bytree': np.linspace(0.5, 0.9, 5),\n",
    "    'n_estimators':[100],\n",
    "    'max_depth': [10]\n",
    "}\n",
    "\n",
    "xgb_model(X_train_sm,y_train_sm,X_test,param_grid,\"Non-Text+TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "rfc_cv_grid.best_score_  0.846885813148789\n",
      "rfc_cv_grid.best_params_  {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}\n",
      "TRAIN DATA ACCURACY 0.9117647058823529\n",
      "\n",
      "Train data f1-score for class '0' 0.9099470276633314\n",
      "\n",
      "Train data f1-score for class '1' 0.9135104578858111\n",
      "TEST DATA ACCURACY 0.825\n",
      "\n",
      "Test data f1-score for class '0' 0.8751486325802615\n",
      "\n",
      "Test data f1-score for class '1' 0.7075208913649026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [10, 25, 50, 75, 100], 'max_depth': [10, 12, 14, 16, 18, 20], 'min_samples_leaf': [5, 10, 15, 20], 'class_weight': ['balanced', 'balanced_subsample']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = final_X_test\n",
    "\n",
    "param_grid = {\"n_estimators\" : [10, 25, 50, 75, 100],\n",
    "           \"max_depth\" : [10, 12, 14, 16, 18, 20],\n",
    "           \"min_samples_leaf\" : [5, 10, 15, 20],\n",
    "           \"class_weight\" : ['balanced','balanced_subsample']}\n",
    "\n",
    "\n",
    "rfc_model(X_train_sm,y_train_sm,X_test,param_grid,\"Non-Text+TFIDF\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non-text Features +  CountVectorizer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targetSentiment</th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>like animation animal look real</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>amazing realistic incredible music</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>classic good remake love glover outstanding</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>nice animation cgi completely lack disney styl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>good mainly sentimental value</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   targetSentiment                                      antonymns_upd  \\\n",
       "0                0                    like animation animal look real   \n",
       "1                0                 amazing realistic incredible music   \n",
       "2                0        classic good remake love glover outstanding   \n",
       "3                1  nice animation cgi completely lack disney styl...   \n",
       "4                0                      good mainly sentimental value   \n",
       "\n",
       "   adj_count  adv_verb_count  review_polar  adj_polar  adv_verb_polar  \\\n",
       "0        2.0             2.0      0.433333   0.200000        0.000000   \n",
       "1        3.0             0.0      0.605556   0.555556        0.000000   \n",
       "2        4.0             1.0      0.516667   0.466667        0.500000   \n",
       "3        5.0             2.0      0.318750   0.391667        0.100000   \n",
       "4        2.0             1.0      0.205556   0.225000        0.166667   \n",
       "\n",
       "   count_words  count_capital_words  count_love  count_shit  \\\n",
       "0           12                    0           0           0   \n",
       "1            6                    0           2           0   \n",
       "2            8                    0           2           1   \n",
       "3           27                    1           0           1   \n",
       "4            8                    0           0           0   \n",
       "\n",
       "   count_quest_marks  count_excl_marks  count_NOT_words  count_emojis  \n",
       "0                  0                 0                0             0  \n",
       "1                  0                 1                0             0  \n",
       "2                  0                 0                0             0  \n",
       "3                  1                 0                0             0  \n",
       "4                  0                 0                0             0  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_data_text.drop(targetSentiment,axis=1)\n",
    "y = final_data_text[targetSentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking both unigram and bigram\n",
    "cvect = CountVectorizer(max_df=0.9, max_features=10000000,\n",
    "                                 min_df=0.001, ngram_range=(1,2))\n",
    "\n",
    "#cvect = CountVectorizer()\n",
    "cvect.fit(X_train[clean_after_antonymns])\n",
    "\n",
    "X_train_cvect_matrix= cvect.transform(X_train[clean_after_antonymns])\n",
    "X_test_cvect_matrix = cvect.transform(X_test[clean_after_antonymns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform the FinalOut test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalOut_cvect_matrix = cvect.transform(FinalOut_final_data_text[clean_after_antonymns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making CountVectorize matrix as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvect_df = pd.DataFrame(X_train_cvect_matrix.toarray(), columns=cvect.get_feature_names(),index=X_train.index)\n",
    "X_test_cvect_df  = pd.DataFrame(X_test_cvect_matrix.toarray(), columns=cvect.get_feature_names(),index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely amazing</th>\n",
       "      <th>absolutely beautiful</th>\n",
       "      <th>absolutely love</th>\n",
       "      <th>absolutely stunning</th>\n",
       "      <th>accurate</th>\n",
       "      <th>achieve</th>\n",
       "      <th>achievement</th>\n",
       "      <th>...</th>\n",
       "      <th>year old</th>\n",
       "      <th>yes</th>\n",
       "      <th>young</th>\n",
       "      <th>young child</th>\n",
       "      <th>young kid</th>\n",
       "      <th>youth</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>zazu</th>\n",
       "      <th>zimmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able  absolute  absolutely  absolutely amazing  absolutely beautiful  \\\n",
       "642      0         0           0                   0                     0   \n",
       "700      0         0           0                   0                     0   \n",
       "226      0         0           0                   0                     0   \n",
       "1697     0         0           0                   0                     0   \n",
       "1010     0         0           0                   0                     0   \n",
       "\n",
       "      absolutely love  absolutely stunning  accurate  achieve  achievement  \\\n",
       "642                 0                    0         0        0            0   \n",
       "700                 0                    0         0        0            0   \n",
       "226                 0                    0         0        0            0   \n",
       "1697                0                    0         0        0            0   \n",
       "1010                0                    0         0        0            0   \n",
       "\n",
       "      ...  year old  yes  young  young child  young kid  youth  yr  yr old  \\\n",
       "642   ...         0    0      0            0          0      0   0       0   \n",
       "700   ...         0    0      0            0          0      0   0       0   \n",
       "226   ...         0    0      0            0          0      0   0       0   \n",
       "1697  ...         0    0      0            0          0      0   0       0   \n",
       "1010  ...         0    0      0            0          0      0   0       0   \n",
       "\n",
       "      zazu  zimmer  \n",
       "642      0       0  \n",
       "700      0       0  \n",
       "226      0       0  \n",
       "1697     0       0  \n",
       "1010     0       0  \n",
       "\n",
       "[5 rows x 2259 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvect_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making FinalOut cvect matrix data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalOut_cvect_df = pd.DataFrame(FinalOut_cvect_matrix.toarray(),columns=cvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalOut_nontext_remaining = FinalOut_final_data_text.drop([clean_after_antonymns],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_remaining = X_train.drop([clean_after_antonymns],axis=1)\n",
    "X_test_remaining = X_test.drop([clean_after_antonymns],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antonymns_upd</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_verb_count</th>\n",
       "      <th>review_polar</th>\n",
       "      <th>adj_polar</th>\n",
       "      <th>adv_verb_polar</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_love</th>\n",
       "      <th>count_shit</th>\n",
       "      <th>count_quest_marks</th>\n",
       "      <th>count_excl_marks</th>\n",
       "      <th>count_NOT_words</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>incredible visual effect bring warm feeling no...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.393087</td>\n",
       "      <td>0.487273</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>good bad fake visual pretty big problem animal...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.427778</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.25</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>love movie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>lion king dislike music fabulous plan go</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>bland colorless lack character song devoid ori...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.00</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          antonymns_upd  adj_count  \\\n",
       "642   incredible visual effect bring warm feeling no...        5.0   \n",
       "700   good bad fake visual pretty big problem animal...        8.0   \n",
       "226                                          love movie        1.0   \n",
       "1697           lion king dislike music fabulous plan go        1.0   \n",
       "1010  bland colorless lack character song devoid ori...       11.0   \n",
       "\n",
       "      adv_verb_count  review_polar  adj_polar  adv_verb_polar  count_words  \\\n",
       "642              2.0      0.393087   0.487273            0.00           22   \n",
       "700              2.0      0.427778   0.005952            0.25           39   \n",
       "226              1.0      0.875000   0.500000            0.50            4   \n",
       "1697             2.0      0.412500   0.400000            0.00           19   \n",
       "1010             8.0      0.230303   0.281481            0.00           82   \n",
       "\n",
       "      count_capital_words  count_love  count_shit  count_quest_marks  \\\n",
       "642                     0           2           1                  0   \n",
       "700                     0           0           0                  0   \n",
       "226                     0           1           0                  0   \n",
       "1697                    0           1           0                  4   \n",
       "1010                    0           1           2                  0   \n",
       "\n",
       "      count_excl_marks  count_NOT_words  count_emojis  \n",
       "642                  1                0             0  \n",
       "700                  0                1             0  \n",
       "226                  1                0             0  \n",
       "1697                 6                1             0  \n",
       "1010                 0                1             0  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_train = pd.concat([X_train_remaining,X_train_cvect_df],axis=1)\n",
    "final_X_test = pd.concat([X_test_remaining,X_test_cvect_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat FinalOut data for non text and count vector matrix like train data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_FinalOut_cvet_ntext = pd.concat([FinalOut_nontext_remaining,FinalOut_cvect_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 2272)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_FinalOut_cvet_ntext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "final_X shape\n",
      "(2400, 2272)\n",
      "(600, 2272)\n",
      "-------------------------------------\n",
      "X _remaining shape\n",
      "(2400, 13)\n",
      "(600, 13)\n",
      "-------------------------------------\n",
      "X _tfidf shape\n",
      "(2400, 2259)\n",
      "(600, 2259)\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------\")\n",
    "print(\"final_X shape\")\n",
    "print(final_X_train.shape)\n",
    "print(final_X_test.shape)\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"X _remaining shape\")\n",
    "print(X_train_remaining.shape)\n",
    "print(X_test_remaining.shape)\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"X _tfidf shape\")\n",
    "print(X_train_cvect_df.shape)\n",
    "print(X_test_cvect_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(final_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_X_train.shape = (2400, 2272)\n",
      "y_train.shape = (2400,)\n",
      "X_train_sm.shape = (3468, 2272)\n",
      "y_train_sm.shape = (3468,)\n",
      "sum(y_train_sm) = 1734\n"
     ]
    }
   ],
   "source": [
    "print(f\"final_X_train.shape = {final_X_train.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")\n",
    "print(f\"X_train_sm.shape = {X_train_sm.shape}\")\n",
    "print(f\"y_train_sm.shape = {y_train_sm.shape}\")\n",
    "print(f\"sum(y_train_sm) = {sum(y_train_sm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.5\n",
      "\n",
      "Train data f1-score for class '0' 0.0\n",
      "\n",
      "Train data f1-score for class '1' 0.6666666666666666\n",
      "TEST DATA ACCURACY 0.27\n",
      "\n",
      "Test data f1-score for class '0' 0.0\n",
      "\n",
      "Test data f1-score for class '1' 0.4251968503937008\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.001 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.7964244521337946\n",
      "\n",
      "Train data f1-score for class '0' 0.7882423515296941\n",
      "\n",
      "Train data f1-score for class '1' 0.8039977790116601\n",
      "TEST DATA ACCURACY 0.7966666666666666\n",
      "\n",
      "Test data f1-score for class '0' 0.8508557457212714\n",
      "\n",
      "Test data f1-score for class '1' 0.680628272251309\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.773356401384083\n",
      "\n",
      "Train data f1-score for class '0' 0.7663495838287753\n",
      "\n",
      "Train data f1-score for class '1' 0.7799552071668533\n",
      "TEST DATA ACCURACY 0.7833333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8399014778325123\n",
      "\n",
      "Test data f1-score for class '1' 0.6649484536082474\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.01 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.8356401384083045\n",
      "\n",
      "Train data f1-score for class '0' 0.830860534124629\n",
      "\n",
      "Train data f1-score for class '1' 0.8401570386988222\n",
      "TEST DATA ACCURACY 0.8283333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8757539203860071\n",
      "\n",
      "Test data f1-score for class '1' 0.7223719676549865\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.8269896193771626\n",
      "\n",
      "Train data f1-score for class '0' 0.821640903686088\n",
      "\n",
      "Train data f1-score for class '1' 0.832026875699888\n",
      "TEST DATA ACCURACY 0.815\n",
      "\n",
      "Test data f1-score for class '0' 0.8654545454545455\n",
      "\n",
      "Test data f1-score for class '1' 0.704\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  0.1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.904555940023068\n",
      "\n",
      "Train data f1-score for class '0' 0.9018677734954047\n",
      "\n",
      "Train data f1-score for class '1' 0.9071007577883806\n",
      "TEST DATA ACCURACY 0.8533333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8964705882352941\n",
      "\n",
      "Test data f1-score for class '1' 0.7485714285714287\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.9498269896193772\n",
      "\n",
      "Train data f1-score for class '0' 0.9491525423728814\n",
      "\n",
      "Train data f1-score for class '1' 0.9504837791690381\n",
      "TEST DATA ACCURACY 0.8666666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.907621247113164\n",
      "\n",
      "Test data f1-score for class '1' 0.7604790419161678\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9711649365628604\n",
      "\n",
      "Train data f1-score for class '0' 0.9709302325581396\n",
      "\n",
      "Train data f1-score for class '1' 0.9713958810068649\n",
      "TEST DATA ACCURACY 0.8566666666666667\n",
      "\n",
      "Test data f1-score for class '0' 0.9004629629629629\n",
      "\n",
      "Test data f1-score for class '1' 0.7440476190476191\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.998558246828143\n",
      "\n",
      "Train data f1-score for class '0' 0.9985578309777905\n",
      "\n",
      "Train data f1-score for class '1' 0.9985586624387431\n",
      "TEST DATA ACCURACY 0.8083333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8679678530424799\n",
      "\n",
      "Test data f1-score for class '1' 0.6504559270516718\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  10 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9948096885813149\n",
      "\n",
      "Train data f1-score for class '0' 0.9947946790052052\n",
      "\n",
      "Train data f1-score for class '1' 0.9948246118458883\n",
      "TEST DATA ACCURACY 0.84\n",
      "\n",
      "Test data f1-score for class '0' 0.8891454965357969\n",
      "\n",
      "Test data f1-score for class '1' 0.7125748502994013\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.9991349480968859\n",
      "\n",
      "Train data f1-score for class '0' 0.9991346985866745\n",
      "\n",
      "Train data f1-score for class '1' 0.999135197463246\n",
      "TEST DATA ACCURACY 0.7983333333333333\n",
      "\n",
      "Test data f1-score for class '0' 0.8601156069364161\n",
      "\n",
      "Test data f1-score for class '1' 0.6388059701492538\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  100 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9994232987312572\n",
      "\n",
      "Train data f1-score for class '0' 0.9994232987312572\n",
      "\n",
      "Train data f1-score for class '1' 0.9994232987312572\n",
      "TEST DATA ACCURACY 0.815\n",
      "\n",
      "Test data f1-score for class '0' 0.8725602755453501\n",
      "\n",
      "Test data f1-score for class '1' 0.662613981762918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=final_X_test\n",
    "\n",
    "C=[0.001,0.01,0.1,1,10,100]\n",
    "Penalty=['l1','l2']\n",
    "        \n",
    "logistic_regr(X_train_sm,y_train_sm,X_test,C_list,Penalty,Features_name=\"Non-Text+CountVect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trex\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "CV_XGB.best_score_  0.8835063437139562\n",
      "CV_XGB.best_params_  {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}\n",
      "TRAIN DATA ACCURACY 0.976643598615917\n",
      "\n",
      "Train data f1-score for class '0' 0.9768637532133676\n",
      "\n",
      "Train data f1-score for class '1' 0.9764192139737992\n",
      "TEST DATA ACCURACY 0.8433333333333334\n",
      "\n",
      "Test data f1-score for class '0' 0.8936651583710408\n",
      "\n",
      "Test data f1-score for class '1' 0.7025316455696202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=-1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'colsample_bytree': array([0.5, 0.6, 0.7, 0.8, 0.9]), 'n_estimators': [100], 'max_depth': [10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = final_X_test\n",
    "\n",
    "param_grid = {\n",
    "    'colsample_bytree': np.linspace(0.5, 0.9, 5),\n",
    "    'n_estimators':[100],\n",
    "    'max_depth': [10]\n",
    "}\n",
    "\n",
    "xgb_model(X_train_sm,y_train_sm,X_test,param_grid,\"Non-Text+CountVect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2272)\n",
      "-------------------------------------------------------------------------------------\n",
      "rfc_cv_grid.best_score_  0.8491926182237601\n",
      "rfc_cv_grid.best_params_  {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}\n",
      "TRAIN DATA ACCURACY 0.8961937716262975\n",
      "\n",
      "Train data f1-score for class '0' 0.8943661971830987\n",
      "\n",
      "Train data f1-score for class '1' 0.8979591836734693\n",
      "TEST DATA ACCURACY 0.835\n",
      "\n",
      "Test data f1-score for class '0' 0.8822829964328182\n",
      "\n",
      "Test data f1-score for class '1' 0.724233983286908\n",
      "(600, 2272)\n"
     ]
    }
   ],
   "source": [
    "X_test = final_X_test\n",
    "\n",
    "param_grid = {\"n_estimators\" : [10, 25, 50, 75, 100],\n",
    "           \"max_depth\" : [10, 12, 14, 16, 18, 20],\n",
    "           \"min_samples_leaf\" : [5, 10, 15, 20],\n",
    "           \"class_weight\" : ['balanced','balanced_subsample']}\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "rfc_cv_grid=rfc_model(X_train_sm,y_train_sm,X_test,param_grid,\"Non-Text+CountVect\")\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2272)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After different scenarios of ML model have run, analyse the model summary for the models giving best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "  Model_Name Train_Accuracy F1_0Class F1_1Class Test_Accuracy F1_0Class_test  \\\n",
      "0   Logistic            0.5         0  0.666667          0.27              0   \n",
      "1   Logistic       0.769608  0.761421  0.777251      0.783333       0.839109   \n",
      "2   Logistic       0.763264  0.757889  0.768406      0.776667       0.833747   \n",
      "3   Logistic       0.772491  0.768554  0.776297      0.796667       0.849383   \n",
      "4   Logistic         0.7797  0.776869   0.78246      0.808333       0.857849   \n",
      "\n",
      "  F1_1Class_test  Features                  Comments  \n",
      "0       0.425197  Non-Text   C : 0.001| Penalty : l1  \n",
      "1       0.668367  Non-Text   C : 0.001| Penalty : l2  \n",
      "2       0.659898  Non-Text    C : 0.01| Penalty : l1  \n",
      "3       0.687179  Non-Text    C : 0.01| Penalty : l2  \n",
      "4       0.705882  Non-Text     C : 0.1| Penalty : l1  \n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "      Model_Name Train_Accuracy F1_0Class F1_1Class Test_Accuracy  \\\n",
      "79      Logistic        0.99481  0.994795  0.994825          0.84   \n",
      "80      Logistic       0.999135  0.999135  0.999135      0.798333   \n",
      "81      Logistic       0.999423  0.999423  0.999423         0.815   \n",
      "82       XGBoost       0.976644  0.976864  0.976419      0.843333   \n",
      "83  RandomForest       0.896194  0.894366  0.897959         0.835   \n",
      "\n",
      "   F1_0Class_test F1_1Class_test            Features  \\\n",
      "79       0.889145       0.712575  Non-Text+CountVect   \n",
      "80       0.860116       0.638806  Non-Text+CountVect   \n",
      "81        0.87256       0.662614  Non-Text+CountVect   \n",
      "82       0.893665       0.702532  Non-Text+CountVect   \n",
      "83       0.882283       0.724234  Non-Text+CountVect   \n",
      "\n",
      "                                             Comments  \n",
      "79                               C : 10| Penalty : l2  \n",
      "80                              C : 100| Penalty : l1  \n",
      "81                              C : 100| Penalty : l2  \n",
      "82   Best_Score_ : 0.8835063437139562| Best_Param_...  \n",
      "83   Best_Score_ : 0.8491926182237601| Best_Param_...  \n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(model_summary).to_csv(\"Model_Summary_v1.csv\")\n",
    "print(print_line)\n",
    "print(model_summary.head())\n",
    "print(print_line)\n",
    "print(model_summary.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After analyzing the xls file, using the Logistic and RFC that is the best for final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is based on 80:20 split of train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the best 3 models : 2 logistc , 1 RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(penalty = 'l1', C = 1.0,random_state = 0)\n",
    "logisticRegr.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "## Predicting for the given TEST data\n",
    "lgr_FinalOut_ver1 = logisticRegr.predict(final_FinalOut_cvet_ntext)    \n",
    "pd.DataFrame(lgr_FinalOut_ver1).to_csv(\"lgr_cvet_ntext_verl1_1.csv\") ## Result .76 F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(penalty = 'l2', C = 1.0,random_state = 0)\n",
    "logisticRegr.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "## Predicting for the given TEST data\n",
    "lgr_FinalOut_ver1_1 = logisticRegr.predict(final_FinalOut_cvet_ntext) \n",
    "pd.DataFrame(lgr_FinalOut_ver1_1).to_csv(\"lgr_cvet_ntext_verl2_1.csv\") ## Result .744 F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicting for the given TEST data\n",
    "rfc_FinalOut_ver1 = rfc_cv_grid.predict(final_FinalOut_cvet_ntext)\n",
    "\n",
    "pd.DataFrame(rfc_FinalOut_ver1).to_csv(\"rfc_cvet_ntext_ver1.csv\") ## Result : .72 F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on full data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3468, 2272)\n",
      "(600, 2272)\n",
      "(4068, 2272)\n",
      "(4068,)\n"
     ]
    }
   ],
   "source": [
    "## Using the best model paramaters and training on full data\n",
    "\n",
    "full_X_train=np.concatenate([X_train_sm,X_test.values])\n",
    "full_y_train=np.concatenate([y_train_sm,y_test])\n",
    "#X_test=final_FinalOut_cvet_ntext\n",
    "\n",
    "print(X_train_sm.shape)\n",
    "print(X_test.shape)\n",
    "print(full_X_train.shape)\n",
    "print(full_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3468, 2272)\n",
      "(1200, 2272)\n",
      "(4068, 2272)\n",
      "(4068,)\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1.0 Penalty :  l1\n",
      "TRAIN DATA ACCURACY 0.9451819075712881\n",
      "\n",
      "Train data f1-score for class '0' 0.9480791618160652\n",
      "\n",
      "Train data f1-score for class '1' 0.9419422025514189\n",
      "-------------------------------------------------------------------------------------\n",
      "C :  1.0 Penalty :  l2\n",
      "TRAIN DATA ACCURACY 0.9643559488692232\n",
      "\n",
      "Train data f1-score for class '0' 0.9664274137531836\n",
      "\n",
      "Train data f1-score for class '1' 0.9620120513492272\n",
      "-------------------------------------------------------------------------------------\n",
      "rfc_cv_grid.best_score_  0.8478367748279253\n",
      "rfc_cv_grid.best_params_  {'n_estimators': 50, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced'}\n",
      "TRAIN DATA ACCURACY 0.8817600786627335\n",
      "\n",
      "Train data f1-score for class '0' 0.8847352024922118\n",
      "\n",
      "Train data f1-score for class '1' 0.8786273025485744\n"
     ]
    }
   ],
   "source": [
    "X_test = final_X_test\n",
    "full_X_train=np.concatenate([X_train_sm,X_test.values])\n",
    "full_y_train=np.concatenate([y_train_sm,y_test])\n",
    "X_test=final_FinalOut_cvet_ntext\n",
    "\n",
    "print(X_train_sm.shape)\n",
    "print(X_test.shape)\n",
    "print(full_X_train.shape)\n",
    "print(full_y_train.shape)\n",
    "## Using Best models for Predictions\n",
    "## 1st :  logistic model\n",
    "C_list=[1.0]\n",
    "Penalty=['l1']\n",
    "lgr=logistic_regr(full_X_train, full_y_train,X_test,C_list,Penalty,Features_name=\"FULL DATA: Non-Text+CountVect\",TESTDATA=False)\n",
    "    \n",
    "lgr_FinalOut_ver_full2_1 = lgr.predict(final_FinalOut_cvet_ntext) ## will use this in stacking model\n",
    "pd.DataFrame(lgr_FinalOut_ver_full2_1).to_csv(\"lgr_cvet_ntext_ver_full2_l1.csv\") ## Result .7xx F1 score\n",
    "\n",
    "\n",
    "lgr_full_train_v2_1_train= lgr.predict(full_X_train) ## will use this pred values for generating stacking model\n",
    "\n",
    "## 2nd :  logistic model\n",
    "\n",
    "C_list=[1.0]\n",
    "Penalty=['l2']\n",
    "lgr=logistic_regr(full_X_train, full_y_train,X_test,C_list,Penalty,Features_name=\"FULL DATA: Non-Text+CountVect\",TESTDATA=False)\n",
    "\n",
    "lgr_FinalOut_ver_full2_2 = lgr.predict(final_FinalOut_cvet_ntext) ## will use this in stacking model\n",
    "\n",
    "pd.DataFrame(lgr_FinalOut_ver_full2_2).to_csv(\"lgr_cvet_ntext_ver_full2_l2.csv\")  ## Result .7xx F1 score\n",
    "\n",
    "lgr_full_train_v2_2_train = lgr.predict(full_X_train) ## will use this pred values for generating stacking model\n",
    "\n",
    "## 3rd :  Random Forest model\n",
    "\n",
    "# using the best paramaters\n",
    "param_grid = {\"n_estimators\" : [50],\n",
    "           \"max_depth\" : [20],\n",
    "           \"min_samples_leaf\" : [5],\n",
    "           \"class_weight\" : ['balanced']}\n",
    "\n",
    "\n",
    "rfc_cv_grid=rfc_model(full_X_train, full_y_train,X_test,param_grid,\"FULL DATA : Non-Text+CountVect\",TESTDATA=False)\n",
    "\n",
    "rfc_FinalOut_ver_full2 = rfc_cv_grid.predict(final_FinalOut_cvet_ntext)\n",
    "pd.DataFrame(rfc_FinalOut_ver_full2).to_csv(\"rfc_cvet_ntext_ver2_.csv\") ## Result .7xx F1 score\n",
    "\n",
    "\n",
    "rfc_full_2_train = rfc_cv_grid.predict(full_X_train) ## will use this pred values for generating stacking model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Top performing models on 80:20 split of train data\n",
      "\n",
      "\tModel_Name\tTrain_Accuracy\tF1_0Class\tF1_1Class\tTest_Accuracy\tF1_0Class_test\tF1_1Class_test\tFeatures\tComments\n",
      "\n",
      "76\tLogistic\t0.94982699\t0.949182243\t0.950455581\t0.858333333\t0.901960784\t0.744744745\tTFIDF_CVect\t C : 1| Penalty : l1\n",
      "77\tLogistic\t0.970011534\t0.969767442\t0.970251716\t0.856666667\t0.900232019\t0.74556213\tTFIDF_CVect\t C : 1| Penalty : l2\n",
      "\n",
      "83\tRandomForest\t0.893310265\t0.891686183\t0.894886364\t0.843333333\t0.89044289\t0.725146199\tTFIDF_CVect\t Best_Score_ : 0.8457324106113033| Best_Param_ : {'n_estimators': 50, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "\n",
    "Top performing models on 80:20 split of train data\n",
    "\n",
    "\tModel_Name\tTrain_Accuracy\tF1_0Class\tF1_1Class\tTest_Accuracy\tF1_0Class_test\tF1_1Class_test\tFeatures\tComments\n",
    "\n",
    "76\tLogistic\t0.94982699\t0.949182243\t0.950455581\t0.858333333\t0.901960784\t0.744744745\tTFIDF_CVect\t C : 1| Penalty : l1\n",
    "77\tLogistic\t0.970011534\t0.969767442\t0.970251716\t0.856666667\t0.900232019\t0.74556213\tTFIDF_CVect\t C : 1| Penalty : l2\n",
    "\n",
    "83\tRandomForest\t0.893310265\t0.891686183\t0.894886364\t0.843333333\t0.89044289\t0.725146199\tTFIDF_CVect\t Best_Score_ : 0.8457324106113033| Best_Param_ : {'n_estimators': 50, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced'}\n",
    "\n",
    "''')\n",
    "\n",
    "## These values be based on older version of models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking on Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train = pd.DataFrame([rfc_full_2_train,lgr_full_train_v2_1_train,lgr_full_train_v2_2_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train = stack_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  0  0  0\n",
       "1  1  0  0\n",
       "2  0  0  0\n",
       "3  0  0  0\n",
       "4  1  1  1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train = stack_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4068, 3)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4068,)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "     'colsample_bytree': np.linspace(0.2, 0.9, 10),\n",
    "     'n_estimators':[100],\n",
    "     'max_depth': [10]\n",
    "}\n",
    "\n",
    "XGB = XGBClassifier(n_jobs=-1)\n",
    "stack_XGB = GridSearchCV(estimator=XGB, param_grid=param_grid, cv= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=-1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'colsample_bytree': array([0.2    , 0.27778, 0.35556, 0.43333, 0.51111, 0.58889, 0.66667,\n",
       "       0.74444, 0.82222, 0.9    ]), 'n_estimators': [100], 'max_depth': [10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_XGB.fit(stack_train,full_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9643559488692232"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_XGB.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalOut_stack_train = pd.DataFrame([rfc_FinalOut_ver_full2,lgr_FinalOut_ver_full2_1,lgr_FinalOut_ver_full2_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalOut_stack_train = FinalOut_stack_train.T\n",
    "\n",
    "FinalOut_stack_train = FinalOut_stack_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 3)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalOut_stack_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_predict_FinalOut =  stack_XGB.predict(FinalOut_stack_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stack_predict_FinalOut).to_csv(\"stack_pred_FinalOut.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" align=center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Model Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly as py\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines",
         "name": "Train Accuracy",
         "text": [
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "XGBoost",
          "RandomForest",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "XGBoost",
          "RandomForest",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "XGBoost",
          "RandomForest",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "XGBoost",
          "RandomForest",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "XGBoost",
          "RandomForest",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "Logistic",
          "XGBoost",
          "RandomForest",
          "Logistic",
          "Logistic",
          "RandomForest"
         ],
         "type": "scatter",
         "uid": "7659c313-965d-4868-96af-6537ab29a40c",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86
         ],
         "y": [
          0.5,
          0.7696078431372549,
          0.7632641291810842,
          0.7724913494809689,
          0.7797001153402537,
          0.779123414071511,
          0.7768166089965398,
          0.7765282583621684,
          0.7756632064590542,
          0.7753748558246828,
          0.7756632064590542,
          0.7753748558246828,
          0.9873125720876585,
          0.9065743944636678,
          0.5,
          0.8460207612456747,
          0.5,
          0.8428489042675894,
          0.7047289504036909,
          0.8647635524798154,
          0.8765859284890427,
          0.9181084198385236,
          0.9936562860438293,
          0.9786620530565168,
          0.9979815455594002,
          0.9976931949250288,
          0.9466551326412919,
          0.8601499423298731,
          0.5,
          0.5135524798154556,
          0.5,
          0.5867935409457901,
          0.6035178777393311,
          0.6066897347174164,
          0.6023644752018454,
          0.6081314878892734,
          0.7497116493656286,
          0.6441753171856978,
          0.7621107266435986,
          0.7332756632064591,
          0.9976931949250288,
          0.9455017301038062,
          0.5,
          0.7742214532871973,
          0.6882929642445214,
          0.8189158016147635,
          0.7990196078431373,
          0.8731257208765859,
          0.919838523644752,
          0.941753171856978,
          0.9884659746251442,
          0.9821222606689735,
          0.9945213379469435,
          0.9916378316032295,
          0.8976355247981546,
          0.8013264129181085,
          0.5,
          0.7707612456747405,
          0.763840830449827,
          0.7860438292964245,
          0.7892156862745098,
          0.8220876585928489,
          0.8682237600922722,
          0.8636101499423299,
          0.8829296424452133,
          0.878316032295271,
          0.8829296424452133,
          0.8814878892733564,
          0.9803921568627451,
          0.9117647058823529,
          0.5,
          0.7964244521337946,
          0.773356401384083,
          0.8356401384083045,
          0.8269896193771626,
          0.904555940023068,
          0.9498269896193772,
          0.9711649365628604,
          0.998558246828143,
          0.9948096885813149,
          0.9991349480968859,
          0.9994232987312572,
          0.976643598615917,
          0.8961937716262975,
          0.9451819075712881,
          0.9643559488692232,
          0.8817600786627335
         ]
        },
        {
         "mode": "lines",
         "name": "Test Accuracy",
         "text": [
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8526528258362168| Best_Param_ : {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.8243944636678201| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 16, 'class_weight': 'balanced'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8696655132641292| Best_Param_ : {'colsample_bytree': 0.5, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.8264129181084199| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 18, 'class_weight': 'balanced'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8252595155709342| Best_Param_ : {'colsample_bytree': 0.6, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.7834486735870819| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 14, 'class_weight': 'balanced_subsample'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8281430219146482| Best_Param_ : {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.7713379469434832| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 10, 'max_depth': 12, 'class_weight': 'balanced_subsample'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8852364475201846| Best_Param_ : {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.846885813148789| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8835063437139562| Best_Param_ : {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.8491926182237601| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}",
          " C : 1.0| Penalty : l1",
          " C : 1.0| Penalty : l2",
          " Best_Score_ : 0.8478367748279253| Best_Param_ : {'n_estimators': 50, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced'}"
         ],
         "type": "scatter",
         "uid": "13e4f121-f5de-4f8b-8132-9e0bce0f5b6a",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86
         ],
         "y": [
          0.27,
          0.7833333333333333,
          0.7766666666666666,
          0.7966666666666666,
          0.8083333333333333,
          0.8083333333333333,
          0.815,
          0.8166666666666667,
          0.8133333333333334,
          0.8133333333333334,
          0.8133333333333334,
          0.8133333333333334,
          0.7933333333333333,
          0.7866666666666666,
          0.73,
          0.7866666666666666,
          0.73,
          0.7616666666666667,
          0.62,
          0.7733333333333333,
          0.7833333333333333,
          0.8133333333333334,
          0.8066666666666666,
          0.8216666666666667,
          0.8,
          0.79,
          0.7916666666666666,
          0.805,
          0.73,
          0.725,
          0.73,
          0.64,
          0.6133333333333333,
          0.6183333333333333,
          0.6116666666666667,
          0.61,
          0.7233333333333334,
          0.645,
          0.7516666666666667,
          0.7166666666666667,
          0.69,
          0.675,
          0.73,
          0.73,
          0.5583333333333333,
          0.7266666666666667,
          0.71,
          0.765,
          0.7833333333333333,
          0.795,
          0.7766666666666666,
          0.7783333333333333,
          0.7633333333333333,
          0.7633333333333333,
          0.7633333333333333,
          0.71,
          0.27,
          0.7833333333333333,
          0.7766666666666666,
          0.8033333333333333,
          0.805,
          0.82,
          0.8233333333333334,
          0.8166666666666667,
          0.7933333333333333,
          0.8133333333333334,
          0.7916666666666666,
          0.79,
          0.8383333333333334,
          0.825,
          0.27,
          0.7966666666666666,
          0.7833333333333333,
          0.8283333333333334,
          0.815,
          0.8533333333333334,
          0.8666666666666667,
          0.8566666666666667,
          0.8083333333333333,
          0.84,
          0.7983333333333333,
          0.815,
          0.8433333333333334,
          0.835,
          null,
          null,
          null
         ]
        },
        {
         "mode": "lines",
         "name": "F1 Test Accuracy",
         "text": [
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8526528258362168| Best_Param_ : {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.8243944636678201| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 16, 'class_weight': 'balanced'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8696655132641292| Best_Param_ : {'colsample_bytree': 0.5, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.8264129181084199| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 18, 'class_weight': 'balanced'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8252595155709342| Best_Param_ : {'colsample_bytree': 0.6, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.7834486735870819| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 14, 'class_weight': 'balanced_subsample'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8281430219146482| Best_Param_ : {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.7713379469434832| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 10, 'max_depth': 12, 'class_weight': 'balanced_subsample'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8852364475201846| Best_Param_ : {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.846885813148789| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}",
          " C : 0.001| Penalty : l1",
          " C : 0.001| Penalty : l2",
          " C : 0.01| Penalty : l1",
          " C : 0.01| Penalty : l2",
          " C : 0.1| Penalty : l1",
          " C : 0.1| Penalty : l2",
          " C : 1| Penalty : l1",
          " C : 1| Penalty : l2",
          " C : 10| Penalty : l1",
          " C : 10| Penalty : l2",
          " C : 100| Penalty : l1",
          " C : 100| Penalty : l2",
          " Best_Score_ : 0.8835063437139562| Best_Param_ : {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}",
          " Best_Score_ : 0.8491926182237601| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}",
          " C : 1.0| Penalty : l1",
          " C : 1.0| Penalty : l2",
          " Best_Score_ : 0.8478367748279253| Best_Param_ : {'n_estimators': 50, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced'}"
         ],
         "type": "scatter",
         "uid": "b75f2082-d76c-4373-b12c-a821beae287d",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86
         ],
         "y": [
          0.4251968503937008,
          0.6683673469387756,
          0.6598984771573604,
          0.6871794871794873,
          0.7058823529411764,
          0.7058823529411764,
          0.7131782945736435,
          0.7164948453608248,
          0.7113402061855669,
          0.7113402061855669,
          0.7113402061855669,
          0.7113402061855669,
          0.6265060240963856,
          0.6464088397790054,
          0,
          0.6631578947368421,
          0,
          0.6570743405275778,
          0.5365853658536586,
          0.6633663366336634,
          0.6596858638743457,
          0.702127659574468,
          0.6704545454545455,
          0.700280112044818,
          0.653179190751445,
          0.6440677966101694,
          0.6290801186943621,
          0.6332288401253918,
          0,
          0.06779661016949153,
          0,
          0.31210191082802546,
          0.40512820512820513,
          0.40519480519480516,
          0.41309823677581864,
          0.415,
          0.5911330049261084,
          0.4634760705289673,
          0.6227848101265823,
          0.5913461538461539,
          0.42592592592592593,
          0.43478260869565216,
          0,
          0.5597826086956521,
          0.4952380952380952,
          0.6019417475728156,
          0.589622641509434,
          0.6393861892583119,
          0.6542553191489362,
          0.6702412868632709,
          0.6338797814207651,
          0.6253521126760564,
          0.5965909090909092,
          0.601123595505618,
          0.6098901098901099,
          0.5561224489795918,
          0.4251968503937008,
          0.6683673469387756,
          0.6581632653061223,
          0.6958762886597939,
          0.706766917293233,
          0.7157894736842104,
          0.7103825136612022,
          0.7027027027027027,
          0.6497175141242938,
          0.6836158192090396,
          0.6478873239436621,
          0.6420454545454546,
          0.6996904024767803,
          0.7075208913649026,
          0.4251968503937008,
          0.680628272251309,
          0.6649484536082474,
          0.7223719676549865,
          0.704,
          0.7485714285714287,
          0.7604790419161678,
          0.7440476190476191,
          0.6504559270516718,
          0.7125748502994013,
          0.6388059701492538,
          0.662613981762918,
          0.7025316455696202,
          0.724233983286908,
          null,
          null,
          null
         ]
        },
        {
         "mode": "markers",
         "name": "Model Features",
         "text": [
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "Non-Text",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "TFIDF",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "Word2Vec",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "CountVect",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+TFIDF",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "Non-Text+CountVect",
          "FULL DATA: Non-Text+CountVect",
          "FULL DATA: Non-Text+CountVect",
          "FULL DATA : Non-Text+CountVect"
         ],
         "type": "scatter",
         "uid": "0afe7a35-c453-4ff1-ab5d-6ed29194a6c3",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "xaxis": {
         "title": {
          "text": "Model #"
         }
        },
        "yaxis": {
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"963ab1c5-51f1-4628-93ab-191ed6e1c703\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"963ab1c5-51f1-4628-93ab-191ed6e1c703\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '963ab1c5-51f1-4628-93ab-191ed6e1c703',\n",
       "                        [{\"mode\": \"lines\", \"name\": \"Train Accuracy\", \"text\": [\"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"XGBoost\", \"RandomForest\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"XGBoost\", \"RandomForest\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"XGBoost\", \"RandomForest\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"XGBoost\", \"RandomForest\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"XGBoost\", \"RandomForest\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"Logistic\", \"XGBoost\", \"RandomForest\", \"Logistic\", \"Logistic\", \"RandomForest\"], \"type\": \"scatter\", \"uid\": \"7659c313-965d-4868-96af-6537ab29a40c\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], \"y\": [0.5, 0.7696078431372549, 0.7632641291810842, 0.7724913494809689, 0.7797001153402537, 0.779123414071511, 0.7768166089965398, 0.7765282583621684, 0.7756632064590542, 0.7753748558246828, 0.7756632064590542, 0.7753748558246828, 0.9873125720876585, 0.9065743944636678, 0.5, 0.8460207612456747, 0.5, 0.8428489042675894, 0.7047289504036909, 0.8647635524798154, 0.8765859284890427, 0.9181084198385236, 0.9936562860438293, 0.9786620530565168, 0.9979815455594002, 0.9976931949250288, 0.9466551326412919, 0.8601499423298731, 0.5, 0.5135524798154556, 0.5, 0.5867935409457901, 0.6035178777393311, 0.6066897347174164, 0.6023644752018454, 0.6081314878892734, 0.7497116493656286, 0.6441753171856978, 0.7621107266435986, 0.7332756632064591, 0.9976931949250288, 0.9455017301038062, 0.5, 0.7742214532871973, 0.6882929642445214, 0.8189158016147635, 0.7990196078431373, 0.8731257208765859, 0.919838523644752, 0.941753171856978, 0.9884659746251442, 0.9821222606689735, 0.9945213379469435, 0.9916378316032295, 0.8976355247981546, 0.8013264129181085, 0.5, 0.7707612456747405, 0.763840830449827, 0.7860438292964245, 0.7892156862745098, 0.8220876585928489, 0.8682237600922722, 0.8636101499423299, 0.8829296424452133, 0.878316032295271, 0.8829296424452133, 0.8814878892733564, 0.9803921568627451, 0.9117647058823529, 0.5, 0.7964244521337946, 0.773356401384083, 0.8356401384083045, 0.8269896193771626, 0.904555940023068, 0.9498269896193772, 0.9711649365628604, 0.998558246828143, 0.9948096885813149, 0.9991349480968859, 0.9994232987312572, 0.976643598615917, 0.8961937716262975, 0.9451819075712881, 0.9643559488692232, 0.8817600786627335]}, {\"mode\": \"lines\", \"name\": \"Test Accuracy\", \"text\": [\" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8526528258362168| Best_Param_ : {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.8243944636678201| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 16, 'class_weight': 'balanced'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8696655132641292| Best_Param_ : {'colsample_bytree': 0.5, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.8264129181084199| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 18, 'class_weight': 'balanced'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8252595155709342| Best_Param_ : {'colsample_bytree': 0.6, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.7834486735870819| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 14, 'class_weight': 'balanced_subsample'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8281430219146482| Best_Param_ : {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.7713379469434832| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 10, 'max_depth': 12, 'class_weight': 'balanced_subsample'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8852364475201846| Best_Param_ : {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.846885813148789| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8835063437139562| Best_Param_ : {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.8491926182237601| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}\", \" C : 1.0| Penalty : l1\", \" C : 1.0| Penalty : l2\", \" Best_Score_ : 0.8478367748279253| Best_Param_ : {'n_estimators': 50, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced'}\"], \"type\": \"scatter\", \"uid\": \"13e4f121-f5de-4f8b-8132-9e0bce0f5b6a\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], \"y\": [0.27, 0.7833333333333333, 0.7766666666666666, 0.7966666666666666, 0.8083333333333333, 0.8083333333333333, 0.815, 0.8166666666666667, 0.8133333333333334, 0.8133333333333334, 0.8133333333333334, 0.8133333333333334, 0.7933333333333333, 0.7866666666666666, 0.73, 0.7866666666666666, 0.73, 0.7616666666666667, 0.62, 0.7733333333333333, 0.7833333333333333, 0.8133333333333334, 0.8066666666666666, 0.8216666666666667, 0.8, 0.79, 0.7916666666666666, 0.805, 0.73, 0.725, 0.73, 0.64, 0.6133333333333333, 0.6183333333333333, 0.6116666666666667, 0.61, 0.7233333333333334, 0.645, 0.7516666666666667, 0.7166666666666667, 0.69, 0.675, 0.73, 0.73, 0.5583333333333333, 0.7266666666666667, 0.71, 0.765, 0.7833333333333333, 0.795, 0.7766666666666666, 0.7783333333333333, 0.7633333333333333, 0.7633333333333333, 0.7633333333333333, 0.71, 0.27, 0.7833333333333333, 0.7766666666666666, 0.8033333333333333, 0.805, 0.82, 0.8233333333333334, 0.8166666666666667, 0.7933333333333333, 0.8133333333333334, 0.7916666666666666, 0.79, 0.8383333333333334, 0.825, 0.27, 0.7966666666666666, 0.7833333333333333, 0.8283333333333334, 0.815, 0.8533333333333334, 0.8666666666666667, 0.8566666666666667, 0.8083333333333333, 0.84, 0.7983333333333333, 0.815, 0.8433333333333334, 0.835, null, null, null]}, {\"mode\": \"lines\", \"name\": \"F1 Test Accuracy\", \"text\": [\" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8526528258362168| Best_Param_ : {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.8243944636678201| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 16, 'class_weight': 'balanced'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8696655132641292| Best_Param_ : {'colsample_bytree': 0.5, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.8264129181084199| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 18, 'class_weight': 'balanced'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8252595155709342| Best_Param_ : {'colsample_bytree': 0.6, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.7834486735870819| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 5, 'max_depth': 14, 'class_weight': 'balanced_subsample'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8281430219146482| Best_Param_ : {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.7713379469434832| Best_Param_ : {'n_estimators': 75, 'min_samples_leaf': 10, 'max_depth': 12, 'class_weight': 'balanced_subsample'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8852364475201846| Best_Param_ : {'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.846885813148789| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}\", \" C : 0.001| Penalty : l1\", \" C : 0.001| Penalty : l2\", \" C : 0.01| Penalty : l1\", \" C : 0.01| Penalty : l2\", \" C : 0.1| Penalty : l1\", \" C : 0.1| Penalty : l2\", \" C : 1| Penalty : l1\", \" C : 1| Penalty : l2\", \" C : 10| Penalty : l1\", \" C : 10| Penalty : l2\", \" C : 100| Penalty : l1\", \" C : 100| Penalty : l2\", \" Best_Score_ : 0.8835063437139562| Best_Param_ : {'colsample_bytree': 0.8, 'max_depth': 10, 'n_estimators': 100}\", \" Best_Score_ : 0.8491926182237601| Best_Param_ : {'n_estimators': 100, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced_subsample'}\", \" C : 1.0| Penalty : l1\", \" C : 1.0| Penalty : l2\", \" Best_Score_ : 0.8478367748279253| Best_Param_ : {'n_estimators': 50, 'min_samples_leaf': 5, 'max_depth': 20, 'class_weight': 'balanced'}\"], \"type\": \"scatter\", \"uid\": \"b75f2082-d76c-4373-b12c-a821beae287d\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], \"y\": [0.4251968503937008, 0.6683673469387756, 0.6598984771573604, 0.6871794871794873, 0.7058823529411764, 0.7058823529411764, 0.7131782945736435, 0.7164948453608248, 0.7113402061855669, 0.7113402061855669, 0.7113402061855669, 0.7113402061855669, 0.6265060240963856, 0.6464088397790054, 0.0, 0.6631578947368421, 0.0, 0.6570743405275778, 0.5365853658536586, 0.6633663366336634, 0.6596858638743457, 0.702127659574468, 0.6704545454545455, 0.700280112044818, 0.653179190751445, 0.6440677966101694, 0.6290801186943621, 0.6332288401253918, 0.0, 0.06779661016949153, 0.0, 0.31210191082802546, 0.40512820512820513, 0.40519480519480516, 0.41309823677581864, 0.415, 0.5911330049261084, 0.4634760705289673, 0.6227848101265823, 0.5913461538461539, 0.42592592592592593, 0.43478260869565216, 0.0, 0.5597826086956521, 0.4952380952380952, 0.6019417475728156, 0.589622641509434, 0.6393861892583119, 0.6542553191489362, 0.6702412868632709, 0.6338797814207651, 0.6253521126760564, 0.5965909090909092, 0.601123595505618, 0.6098901098901099, 0.5561224489795918, 0.4251968503937008, 0.6683673469387756, 0.6581632653061223, 0.6958762886597939, 0.706766917293233, 0.7157894736842104, 0.7103825136612022, 0.7027027027027027, 0.6497175141242938, 0.6836158192090396, 0.6478873239436621, 0.6420454545454546, 0.6996904024767803, 0.7075208913649026, 0.4251968503937008, 0.680628272251309, 0.6649484536082474, 0.7223719676549865, 0.704, 0.7485714285714287, 0.7604790419161678, 0.7440476190476191, 0.6504559270516718, 0.7125748502994013, 0.6388059701492538, 0.662613981762918, 0.7025316455696202, 0.724233983286908, null, null, null]}, {\"mode\": \"markers\", \"name\": \"Model Features\", \"text\": [\"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"Non-Text\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"TFIDF\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"Word2Vec\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"CountVect\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+TFIDF\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"Non-Text+CountVect\", \"FULL DATA: Non-Text+CountVect\", \"FULL DATA: Non-Text+CountVect\", \"FULL DATA : Non-Text+CountVect\"], \"type\": \"scatter\", \"uid\": \"0afe7a35-c453-4ff1-ab5d-6ed29194a6c3\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}],\n",
       "                        {\"xaxis\": {\"title\": {\"text\": \"Model #\"}}, \"yaxis\": {\"title\": {\"text\": \"Accuracy\"}}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('963ab1c5-51f1-4628-93ab-191ed6e1c703');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = {\n",
    "    'data': [\n",
    "        {'x': model_summary.index, 'y': model_summary.Train_Accuracy, 'text': model_summary.Comments,'text':model_summary.Model_Name, 'mode': 'lines', 'name': 'Train Accuracy'},\n",
    "        {'x': model_summary.index, 'y': model_summary.Test_Accuracy, 'text': model_summary.Comments, 'mode': 'lines', 'name': 'Test Accuracy'},\n",
    "        {'x': model_summary.index, 'y': model_summary.F1_1Class_test, 'text': model_summary.Comments, 'mode': 'lines', 'name': 'F1 Test Accuracy'},\n",
    "        {'x': model_summary.index, 'y':np.zeros(87), 'text': model_summary.Features,'mode': 'markers','name': 'Model Features'}\n",
    "    ],\n",
    "    'layout': {\n",
    "        'xaxis': {'title': 'Model #'},\n",
    "        'yaxis': {'title': \"Accuracy\"}\n",
    "    }\n",
    "}\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model ( F1 1-Class = .76 ) was given by Logistic Regression with C=1.0 , Penalty=L1 with CountVect + Non-Text features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
